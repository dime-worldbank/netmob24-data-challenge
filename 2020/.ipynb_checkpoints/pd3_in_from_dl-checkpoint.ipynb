{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd296ca-59d2-4eec-a32e-492674eb1ee0",
   "metadata": {},
   "source": [
    "This is the pop_density geohash 3 dataset for 2020 for IN  \n",
    "Please run the file by 3 months  \n",
    "It is set 20200101 - 20200331 for now, please change while submitting jobs  \n",
    "Please remember to check logging information to fillin data missing dates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geohash2\n",
      "  Using cached geohash2-1.1-py3-none-any.whl\n",
      "Collecting docutils>=0.3\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Installing collected packages: docutils, geohash2\n",
      "Successfully installed docutils-0.21.2 geohash2-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine class\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1706be65-38bc-40c2-a13c-07032a38ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'pop_density20'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'IN'\n",
    "start_date = 20200101\n",
    "end_date = 20200331\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Define the sorted longitude boundaries\n",
    "sorted_lons = [67.5, 68.90625, 70.3125, 71.71875, 73.125, 74.53125, 75.9375, 77.34375, 78.75, 80.15625, 81.5625, 82.96875, 84.375, 85.78125, 87.1875, 88.59375]\n",
    "\n",
    "# Initialize a list to store failed insertions\n",
    "failed_inserts = []\n",
    "# Define the buffer value\n",
    "buffer_value = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2185a185-453b-4e44-88ed-cccba705c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "                return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148ea67f-8013-4887-867f-4f30f018dc5a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:13:10,603 - INFO - Executing SQL query for date 20190323\n",
      "2024-06-08 16:23:10,924 - INFO - Inserted data into table pd_in_20190323_agg3\n",
      "2024-06-08 16:23:10,925 - INFO - Data extraction, aggregation, and saving completed.\n"
     ]
    }
   ],
   "source": [
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    try:\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Loop through each longitude range\n",
    "        for i in range(len(sorted_lons) + 1):\n",
    "            if i == 0:\n",
    "                min_lon = sorted_lons[i] - buffer_value\n",
    "                max_lon = sorted_lons[i]\n",
    "            elif i == len(sorted_lons):\n",
    "                min_lon = sorted_lons[i - 1]\n",
    "                max_lon = sorted_lons[i - 1] + buffer_value\n",
    "            else:\n",
    "                min_lon = sorted_lons[i - 1]\n",
    "                max_lon = sorted_lons[i]\n",
    "            \n",
    "            # Construct the SQL query\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND lng BETWEEN {min_lon} AND {max_lon}\n",
    "            \"\"\"\n",
    "            \n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and longitude range {min_lon} to {max_lon}\")\n",
    "            \n",
    "            try:\n",
    "                pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "                \n",
    "                # Convert the generator to a DataFrame\n",
    "                chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "                if chunks:\n",
    "                    pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "                    \n",
    "                    # Calculate geohashes\n",
    "                    pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "                    \n",
    "                    # Aggregate data for geohash3\n",
    "                    aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                        no_of_points=('geohash3', 'size'),\n",
    "                        no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "                    ).reset_index()\n",
    "                    \n",
    "                    # Filter rows with no_of_unique_users > 10\n",
    "                    filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "                    \n",
    "                    # Add the local_date column\n",
    "                    filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "                    \n",
    "                    # Insert filtered aggregated data for geohash3 into SQL table\n",
    "                    if not filtered_data_3.empty:\n",
    "                        table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                        if not insert_data_with_retry(filtered_data_3, table_name_agg3, con):\n",
    "                            failed_inserts.append((formatted_current_date, min_lon, max_lon))\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error while processing data for date {formatted_current_date} and longitude range {min_lon} to {max_lon}: {e}\")\n",
    "                failed_inserts.append((formatted_current_date, min_lon, max_lon))\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "        failed_inserts.append((formatted_current_date, None, None))\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Print the failed inserts\n",
    "if failed_inserts:\n",
    "    print(\"Failed inserts:\")\n",
    "    for failed_insert in failed_inserts:\n",
    "        print(failed_insert)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9acd1-22f1-4d35-adb5-e3c7cc97b38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38845288-a751-406b-aa05-2f614fc729b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dba193-5e42-4b42-9296-f43e304f3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bfc2b-c81c-4777-9d2c-a7b2888b3af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449ed48-7ea4-4535-87e5-cfca49296f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e7e0d-733e-44a1-b396-40e14e75c9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1599829-fd47-4e09-bee4-2bb7bb18f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e239e-b566-4eaf-aba7-65eb0552cbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
