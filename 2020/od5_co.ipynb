{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-geohash\n",
      "  Using cached python_geohash-0.8.5-cp39-cp39-linux_x86_64.whl\n",
      "Installing collected packages: python-geohash\n",
      "Successfully installed python-geohash-0.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfaadf6-a882-4c99-a7ba-5977fdb9031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5), axis=1)\n",
    "        pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5), axis=1)\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "        # Filter aggregated data\n",
    "        filtered_df5 = aggregated_df5.loc[aggregated_df5['trip_count'] > 9]\n",
    "        return filtered_df5\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data in chunks\n",
    "def insert_data_in_chunks(df, table_name, engine, chunk_size):\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88d424e-75fc-4327-8bd0-62386ee653eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with 502 handelling\n",
    "\n",
    "# Main processing loop\n",
    "def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "    errored_dates = []  # List to store dates that encounter errors\n",
    "        \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        try:\n",
    "            filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "            # Create the SQL engine\n",
    "            output_schema_name = \"od_matrix20\"\n",
    "            final_table_5 = f\"od_{country_code.lower()}_{event_date}_agg5_10\"\n",
    "            con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "            # Create the SQL table with the correct name for 5-level geohash\n",
    "            create_table_query_5 = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {final_table_5} (\n",
    "                start_geohash5 varchar,\n",
    "                end_geohash5 varchar,\n",
    "                trip_count bigint,\n",
    "                m_duration_min double,\n",
    "                mdn_duration_min double,\n",
    "                sd_duration_min double,\n",
    "                m_length_m double,\n",
    "                mdn_length_m double,\n",
    "                sd_length_m double,\n",
    "                m_points_no double,\n",
    "                mdn_points_no double,\n",
    "                sd_points_no double\n",
    "            )\n",
    "            WITH (\n",
    "              bucketed_by = ARRAY['end_geohash5'],\n",
    "              bucket_count = 30\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            with con.connect() as connection:\n",
    "                connection.execute(create_table_query_5)\n",
    "                \n",
    "            filtered_df5 = filtered_df5.astype({\n",
    "                'trip_count': 'int'\n",
    "            })\n",
    "            \n",
    "            # Insert data into the table with the correct name\n",
    "            if not filtered_df5.empty:\n",
    "                insert_data_in_chunks(filtered_df5, final_table_5, con, 400)\n",
    "                logging.info(f\"Data inserted into {final_table_5}\")\n",
    "            else:\n",
    "                logging.info(f\"No data to insert for {final_table_5} for 5-level geohash\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process data for date {event_date}: {e}\")\n",
    "            errored_dates.append(event_date)  # Record the errored date\n",
    "\n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Log the errored dates\n",
    "    if errored_dates:\n",
    "        logging.info(f\"Errored dates: {', '.join(errored_dates)}\")\n",
    "    else:\n",
    "        logging.info(\"No errors encountered during processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f08b612-f19e-4f45-b189-5d258aa4876e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 01:33:20,279 - INFO - Executing SQL query for date 20190201\n",
      "2024-06-18 01:34:03,714 - INFO - Data inserted into od_in_20190201_agg5_10\n",
      "2024-06-18 01:34:20,227 - INFO - Executing SQL query for date 20190202\n",
      "2024-06-18 01:34:55,156 - INFO - Data inserted into od_in_20190202_agg5_10\n",
      "2024-06-18 01:35:09,557 - INFO - Executing SQL query for date 20190203\n",
      "2024-06-18 01:35:39,155 - INFO - Data inserted into od_in_20190203_agg5_10\n",
      "2024-06-18 01:35:54,346 - INFO - Executing SQL query for date 20190204\n",
      "2024-06-18 01:36:29,140 - INFO - Data inserted into od_in_20190204_agg5_10\n",
      "2024-06-18 01:36:44,139 - INFO - Executing SQL query for date 20190205\n",
      "2024-06-18 01:37:21,804 - INFO - Data inserted into od_in_20190205_agg5_10\n",
      "2024-06-18 01:37:37,742 - INFO - Executing SQL query for date 20190206\n",
      "2024-06-18 01:38:17,041 - INFO - Data inserted into od_in_20190206_agg5_10\n",
      "2024-06-18 01:38:34,926 - INFO - Executing SQL query for date 20190207\n",
      "2024-06-18 01:39:15,471 - INFO - Data inserted into od_in_20190207_agg5_10\n",
      "2024-06-18 01:39:33,140 - INFO - Executing SQL query for date 20190208\n",
      "2024-06-18 01:40:13,579 - INFO - Data inserted into od_in_20190208_agg5_10\n",
      "2024-06-18 01:40:29,191 - INFO - Executing SQL query for date 20190209\n",
      "2024-06-18 01:41:08,189 - INFO - Data inserted into od_in_20190209_agg5_10\n",
      "2024-06-18 01:41:23,215 - INFO - Executing SQL query for date 20190210\n",
      "2024-06-18 01:41:57,339 - INFO - Data inserted into od_in_20190210_agg5_10\n",
      "2024-06-18 01:42:12,528 - INFO - Executing SQL query for date 20190211\n",
      "2024-06-18 01:42:50,831 - INFO - Data inserted into od_in_20190211_agg5_10\n",
      "2024-06-18 01:43:05,009 - INFO - Executing SQL query for date 20190212\n",
      "2024-06-18 01:43:39,498 - INFO - Data inserted into od_in_20190212_agg5_10\n",
      "2024-06-18 01:43:54,679 - INFO - Executing SQL query for date 20190213\n",
      "2024-06-18 01:44:31,851 - INFO - Data inserted into od_in_20190213_agg5_10\n",
      "2024-06-18 01:44:49,871 - INFO - Executing SQL query for date 20190214\n",
      "2024-06-18 01:45:32,332 - INFO - Data inserted into od_in_20190214_agg5_10\n",
      "2024-06-18 01:45:49,180 - INFO - Executing SQL query for date 20190215\n",
      "2024-06-18 01:46:30,888 - INFO - Data inserted into od_in_20190215_agg5_10\n",
      "2024-06-18 01:46:46,987 - INFO - Executing SQL query for date 20190216\n",
      "2024-06-18 01:47:28,690 - INFO - Data inserted into od_in_20190216_agg5_10\n",
      "2024-06-18 01:47:43,480 - INFO - Executing SQL query for date 20190217\n",
      "2024-06-18 01:48:18,189 - INFO - Data inserted into od_in_20190217_agg5_10\n",
      "2024-06-18 01:48:34,371 - INFO - Executing SQL query for date 20190218\n",
      "2024-06-18 01:49:17,177 - INFO - Data inserted into od_in_20190218_agg5_10\n",
      "2024-06-18 01:49:33,991 - INFO - Executing SQL query for date 20190219\n",
      "2024-06-18 01:50:17,794 - INFO - Data inserted into od_in_20190219_agg5_10\n",
      "2024-06-18 01:50:36,565 - INFO - Executing SQL query for date 20190220\n",
      "2024-06-18 01:51:23,154 - INFO - Data inserted into od_in_20190220_agg5_10\n",
      "2024-06-18 01:51:39,658 - INFO - Executing SQL query for date 20190221\n",
      "2024-06-18 01:52:25,286 - INFO - Data inserted into od_in_20190221_agg5_10\n",
      "2024-06-18 01:52:41,345 - INFO - Executing SQL query for date 20190222\n",
      "2024-06-18 01:53:27,381 - INFO - Data inserted into od_in_20190222_agg5_10\n",
      "2024-06-18 01:53:48,455 - INFO - Executing SQL query for date 20190223\n",
      "2024-06-18 01:54:33,177 - INFO - Data inserted into od_in_20190223_agg5_10\n",
      "2024-06-18 01:54:49,910 - INFO - Executing SQL query for date 20190224\n",
      "2024-06-18 01:55:29,651 - INFO - Data inserted into od_in_20190224_agg5_10\n",
      "2024-06-18 01:55:46,849 - INFO - Executing SQL query for date 20190225\n",
      "2024-06-18 01:56:31,344 - INFO - Data inserted into od_in_20190225_agg5_10\n",
      "2024-06-18 01:56:48,529 - INFO - Executing SQL query for date 20190226\n",
      "2024-06-18 01:57:35,438 - INFO - Data inserted into od_in_20190226_agg5_10\n",
      "2024-06-18 01:57:53,303 - INFO - Executing SQL query for date 20190227\n",
      "2024-06-18 01:58:40,536 - INFO - Data inserted into od_in_20190227_agg5_10\n",
      "2024-06-18 01:58:58,785 - INFO - Executing SQL query for date 20190228\n",
      "2024-06-18 01:59:48,434 - INFO - Data inserted into od_in_20190228_agg5_10\n",
      "2024-06-18 01:59:48,435 - INFO - Total processing time: 1612.00 seconds\n",
      "2024-06-18 01:59:48,435 - INFO - No errors encountered during processing.\n"
     ]
    }
   ],
   "source": [
    "process_date_range(datetime(2020, 1, 1), datetime(2020, 12, 31), 'CO', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738150b3-c8a1-46ec-ac37-c8a8e4b3a978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd762a9-1202-4601-a6f2-9d908a44e107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c59fc9-2b81-4658-af45-69100b9c43ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c003a02-68a9-46b8-8e01-23717c123282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d582c8a-8735-4726-b083-9f148d2b55c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07d5cc-2154-4160-9c19-c4978d930922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad6207-9c36-403e-a2c9-473609211506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a39b4-4e5d-4fec-8f6b-17cedaa74efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8083f-bb6f-4ca6-8008-3861ebd079b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ab54-7ea9-4f44-8d58-48a9c5ed510f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9950b-f813-4496-95ee-8a850ab07181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2f14e-fa1b-4810-a38d-e529d81569d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf17b0-d474-4e16-80a7-0538ca8d1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e4671-e3f7-4af4-92ec-49e7a9c8235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9771058-c43d-4cf3-97eb-3f91d2ca61ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df0a19-7477-4067-adef-72aa1d701faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb6f36-52ec-4aca-86d5-1fec273ea6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5ae12-830c-496c-be73-f527048603c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff191f4-b11e-4639-9c4e-502decf49696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d565d9-6119-467c-bef4-082acc505204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca5d0d-a0f6-43b0-af28-3d06ec9a23b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
