{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c3e060-58c8-4d7d-acdd-91b3fbd78201",
   "metadata": {},
   "source": [
    "This is the pop_density geohash 3 & 5 dataset for 2020 for ID  \n",
    "Please run the file by 3 months  \n",
    "It is set 20200101 - 20200331 for now, please change while submitting jobs  \n",
    "Please remember to check logging information to fillin data missing dates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from geohash2) (0.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e97370-6219-4f1c-8209-ff98251ec80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5231d678-d812-49aa-bc55-32d4da24edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'pop_density20'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'ID'\n",
    "start_date = 20200101\n",
    "end_date = 20200331\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ef2bd1-d4d2-43ae-b115-478ad96ecba6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 14:54:36,458 - INFO - Executing SQL query for date 20190714\n",
      "2024-06-07 15:00:39,752 - INFO - Inserted data into table pd_id_20190714_agg5\n",
      "2024-06-07 15:00:43,344 - INFO - Inserted data into table pd_id_20190714_agg3\n",
      "2024-06-07 15:00:43,345 - INFO - Data extraction, aggregation, and saving completed.\n",
      "2024-06-07 15:00:43,345 - INFO - Total time taken: 368.349977016449 seconds\n"
     ]
    }
   ],
   "source": [
    "# For ID - should be working\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    try:\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Construct the SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        \n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f050d-4541-4cc4-ac6d-dc770f6123d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b85b9-52ea-48b5-8f7b-aa01fab6a4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e78f49-0217-4c64-9cf2-9edf215fcf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699b124-a645-4ce0-bae3-7ec69fd06188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8ee5da-089b-47c0-8a5c-825bf7e3d539",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 15:03:51,092 - INFO - Executing SQL query for date 20190719\n",
      "2024-06-07 15:09:50,247 - INFO - Inserted data into table pd_id_20190719_agg5\n",
      "2024-06-07 15:09:53,109 - INFO - Inserted data into table pd_id_20190719_agg3\n",
      "2024-06-07 15:09:53,110 - INFO - Executing SQL query for date 20190731\n",
      "2024-06-07 15:16:15,481 - INFO - Inserted data into table pd_id_20190731_agg5\n",
      "2024-06-07 15:16:18,405 - INFO - Inserted data into table pd_id_20190731_agg3\n",
      "2024-06-07 15:16:18,405 - INFO - Executing SQL query for date 20190808\n",
      "2024-06-07 15:22:49,021 - INFO - Inserted data into table pd_id_20190808_agg5\n",
      "2024-06-07 15:22:52,046 - INFO - Inserted data into table pd_id_20190808_agg3\n",
      "2024-06-07 15:22:52,047 - INFO - Executing SQL query for date 20190817\n",
      "2024-06-07 15:30:57,505 - INFO - Inserted data into table pd_id_20190817_agg5\n",
      "2024-06-07 15:31:00,979 - INFO - Inserted data into table pd_id_20190817_agg3\n",
      "2024-06-07 15:31:00,980 - INFO - Executing SQL query for date 20191020\n",
      "2024-06-07 15:42:25,446 - INFO - Inserted data into table pd_id_20191020_agg5\n",
      "2024-06-07 15:42:29,449 - INFO - Inserted data into table pd_id_20191020_agg3\n",
      "2024-06-07 15:42:29,450 - INFO - Executing SQL query for date 20191021\n",
      "2024-06-07 15:52:46,530 - INFO - Inserted data into table pd_id_20191021_agg5\n",
      "2024-06-07 15:52:50,220 - INFO - Inserted data into table pd_id_20191021_agg3\n",
      "2024-06-07 15:52:50,220 - INFO - Data extraction, aggregation, and saving completed.\n",
      "2024-06-07 15:52:50,221 - INFO - Total time taken: 2939.129183769226 seconds\n"
     ]
    }
   ],
   "source": [
    "# # Filling missing date gaps for certain dates\n",
    "\n",
    "# import logging\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "# import pandas as pd\n",
    "# import geohash2\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Database connection setup\n",
    "# output_schema_name = 'pop_density'\n",
    "# con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# start_time = time.time()  # Start timing\n",
    "\n",
    "# # Define the input parameters\n",
    "# country_code = 'ID'\n",
    "# date_list = ['20190719','20190731', '20190808', '20190817', '20191020', '20191021']\n",
    "\n",
    "# # Define the input schema and table name\n",
    "# schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "# pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# # Function to insert data with retry mechanism\n",
    "# def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "#     for attempt in range(retries):\n",
    "#         try:\n",
    "#             df.to_sql(\n",
    "#                 table_name, \n",
    "#                 con, \n",
    "#                 index=False, \n",
    "#                 if_exists=\"append\", \n",
    "#                 method=\"multi\"\n",
    "#             )\n",
    "#             logging.info(f\"Inserted data into table {table_name}\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "#             if attempt < retries - 1:\n",
    "#                 time.sleep(delay)\n",
    "#             else:\n",
    "#                 logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# # Loop through each date in the date_list\n",
    "# for date_str in date_list:\n",
    "#     try:\n",
    "#         # Convert date string to datetime object\n",
    "#         current_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "        \n",
    "#         # Calculate the lookback and lookahead dates\n",
    "#         lookback_date = current_date - timedelta(days=1)\n",
    "#         lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "#         # Format dates for the SQL query\n",
    "#         formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "#         formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "#         formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "#         # Construct the SQL query\n",
    "#         query = f\"\"\"\n",
    "#         SELECT \n",
    "#             cuebiq_id, \n",
    "#             event_zoned_datetime, \n",
    "#             processing_date,\n",
    "#             lat,\n",
    "#             lng\n",
    "#         FROM {pe_dl_table}\n",
    "#         WHERE \n",
    "#             processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "#             AND country_code = '{country_code}' \n",
    "#             AND event_zoned_datetime IS NOT NULL\n",
    "#             AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#             AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "#         \"\"\"\n",
    "        \n",
    "#         logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        \n",
    "#         pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "#         # Convert the generator to a DataFrame\n",
    "#         chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "#         if chunks:\n",
    "#             pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "#             # Calculate geohashes\n",
    "#             pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "#             pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "#             # Aggregate data for geohash5\n",
    "#             aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "#                 no_of_points=('geohash5', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "#             ).reset_index()\n",
    "\n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Add the local_date column\n",
    "#             filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "#             # Insert filtered aggregated data for geohash5 into SQL table\n",
    "#             if not filtered_data_5.empty:\n",
    "#                 table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "#                 insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "#             # Aggregate data for geohash3\n",
    "#             aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "#                 no_of_points=('geohash3', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Add the local_date column\n",
    "#             filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "#             # Insert filtered aggregated data for geohash3 into SQL table\n",
    "#             if not filtered_data_3.empty:\n",
    "#                 table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "#                 insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "\n",
    "# logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "# end_time = time.time()  # End timing\n",
    "\n",
    "# # Calculate and print the total time taken\n",
    "# total_time = end_time - start_time\n",
    "# logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e69b3-8d15-4d0a-af60-eeee6c1d0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805048e-4e87-4a49-bee2-118bb8758578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ffbd7-0397-46f5-8a17-9c7f8432d081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28251e7e-fae0-4f5f-bf08-b61bbcff553a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64023c-2208-4449-8f72-bf39363a465b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50916e84-52a0-4694-b662-0f4b554c60dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71e37f-1292-4161-8bbf-b5fdc1114214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
