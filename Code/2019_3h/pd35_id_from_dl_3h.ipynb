{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from geohash2) (0.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a044108c-c3e8-4325-8f00-6fcbe62ea69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277f112-2f83-42fc-9b8c-601e7b6f5536",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export to jupyter notebook\n",
    "should be working for CO, ID, IN, MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6543700-d70a-4f86-b7a2-6f8f7c58fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to determine the 3-hour interval based on a given date\n",
    "# def get_3_hour_interval(start_hour, current_date):\n",
    "#     start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "#     end_time = start_time + pd.Timedelta(hours=3)\n",
    "#     return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a883a0ca-4e38-476c-843f-8d41e0b28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = '2019-11-20'\n",
    "end_date = '2019-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'ID'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "# export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_path = '/home/jovyan/Data/pd3/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n",
    "\n",
    "# List to record errors\n",
    "error_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381a613-35a2-4ea3-8ab0-3215806ca7d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 06:35:37,576 - INFO - Executing SQL query for date 20191120 and interval 0 to 3\n",
      "2024-07-03 06:36:52,867 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 06:36:53,247 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 06:36:53,276 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 06:36:53,603 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 06:36:53,614 - INFO - Appended data for date 20191120 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:36:53,615 - INFO - Executing SQL query for date 20191120 and interval 3 to 6\n",
      "2024-07-03 06:38:04,985 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 06:38:05,147 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 06:38:05,165 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 06:38:05,304 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 06:38:05,318 - INFO - Appended data for date 20191120 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:38:05,319 - INFO - Executing SQL query for date 20191120 and interval 6 to 9\n",
      "2024-07-03 06:40:31,996 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 06:40:32,450 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 06:40:32,483 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 06:40:32,898 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 06:40:32,909 - INFO - Appended data for date 20191120 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:40:32,910 - INFO - Executing SQL query for date 20191120 and interval 9 to 12\n",
      "2024-07-03 06:42:43,095 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 06:42:43,431 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 06:42:43,451 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 06:42:43,758 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 06:42:43,768 - INFO - Appended data for date 20191120 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:42:43,769 - INFO - Executing SQL query for date 20191120 and interval 12 to 15\n",
      "2024-07-03 06:45:17,091 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 06:45:17,460 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 06:45:17,478 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 06:45:17,792 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 06:45:17,802 - INFO - Appended data for date 20191120 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:45:17,803 - INFO - Executing SQL query for date 20191120 and interval 15 to 18\n",
      "2024-07-03 06:47:35,496 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 06:47:36,081 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 06:47:36,103 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 06:47:36,601 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 06:47:36,612 - INFO - Appended data for date 20191120 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:47:36,613 - INFO - Executing SQL query for date 20191120 and interval 18 to 21\n",
      "2024-07-03 06:49:51,180 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 06:49:51,616 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 06:49:51,637 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 06:49:52,019 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 06:49:52,031 - INFO - Appended data for date 20191120 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:49:52,031 - INFO - Executing SQL query for date 20191120 and interval 21 to 24\n",
      "2024-07-03 06:52:13,768 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 06:52:14,119 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 06:52:14,138 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 06:52:14,466 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 06:52:14,477 - INFO - Appended data for date 20191120 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:52:14,477 - INFO - Executing SQL query for date 20191121 and interval 0 to 3\n",
      "2024-07-03 06:53:28,020 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 06:53:28,204 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 06:53:28,219 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 06:53:28,386 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 06:53:28,397 - INFO - Appended data for date 20191121 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:53:28,398 - INFO - Executing SQL query for date 20191121 and interval 3 to 6\n",
      "2024-07-03 06:55:36,255 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 06:55:36,420 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 06:55:36,436 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 06:55:36,596 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 06:55:36,609 - INFO - Appended data for date 20191121 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:55:36,610 - INFO - Executing SQL query for date 20191121 and interval 6 to 9\n",
      "2024-07-03 06:58:25,670 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 06:58:26,020 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 06:58:26,057 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 06:58:26,356 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 06:58:26,367 - INFO - Appended data for date 20191121 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:58:26,367 - INFO - Executing SQL query for date 20191121 and interval 9 to 12\n",
      "2024-07-03 06:59:51,716 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 06:59:52,061 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 06:59:52,084 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 06:59:52,383 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 06:59:52,396 - INFO - Appended data for date 20191121 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 06:59:52,396 - INFO - Executing SQL query for date 20191121 and interval 12 to 15\n",
      "2024-07-03 07:02:15,238 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:02:15,613 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:02:15,632 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:02:15,958 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:02:15,969 - INFO - Appended data for date 20191121 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:02:15,970 - INFO - Executing SQL query for date 20191121 and interval 15 to 18\n",
      "2024-07-03 07:05:31,293 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:05:32,496 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:05:32,578 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:05:33,679 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:05:33,693 - INFO - Appended data for date 20191121 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:05:33,694 - INFO - Executing SQL query for date 20191121 and interval 18 to 21\n",
      "2024-07-03 07:07:02,285 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 07:07:02,746 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:07:02,765 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 07:07:03,145 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:07:03,155 - INFO - Appended data for date 20191121 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:07:03,156 - INFO - Executing SQL query for date 20191121 and interval 21 to 24\n",
      "2024-07-03 07:09:24,686 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:09:25,023 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:09:25,041 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:09:25,329 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:09:25,341 - INFO - Appended data for date 20191121 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:09:25,342 - INFO - Executing SQL query for date 20191122 and interval 0 to 3\n",
      "2024-07-03 07:11:16,015 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:11:16,298 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:11:16,318 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:11:16,584 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:11:16,595 - INFO - Appended data for date 20191122 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:11:16,596 - INFO - Executing SQL query for date 20191122 and interval 3 to 6\n",
      "2024-07-03 07:12:19,689 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 07:12:20,009 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:12:20,074 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 07:12:20,379 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:12:20,391 - INFO - Appended data for date 20191122 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:12:20,391 - INFO - Executing SQL query for date 20191122 and interval 6 to 9\n",
      "2024-07-03 07:14:03,939 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 07:14:04,261 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:14:04,284 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 07:14:04,568 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:14:04,580 - INFO - Appended data for date 20191122 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:14:04,581 - INFO - Executing SQL query for date 20191122 and interval 9 to 12\n",
      "2024-07-03 07:16:38,849 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 07:16:39,195 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:16:39,213 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 07:16:39,521 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:16:39,531 - INFO - Appended data for date 20191122 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:16:39,532 - INFO - Executing SQL query for date 20191122 and interval 12 to 15\n",
      "2024-07-03 07:18:07,867 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:18:08,977 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:18:09,004 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:18:09,774 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:18:09,787 - INFO - Appended data for date 20191122 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:18:09,788 - INFO - Executing SQL query for date 20191122 and interval 15 to 18\n",
      "2024-07-03 07:19:38,897 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:19:39,513 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:19:39,535 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:19:40,087 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:19:40,103 - INFO - Appended data for date 20191122 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:19:40,104 - INFO - Executing SQL query for date 20191122 and interval 18 to 21\n",
      "2024-07-03 07:20:21,742 - INFO - failed after 3 attempts\n",
      "2024-07-03 07:20:23,842 - INFO - failed after 3 attempts\n",
      "2024-07-03 07:20:23,843 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 501, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 821, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 838, in fetch\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 615, in cancel\n",
      "    self._query.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 862, in cancel\n",
      "    self._request.raise_response_error(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "2024-07-03 07:20:23,846 - ERROR - Error while processing data for date 20191122 interval 18 to 21: error 502: bad gateway\n",
      "2024-07-03 07:20:23,846 - INFO - Executing SQL query for date 20191122 and interval 21 to 24\n",
      "2024-07-03 07:20:24,514 - INFO - failed after 3 attempts\n",
      "2024-07-03 07:20:24,515 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 482, in execute\n",
      "    self._prepare_statement(operation, statement_name)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 379, in _prepare_statement\n",
      "    query.execute()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 807, in execute\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 612, in cancel\n",
      "    raise trino.exceptions.OperationalError(\n",
      "trino.exceptions.OperationalError: Cancel query failed; no running query\n",
      "2024-07-03 07:23:08,189 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:23:08,522 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:23:08,541 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:23:08,832 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:23:08,844 - INFO - Appended data for date 20191122 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:23:08,845 - INFO - Executing SQL query for date 20191123 and interval 0 to 3\n",
      "2024-07-03 07:24:34,429 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:24:34,800 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:24:34,820 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:24:35,068 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:24:35,080 - INFO - Appended data for date 20191123 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:24:35,080 - INFO - Executing SQL query for date 20191123 and interval 3 to 6\n",
      "2024-07-03 07:25:30,787 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 07:25:30,944 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:25:30,961 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 07:25:31,093 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:25:31,103 - INFO - Appended data for date 20191123 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:25:31,104 - INFO - Executing SQL query for date 20191123 and interval 6 to 9\n",
      "2024-07-03 07:27:03,450 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 07:27:03,789 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:27:03,808 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 07:27:04,106 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:27:04,118 - INFO - Appended data for date 20191123 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:27:04,119 - INFO - Executing SQL query for date 20191123 and interval 9 to 12\n",
      "2024-07-03 07:28:09,182 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 07:28:09,545 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:28:09,565 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 07:28:09,901 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:28:09,914 - INFO - Appended data for date 20191123 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:28:09,915 - INFO - Executing SQL query for date 20191123 and interval 12 to 15\n",
      "2024-07-03 07:29:48,692 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:29:49,056 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:29:49,077 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:29:49,393 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:29:49,404 - INFO - Appended data for date 20191123 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:29:49,405 - INFO - Executing SQL query for date 20191123 and interval 15 to 18\n",
      "2024-07-03 07:31:04,301 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:31:04,914 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:31:04,978 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:31:05,431 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:31:05,442 - INFO - Appended data for date 20191123 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:31:05,443 - INFO - Executing SQL query for date 20191123 and interval 18 to 21\n",
      "2024-07-03 07:32:16,497 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 07:32:16,939 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:32:16,958 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 07:32:17,322 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:32:17,334 - INFO - Appended data for date 20191123 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:32:17,335 - INFO - Executing SQL query for date 20191123 and interval 21 to 24\n",
      "2024-07-03 07:34:12,887 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:34:13,579 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:34:13,612 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:34:14,207 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:34:14,219 - INFO - Appended data for date 20191123 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:34:14,220 - INFO - Executing SQL query for date 20191124 and interval 0 to 3\n",
      "2024-07-03 07:35:06,091 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:35:06,775 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:35:06,793 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:35:07,382 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:35:07,394 - INFO - Appended data for date 20191124 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:35:07,395 - INFO - Executing SQL query for date 20191124 and interval 3 to 6\n",
      "2024-07-03 07:35:58,445 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 07:35:58,602 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:35:58,620 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 07:35:58,747 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:35:58,758 - INFO - Appended data for date 20191124 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:35:58,759 - INFO - Executing SQL query for date 20191124 and interval 6 to 9\n",
      "2024-07-03 07:37:23,134 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 07:37:23,402 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:37:23,419 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 07:37:23,646 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:37:23,658 - INFO - Appended data for date 20191124 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:37:23,659 - INFO - Executing SQL query for date 20191124 and interval 9 to 12\n",
      "2024-07-03 07:38:28,552 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 07:38:28,911 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:38:28,931 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 07:38:29,238 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:38:29,249 - INFO - Appended data for date 20191124 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:38:29,250 - INFO - Executing SQL query for date 20191124 and interval 12 to 15\n",
      "2024-07-03 07:39:28,557 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:39:28,881 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:39:28,901 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:39:29,195 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:39:29,208 - INFO - Appended data for date 20191124 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:39:29,209 - INFO - Executing SQL query for date 20191124 and interval 15 to 18\n",
      "2024-07-03 07:41:09,170 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:41:09,546 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:41:09,576 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:41:09,957 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:41:09,978 - INFO - Appended data for date 20191124 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:41:09,979 - INFO - Executing SQL query for date 20191124 and interval 18 to 21\n",
      "2024-07-03 07:42:40,625 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 07:42:41,288 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:42:41,315 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 07:42:41,777 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:42:41,789 - INFO - Appended data for date 20191124 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:42:41,790 - INFO - Executing SQL query for date 20191124 and interval 21 to 24\n",
      "2024-07-03 07:43:43,535 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:43:43,969 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:43:43,991 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:43:44,350 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:43:44,374 - INFO - Appended data for date 20191124 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:43:44,376 - INFO - Executing SQL query for date 20191125 and interval 0 to 3\n",
      "2024-07-03 07:45:15,743 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:45:15,896 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:45:15,916 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:45:16,044 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:45:16,055 - INFO - Appended data for date 20191125 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:45:16,056 - INFO - Executing SQL query for date 20191125 and interval 3 to 6\n",
      "2024-07-03 07:46:00,087 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 07:46:00,483 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:46:00,498 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 07:46:00,796 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:46:00,809 - INFO - Appended data for date 20191125 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:46:00,810 - INFO - Executing SQL query for date 20191125 and interval 6 to 9\n",
      "2024-07-03 07:46:55,824 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 07:46:56,092 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:46:56,111 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 07:46:56,329 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:46:56,354 - INFO - Appended data for date 20191125 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:46:56,355 - INFO - Executing SQL query for date 20191125 and interval 9 to 12\n",
      "2024-07-03 07:47:47,717 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 07:47:47,963 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:47:47,982 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 07:47:48,204 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:47:48,215 - INFO - Appended data for date 20191125 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:47:48,216 - INFO - Executing SQL query for date 20191125 and interval 12 to 15\n",
      "2024-07-03 07:49:27,983 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:49:28,296 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:49:28,312 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:49:28,539 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:49:28,549 - INFO - Appended data for date 20191125 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:49:28,550 - INFO - Executing SQL query for date 20191125 and interval 15 to 18\n",
      "2024-07-03 07:50:13,102 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:50:13,420 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:50:13,435 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:50:13,702 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:50:13,713 - INFO - Appended data for date 20191125 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:50:13,713 - INFO - Executing SQL query for date 20191125 and interval 18 to 21\n",
      "2024-07-03 07:51:07,247 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 07:51:07,410 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:51:07,424 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 07:51:07,548 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:51:07,558 - INFO - Appended data for date 20191125 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:51:07,559 - INFO - Executing SQL query for date 20191125 and interval 21 to 24\n",
      "2024-07-03 07:52:16,247 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:52:16,347 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:52:16,362 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:52:16,456 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:52:16,468 - INFO - Appended data for date 20191125 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:52:16,469 - INFO - Executing SQL query for date 20191126 and interval 0 to 3\n",
      "2024-07-03 07:52:53,465 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:52:53,577 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:52:53,590 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:52:53,708 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:52:53,773 - INFO - Appended data for date 20191126 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:52:53,774 - INFO - Executing SQL query for date 20191126 and interval 3 to 6\n",
      "2024-07-03 07:53:35,641 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 07:53:35,696 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:53:35,709 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 07:53:35,756 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 07:53:35,767 - INFO - Appended data for date 20191126 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:53:35,767 - INFO - Executing SQL query for date 20191126 and interval 6 to 9\n",
      "2024-07-03 07:54:53,198 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 07:54:53,317 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:54:53,331 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 07:54:53,430 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 07:54:53,440 - INFO - Appended data for date 20191126 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:54:53,441 - INFO - Executing SQL query for date 20191126 and interval 9 to 12\n",
      "2024-07-03 07:55:43,271 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 07:55:43,405 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:55:43,421 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 07:55:43,575 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 07:55:43,585 - INFO - Appended data for date 20191126 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:55:43,586 - INFO - Executing SQL query for date 20191126 and interval 12 to 15\n",
      "2024-07-03 07:56:29,796 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 07:56:30,030 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:56:30,073 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 07:56:30,296 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 07:56:30,308 - INFO - Appended data for date 20191126 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:56:30,309 - INFO - Executing SQL query for date 20191126 and interval 15 to 18\n",
      "2024-07-03 07:57:23,289 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 07:57:23,430 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:57:23,443 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 07:57:23,562 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 07:57:23,575 - INFO - Appended data for date 20191126 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:57:23,576 - INFO - Executing SQL query for date 20191126 and interval 18 to 21\n",
      "2024-07-03 07:58:33,316 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 07:58:33,475 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:58:33,490 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 07:58:33,629 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 07:58:33,639 - INFO - Appended data for date 20191126 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:58:33,640 - INFO - Executing SQL query for date 20191126 and interval 21 to 24\n",
      "2024-07-03 07:59:12,949 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 07:59:13,053 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:59:13,066 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 07:59:13,152 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 07:59:13,161 - INFO - Appended data for date 20191126 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:59:13,164 - INFO - Executing SQL query for date 20191127 and interval 0 to 3\n",
      "2024-07-03 07:59:45,781 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 07:59:45,889 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:59:45,904 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 07:59:46,013 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 07:59:46,024 - INFO - Appended data for date 20191127 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 07:59:46,025 - INFO - Executing SQL query for date 20191127 and interval 3 to 6\n",
      "2024-07-03 08:00:18,799 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:00:18,966 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:00:18,978 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:00:19,076 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:00:19,087 - INFO - Appended data for date 20191127 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:00:19,088 - INFO - Executing SQL query for date 20191127 and interval 6 to 9\n",
      "2024-07-03 08:01:18,153 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:01:18,272 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:01:18,287 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:01:18,385 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:01:18,397 - INFO - Appended data for date 20191127 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:01:18,397 - INFO - Executing SQL query for date 20191127 and interval 9 to 12\n",
      "2024-07-03 08:02:38,719 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:02:38,850 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:02:38,864 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:02:38,975 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:02:38,988 - INFO - Appended data for date 20191127 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:02:38,989 - INFO - Executing SQL query for date 20191127 and interval 12 to 15\n",
      "2024-07-03 08:03:23,402 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:03:23,570 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:03:23,584 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:03:23,721 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:03:23,733 - INFO - Appended data for date 20191127 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:03:23,734 - INFO - Executing SQL query for date 20191127 and interval 15 to 18\n",
      "2024-07-03 08:04:13,268 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:04:13,586 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:04:13,601 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:04:13,964 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:04:13,997 - INFO - Appended data for date 20191127 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:04:13,998 - INFO - Executing SQL query for date 20191127 and interval 18 to 21\n",
      "2024-07-03 08:04:51,693 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:04:51,890 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:04:51,903 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:04:52,080 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:04:52,092 - INFO - Appended data for date 20191127 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:04:52,093 - INFO - Executing SQL query for date 20191127 and interval 21 to 24\n",
      "2024-07-03 08:06:14,961 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:06:15,060 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:06:15,074 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:06:15,160 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:06:15,171 - INFO - Appended data for date 20191127 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:06:15,172 - INFO - Executing SQL query for date 20191128 and interval 0 to 3\n",
      "2024-07-03 08:07:08,113 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:07:08,223 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:07:08,236 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:07:08,307 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:07:08,318 - INFO - Appended data for date 20191128 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:07:08,318 - INFO - Executing SQL query for date 20191128 and interval 3 to 6\n",
      "2024-07-03 08:07:35,598 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:07:35,653 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:07:35,667 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:07:35,714 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:07:35,724 - INFO - Appended data for date 20191128 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:07:35,725 - INFO - Executing SQL query for date 20191128 and interval 6 to 9\n",
      "2024-07-03 08:08:18,533 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:08:18,679 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:08:18,703 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:08:18,809 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:08:18,822 - INFO - Appended data for date 20191128 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:08:18,823 - INFO - Executing SQL query for date 20191128 and interval 9 to 12\n",
      "2024-07-03 08:09:42,430 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:09:42,565 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:09:42,581 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:09:42,693 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:09:42,705 - INFO - Appended data for date 20191128 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:09:42,706 - INFO - Executing SQL query for date 20191128 and interval 12 to 15\n",
      "2024-07-03 08:10:36,107 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:10:36,293 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:10:36,306 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:10:36,422 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:10:36,434 - INFO - Appended data for date 20191128 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:10:36,435 - INFO - Executing SQL query for date 20191128 and interval 15 to 18\n",
      "2024-07-03 08:11:19,105 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:11:19,402 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:11:19,417 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:11:19,692 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:11:19,703 - INFO - Appended data for date 20191128 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:11:19,704 - INFO - Executing SQL query for date 20191128 and interval 18 to 21\n",
      "2024-07-03 08:12:01,823 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:12:01,991 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:12:02,005 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:12:02,150 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:12:02,161 - INFO - Appended data for date 20191128 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:12:02,161 - INFO - Executing SQL query for date 20191128 and interval 21 to 24\n",
      "2024-07-03 08:13:23,957 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:13:24,065 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:13:24,079 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:13:24,176 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:13:24,187 - INFO - Appended data for date 20191128 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:13:24,188 - INFO - Executing SQL query for date 20191129 and interval 0 to 3\n",
      "2024-07-03 08:14:08,870 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:14:08,928 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:14:08,943 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:14:09,001 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:14:09,019 - INFO - Appended data for date 20191129 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:14:09,020 - INFO - Executing SQL query for date 20191129 and interval 3 to 6\n",
      "2024-07-03 08:14:46,419 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:14:46,501 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:14:46,512 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:14:46,561 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:14:46,572 - INFO - Appended data for date 20191129 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:14:46,573 - INFO - Executing SQL query for date 20191129 and interval 6 to 9\n",
      "2024-07-03 08:15:22,775 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:15:22,900 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:15:22,915 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:15:23,020 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:15:23,031 - INFO - Appended data for date 20191129 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:15:23,032 - INFO - Executing SQL query for date 20191129 and interval 9 to 12\n",
      "2024-07-03 08:16:45,081 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:16:45,211 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:16:45,225 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:16:45,334 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:16:45,344 - INFO - Appended data for date 20191129 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:16:45,345 - INFO - Executing SQL query for date 20191129 and interval 12 to 15\n",
      "2024-07-03 08:17:40,592 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:17:40,771 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:17:40,788 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:17:40,917 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:17:40,928 - INFO - Appended data for date 20191129 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:17:40,929 - INFO - Executing SQL query for date 20191129 and interval 15 to 18\n",
      "2024-07-03 08:18:34,014 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:18:34,171 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:18:34,188 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:18:34,315 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:18:34,327 - INFO - Appended data for date 20191129 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:18:34,328 - INFO - Executing SQL query for date 20191129 and interval 18 to 21\n",
      "2024-07-03 08:19:49,025 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:19:49,181 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:19:49,196 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:19:49,330 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:19:49,345 - INFO - Appended data for date 20191129 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:19:49,346 - INFO - Executing SQL query for date 20191129 and interval 21 to 24\n",
      "2024-07-03 08:20:20,136 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:20:20,279 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:20:20,297 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:20:20,399 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:20:20,410 - INFO - Appended data for date 20191129 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:20:20,411 - INFO - Executing SQL query for date 20191130 and interval 0 to 3\n",
      "2024-07-03 08:21:44,394 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:21:44,455 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:21:44,468 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:21:44,518 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:21:44,532 - INFO - Appended data for date 20191130 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:21:44,533 - INFO - Executing SQL query for date 20191130 and interval 3 to 6\n",
      "2024-07-03 08:22:29,494 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:22:29,591 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:22:29,604 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:22:29,694 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:22:29,707 - INFO - Appended data for date 20191130 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:22:29,708 - INFO - Executing SQL query for date 20191130 and interval 6 to 9\n",
      "2024-07-03 08:23:09,173 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:23:09,295 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:23:09,308 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:23:09,435 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:23:09,477 - INFO - Appended data for date 20191130 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:23:09,478 - INFO - Executing SQL query for date 20191130 and interval 9 to 12\n",
      "2024-07-03 08:23:58,216 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:23:58,374 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:23:58,391 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:23:58,548 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:23:58,559 - INFO - Appended data for date 20191130 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:23:58,560 - INFO - Executing SQL query for date 20191130 and interval 12 to 15\n",
      "2024-07-03 08:25:11,800 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:25:12,005 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:25:12,022 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:25:12,218 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:25:12,235 - INFO - Appended data for date 20191130 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:25:12,236 - INFO - Executing SQL query for date 20191130 and interval 15 to 18\n",
      "2024-07-03 08:25:59,995 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:26:00,139 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:26:00,157 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:26:00,282 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:26:00,293 - INFO - Appended data for date 20191130 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:26:00,294 - INFO - Executing SQL query for date 20191130 and interval 18 to 21\n",
      "2024-07-03 08:26:41,844 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:26:42,014 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:26:42,028 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:26:42,155 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:26:42,166 - INFO - Appended data for date 20191130 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:26:42,167 - INFO - Executing SQL query for date 20191130 and interval 21 to 24\n",
      "2024-07-03 08:28:08,837 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:28:08,954 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:28:08,971 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:28:09,069 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:28:09,081 - INFO - Appended data for date 20191130 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:28:09,081 - INFO - Executing SQL query for date 20191201 and interval 0 to 3\n",
      "2024-07-03 08:28:55,337 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:28:55,412 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:28:55,426 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:28:55,481 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:28:55,493 - INFO - Appended data for date 20191201 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:28:55,493 - INFO - Executing SQL query for date 20191201 and interval 3 to 6\n",
      "2024-07-03 08:29:25,627 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:29:25,710 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:29:25,723 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:29:25,783 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:29:25,797 - INFO - Appended data for date 20191201 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:29:25,797 - INFO - Executing SQL query for date 20191201 and interval 6 to 9\n",
      "2024-07-03 08:30:00,164 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:30:00,315 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:30:00,330 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:30:00,446 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:30:00,459 - INFO - Appended data for date 20191201 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:30:00,460 - INFO - Executing SQL query for date 20191201 and interval 9 to 12\n",
      "2024-07-03 08:30:37,204 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:30:37,475 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:30:37,492 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:30:37,711 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:30:37,731 - INFO - Appended data for date 20191201 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:30:37,732 - INFO - Executing SQL query for date 20191201 and interval 12 to 15\n",
      "2024-07-03 08:31:27,636 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:31:27,777 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:31:27,792 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:31:27,912 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:31:27,930 - INFO - Appended data for date 20191201 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:31:27,931 - INFO - Executing SQL query for date 20191201 and interval 15 to 18\n",
      "2024-07-03 08:32:46,285 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:32:46,429 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:32:46,444 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:32:46,565 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:32:46,575 - INFO - Appended data for date 20191201 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:32:46,576 - INFO - Executing SQL query for date 20191201 and interval 18 to 21\n",
      "2024-07-03 08:33:28,441 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:33:28,714 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:33:28,735 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:33:28,931 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:33:28,941 - INFO - Appended data for date 20191201 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:33:28,942 - INFO - Executing SQL query for date 20191201 and interval 21 to 24\n",
      "2024-07-03 08:34:03,434 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:34:03,541 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:34:03,557 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:34:03,686 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:34:03,699 - INFO - Appended data for date 20191201 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:34:03,700 - INFO - Executing SQL query for date 20191202 and interval 0 to 3\n",
      "2024-07-03 08:34:40,327 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:34:40,389 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:34:40,404 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:34:40,470 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:34:40,481 - INFO - Appended data for date 20191202 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:34:40,482 - INFO - Executing SQL query for date 20191202 and interval 3 to 6\n",
      "2024-07-03 08:35:53,574 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:35:53,674 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:35:53,689 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:35:53,763 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:35:53,775 - INFO - Appended data for date 20191202 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:35:53,776 - INFO - Executing SQL query for date 20191202 and interval 6 to 9\n",
      "2024-07-03 08:36:46,130 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:36:46,271 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:36:46,288 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:36:46,398 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:36:46,408 - INFO - Appended data for date 20191202 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:36:46,409 - INFO - Executing SQL query for date 20191202 and interval 9 to 12\n",
      "2024-07-03 08:37:27,025 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:37:27,178 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:37:27,192 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:37:27,314 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:37:27,328 - INFO - Appended data for date 20191202 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:37:27,328 - INFO - Executing SQL query for date 20191202 and interval 12 to 15\n",
      "2024-07-03 08:38:11,947 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:38:12,086 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:38:12,101 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:38:12,216 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:38:12,229 - INFO - Appended data for date 20191202 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:38:12,230 - INFO - Executing SQL query for date 20191202 and interval 15 to 18\n",
      "2024-07-03 08:39:34,951 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:39:35,087 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:39:35,102 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:39:35,218 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:39:35,229 - INFO - Appended data for date 20191202 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:39:35,230 - INFO - Executing SQL query for date 20191202 and interval 18 to 21\n",
      "2024-07-03 08:40:35,524 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:40:35,701 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:40:35,716 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:40:35,839 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:40:35,851 - INFO - Appended data for date 20191202 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:40:35,864 - INFO - Executing SQL query for date 20191202 and interval 21 to 24\n",
      "2024-07-03 08:41:09,792 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:41:09,909 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:41:09,923 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:41:10,076 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:41:10,096 - INFO - Appended data for date 20191202 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:41:10,097 - INFO - Executing SQL query for date 20191203 and interval 0 to 3\n",
      "2024-07-03 08:41:53,465 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:41:53,523 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:41:53,540 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:41:53,588 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:41:53,600 - INFO - Appended data for date 20191203 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:41:53,601 - INFO - Executing SQL query for date 20191203 and interval 3 to 6\n",
      "2024-07-03 08:43:02,389 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:43:02,448 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:43:02,466 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:43:02,514 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:43:02,525 - INFO - Appended data for date 20191203 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:43:02,526 - INFO - Executing SQL query for date 20191203 and interval 6 to 9\n",
      "2024-07-03 08:43:43,656 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:43:43,789 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:43:43,808 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:43:43,908 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:43:43,920 - INFO - Appended data for date 20191203 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:43:43,921 - INFO - Executing SQL query for date 20191203 and interval 9 to 12\n",
      "2024-07-03 08:44:28,238 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:44:28,428 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:44:28,447 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:44:28,582 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:44:28,598 - INFO - Appended data for date 20191203 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:44:28,599 - INFO - Executing SQL query for date 20191203 and interval 12 to 15\n",
      "2024-07-03 08:45:38,364 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:45:38,570 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:45:38,585 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:45:38,765 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:45:38,780 - INFO - Appended data for date 20191203 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:45:38,781 - INFO - Executing SQL query for date 20191203 and interval 15 to 18\n",
      "2024-07-03 08:46:21,924 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:46:22,062 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:46:22,076 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:46:22,195 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:46:22,207 - INFO - Appended data for date 20191203 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:46:22,207 - INFO - Executing SQL query for date 20191203 and interval 18 to 21\n",
      "2024-07-03 08:47:47,096 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:47:47,270 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:47:47,284 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:47:47,438 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:47:47,464 - INFO - Appended data for date 20191203 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:47:47,464 - INFO - Executing SQL query for date 20191203 and interval 21 to 24\n",
      "2024-07-03 08:48:46,903 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:48:47,186 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:48:47,209 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:48:47,466 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:48:47,485 - INFO - Appended data for date 20191203 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:48:47,486 - INFO - Executing SQL query for date 20191204 and interval 0 to 3\n",
      "2024-07-03 08:49:23,076 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:49:23,136 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:49:23,150 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:49:23,207 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:49:23,219 - INFO - Appended data for date 20191204 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:49:23,220 - INFO - Executing SQL query for date 20191204 and interval 3 to 6\n",
      "2024-07-03 08:50:12,904 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:50:12,967 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:50:12,986 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:50:13,045 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:50:13,056 - INFO - Appended data for date 20191204 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:50:13,057 - INFO - Executing SQL query for date 20191204 and interval 6 to 9\n",
      "2024-07-03 08:51:30,056 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:51:30,174 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:51:30,191 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:51:30,289 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:51:30,301 - INFO - Appended data for date 20191204 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:51:30,302 - INFO - Executing SQL query for date 20191204 and interval 9 to 12\n",
      "2024-07-03 08:52:13,817 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:52:14,010 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:52:14,025 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:52:14,143 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:52:14,154 - INFO - Appended data for date 20191204 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:52:14,155 - INFO - Executing SQL query for date 20191204 and interval 12 to 15\n",
      "2024-07-03 08:52:57,988 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 08:52:58,295 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:52:58,311 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 08:52:58,572 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 08:52:58,586 - INFO - Appended data for date 20191204 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:52:58,587 - INFO - Executing SQL query for date 20191204 and interval 15 to 18\n",
      "2024-07-03 08:53:52,771 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 08:53:52,923 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:53:52,936 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 08:53:53,055 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 08:53:53,067 - INFO - Appended data for date 20191204 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:53:53,068 - INFO - Executing SQL query for date 20191204 and interval 18 to 21\n",
      "2024-07-03 08:55:11,080 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 08:55:11,285 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:55:11,304 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 08:55:11,475 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 08:55:11,486 - INFO - Appended data for date 20191204 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:55:11,487 - INFO - Executing SQL query for date 20191204 and interval 21 to 24\n",
      "2024-07-03 08:55:49,010 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 08:55:49,123 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:55:49,136 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 08:55:49,221 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 08:55:49,233 - INFO - Appended data for date 20191204 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:55:49,234 - INFO - Executing SQL query for date 20191205 and interval 0 to 3\n",
      "2024-07-03 08:56:27,840 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 08:56:27,931 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:56:27,945 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 08:56:28,005 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 08:56:28,019 - INFO - Appended data for date 20191205 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:56:28,020 - INFO - Executing SQL query for date 20191205 and interval 3 to 6\n",
      "2024-07-03 08:57:05,974 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 08:57:06,094 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:57:06,124 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 08:57:06,223 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 08:57:06,264 - INFO - Appended data for date 20191205 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:57:06,265 - INFO - Executing SQL query for date 20191205 and interval 6 to 9\n",
      "2024-07-03 08:58:08,300 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 08:58:08,427 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:58:08,480 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 08:58:08,594 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 08:58:08,610 - INFO - Appended data for date 20191205 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:58:08,611 - INFO - Executing SQL query for date 20191205 and interval 9 to 12\n",
      "2024-07-03 08:59:45,501 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 08:59:45,781 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:59:45,799 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 08:59:46,065 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 08:59:46,080 - INFO - Appended data for date 20191205 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 08:59:46,081 - INFO - Executing SQL query for date 20191205 and interval 12 to 15\n",
      "2024-07-03 09:00:32,538 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:00:32,693 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:00:32,710 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:00:32,830 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:00:32,841 - INFO - Appended data for date 20191205 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:00:32,842 - INFO - Executing SQL query for date 20191205 and interval 15 to 18\n",
      "2024-07-03 09:01:26,274 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:01:26,409 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:01:26,424 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:01:26,542 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:01:26,553 - INFO - Appended data for date 20191205 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:01:26,554 - INFO - Executing SQL query for date 20191205 and interval 18 to 21\n",
      "2024-07-03 09:02:47,756 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:02:47,894 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:02:47,910 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:02:48,028 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:02:48,039 - INFO - Appended data for date 20191205 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:02:48,040 - INFO - Executing SQL query for date 20191205 and interval 21 to 24\n",
      "2024-07-03 09:03:31,418 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:03:31,557 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:03:31,573 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:03:31,666 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:03:31,678 - INFO - Appended data for date 20191205 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:03:31,678 - INFO - Executing SQL query for date 20191206 and interval 0 to 3\n",
      "2024-07-03 09:04:16,561 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:04:16,618 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:04:16,633 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:04:16,688 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:04:16,700 - INFO - Appended data for date 20191206 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:04:16,701 - INFO - Executing SQL query for date 20191206 and interval 3 to 6\n",
      "2024-07-03 09:05:56,412 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:05:56,469 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:05:56,481 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:05:56,527 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:05:56,537 - INFO - Appended data for date 20191206 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:05:56,537 - INFO - Executing SQL query for date 20191206 and interval 6 to 9\n",
      "2024-07-03 09:06:49,789 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:06:49,998 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:06:50,012 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:06:50,184 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:06:50,196 - INFO - Appended data for date 20191206 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:06:50,196 - INFO - Executing SQL query for date 20191206 and interval 9 to 12\n",
      "2024-07-03 09:07:39,826 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:07:39,992 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:07:40,011 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:07:40,125 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:07:40,136 - INFO - Appended data for date 20191206 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:07:40,137 - INFO - Executing SQL query for date 20191206 and interval 12 to 15\n",
      "2024-07-03 09:09:19,182 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:09:19,315 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:09:19,337 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:09:19,452 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:09:19,462 - INFO - Appended data for date 20191206 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:09:19,463 - INFO - Executing SQL query for date 20191206 and interval 15 to 18\n",
      "2024-07-03 09:10:14,830 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:10:14,966 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:10:14,982 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:10:15,099 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:10:15,116 - INFO - Appended data for date 20191206 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:10:15,117 - INFO - Executing SQL query for date 20191206 and interval 18 to 21\n",
      "2024-07-03 09:11:12,413 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:11:12,562 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:11:12,579 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:11:12,704 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:11:12,720 - INFO - Appended data for date 20191206 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:11:12,721 - INFO - Executing SQL query for date 20191206 and interval 21 to 24\n",
      "2024-07-03 09:12:34,365 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:12:34,476 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:12:34,490 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:12:34,585 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:12:34,596 - INFO - Appended data for date 20191206 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:12:34,596 - INFO - Executing SQL query for date 20191207 and interval 0 to 3\n",
      "2024-07-03 09:13:21,793 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:13:21,853 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:13:21,873 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:13:21,922 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:13:21,932 - INFO - Appended data for date 20191207 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:13:21,933 - INFO - Executing SQL query for date 20191207 and interval 3 to 6\n",
      "2024-07-03 09:14:44,523 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:14:44,577 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:14:44,593 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:14:44,639 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:14:44,650 - INFO - Appended data for date 20191207 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:14:44,650 - INFO - Executing SQL query for date 20191207 and interval 6 to 9\n",
      "2024-07-03 09:15:33,543 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:15:33,654 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:15:33,669 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:15:33,784 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:15:33,798 - INFO - Appended data for date 20191207 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:15:33,799 - INFO - Executing SQL query for date 20191207 and interval 9 to 12\n",
      "2024-07-03 09:16:27,044 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:16:27,177 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:16:27,191 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:16:27,301 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:16:27,312 - INFO - Appended data for date 20191207 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:16:27,312 - INFO - Executing SQL query for date 20191207 and interval 12 to 15\n",
      "2024-07-03 09:17:58,593 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:17:58,787 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:17:58,802 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:17:59,029 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:17:59,040 - INFO - Appended data for date 20191207 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:17:59,041 - INFO - Executing SQL query for date 20191207 and interval 15 to 18\n",
      "2024-07-03 09:18:49,011 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:18:49,291 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:18:49,312 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:18:49,563 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:18:49,577 - INFO - Appended data for date 20191207 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:18:49,578 - INFO - Executing SQL query for date 20191207 and interval 18 to 21\n",
      "2024-07-03 09:19:44,582 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:19:44,736 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:19:44,752 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:19:44,880 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:19:44,892 - INFO - Appended data for date 20191207 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:19:44,892 - INFO - Executing SQL query for date 20191207 and interval 21 to 24\n",
      "2024-07-03 09:21:17,994 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:21:18,106 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:21:18,121 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:21:18,219 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:21:18,230 - INFO - Appended data for date 20191207 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:21:18,230 - INFO - Executing SQL query for date 20191208 and interval 0 to 3\n",
      "2024-07-03 09:21:57,817 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:21:57,900 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:21:57,914 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:21:58,000 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:21:58,012 - INFO - Appended data for date 20191208 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:21:58,012 - INFO - Executing SQL query for date 20191208 and interval 3 to 6\n",
      "2024-07-03 09:22:41,765 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:22:41,821 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:22:41,836 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:22:41,881 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:22:41,891 - INFO - Appended data for date 20191208 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:22:41,891 - INFO - Executing SQL query for date 20191208 and interval 6 to 9\n",
      "2024-07-03 09:24:13,352 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:24:13,452 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:24:13,467 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:24:13,551 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:24:13,563 - INFO - Appended data for date 20191208 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:24:13,564 - INFO - Executing SQL query for date 20191208 and interval 9 to 12\n",
      "2024-07-03 09:25:01,129 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:25:01,269 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:25:01,287 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:25:01,412 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:25:01,423 - INFO - Appended data for date 20191208 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:25:01,424 - INFO - Executing SQL query for date 20191208 and interval 12 to 15\n",
      "2024-07-03 09:25:48,546 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:25:48,690 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:25:48,707 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:25:48,826 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:25:48,837 - INFO - Appended data for date 20191208 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:25:48,837 - INFO - Executing SQL query for date 20191208 and interval 15 to 18\n",
      "2024-07-03 09:26:49,588 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:26:49,732 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:26:49,752 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:26:49,872 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:26:49,882 - INFO - Appended data for date 20191208 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:26:49,883 - INFO - Executing SQL query for date 20191208 and interval 18 to 21\n",
      "2024-07-03 09:28:13,592 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:28:13,742 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:28:13,761 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:28:13,885 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:28:13,896 - INFO - Appended data for date 20191208 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:28:13,897 - INFO - Executing SQL query for date 20191208 and interval 21 to 24\n",
      "2024-07-03 09:29:03,216 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:29:03,400 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:29:03,414 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:29:03,617 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:29:03,628 - INFO - Appended data for date 20191208 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:29:03,629 - INFO - Executing SQL query for date 20191209 and interval 0 to 3\n",
      "2024-07-03 09:29:43,148 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:29:43,204 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:29:43,216 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:29:43,261 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:29:43,273 - INFO - Appended data for date 20191209 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:29:43,274 - INFO - Executing SQL query for date 20191209 and interval 3 to 6\n",
      "2024-07-03 09:30:41,927 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:30:41,994 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:30:42,010 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:30:42,058 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:30:42,070 - INFO - Appended data for date 20191209 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:30:42,071 - INFO - Executing SQL query for date 20191209 and interval 6 to 9\n",
      "2024-07-03 09:32:02,749 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:32:02,876 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:32:02,896 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:32:03,002 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:32:03,015 - INFO - Appended data for date 20191209 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:32:03,016 - INFO - Executing SQL query for date 20191209 and interval 9 to 12\n",
      "2024-07-03 09:32:50,433 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:32:50,569 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:32:50,584 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:32:50,707 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:32:50,718 - INFO - Appended data for date 20191209 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:32:50,719 - INFO - Executing SQL query for date 20191209 and interval 12 to 15\n",
      "2024-07-03 09:33:53,493 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:33:53,627 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:33:53,641 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:33:53,759 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:33:53,772 - INFO - Appended data for date 20191209 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:33:53,773 - INFO - Executing SQL query for date 20191209 and interval 15 to 18\n",
      "2024-07-03 09:35:13,083 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:35:13,267 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:35:13,284 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:35:13,421 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:35:13,432 - INFO - Appended data for date 20191209 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:35:13,433 - INFO - Executing SQL query for date 20191209 and interval 18 to 21\n",
      "2024-07-03 09:36:17,858 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:36:18,002 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:36:18,016 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:36:18,143 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:36:18,154 - INFO - Appended data for date 20191209 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:36:18,155 - INFO - Executing SQL query for date 20191209 and interval 21 to 24\n",
      "2024-07-03 09:37:27,779 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:37:27,878 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:37:27,892 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:37:27,975 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:37:27,987 - INFO - Appended data for date 20191209 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:37:27,987 - INFO - Executing SQL query for date 20191210 and interval 0 to 3\n",
      "2024-07-03 09:38:29,914 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:38:29,974 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:38:29,989 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:38:30,038 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:38:30,049 - INFO - Appended data for date 20191210 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:38:30,049 - INFO - Executing SQL query for date 20191210 and interval 3 to 6\n",
      "2024-07-03 09:39:45,425 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:39:45,484 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:39:45,498 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:39:45,572 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:39:45,584 - INFO - Appended data for date 20191210 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:39:45,584 - INFO - Executing SQL query for date 20191210 and interval 6 to 9\n",
      "2024-07-03 09:40:23,373 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:40:23,528 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:40:23,541 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:40:23,679 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:40:23,690 - INFO - Appended data for date 20191210 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:40:23,691 - INFO - Executing SQL query for date 20191210 and interval 9 to 12\n",
      "2024-07-03 09:41:47,536 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:41:47,664 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:41:47,682 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:41:47,794 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:41:47,804 - INFO - Appended data for date 20191210 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:41:47,804 - INFO - Executing SQL query for date 20191210 and interval 12 to 15\n",
      "2024-07-03 09:42:49,069 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:42:49,238 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:42:49,254 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:42:49,368 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:42:49,379 - INFO - Appended data for date 20191210 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:42:49,380 - INFO - Executing SQL query for date 20191210 and interval 15 to 18\n",
      "2024-07-03 09:43:35,924 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:43:36,088 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:43:36,109 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:43:36,238 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:43:36,250 - INFO - Appended data for date 20191210 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:43:36,250 - INFO - Executing SQL query for date 20191210 and interval 18 to 21\n",
      "2024-07-03 09:44:36,416 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:44:36,558 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:44:36,575 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:44:36,698 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:44:36,710 - INFO - Appended data for date 20191210 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:44:36,710 - INFO - Executing SQL query for date 20191210 and interval 21 to 24\n",
      "2024-07-03 09:45:48,996 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:45:49,103 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:45:49,119 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:45:49,219 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:45:49,232 - INFO - Appended data for date 20191210 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:45:49,233 - INFO - Executing SQL query for date 20191211 and interval 0 to 3\n",
      "2024-07-03 09:46:28,361 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:46:28,419 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:46:28,432 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:46:28,501 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:46:28,513 - INFO - Appended data for date 20191211 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:46:28,514 - INFO - Executing SQL query for date 20191211 and interval 3 to 6\n",
      "2024-07-03 09:47:05,249 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:47:05,305 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:47:05,318 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:47:05,361 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:47:05,372 - INFO - Appended data for date 20191211 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:47:05,372 - INFO - Executing SQL query for date 20191211 and interval 6 to 9\n",
      "2024-07-03 09:48:50,525 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:48:50,645 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:48:50,663 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:48:50,770 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:48:50,781 - INFO - Appended data for date 20191211 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:48:50,782 - INFO - Executing SQL query for date 20191211 and interval 9 to 12\n",
      "2024-07-03 09:49:33,260 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:49:33,417 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:49:33,431 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:49:33,546 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:49:33,557 - INFO - Appended data for date 20191211 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:49:33,557 - INFO - Executing SQL query for date 20191211 and interval 12 to 15\n",
      "2024-07-03 09:50:20,309 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:50:20,443 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:50:20,459 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:50:20,574 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:50:20,587 - INFO - Appended data for date 20191211 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:50:20,588 - INFO - Executing SQL query for date 20191211 and interval 15 to 18\n",
      "2024-07-03 09:52:01,723 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 09:52:01,858 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:52:01,875 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 09:52:02,007 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 09:52:02,019 - INFO - Appended data for date 20191211 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:52:02,020 - INFO - Executing SQL query for date 20191211 and interval 18 to 21\n",
      "2024-07-03 09:52:52,956 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 09:52:53,139 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:52:53,166 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 09:52:53,288 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 09:52:53,299 - INFO - Appended data for date 20191211 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:52:53,299 - INFO - Executing SQL query for date 20191211 and interval 21 to 24\n",
      "2024-07-03 09:53:38,642 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 09:53:38,740 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:53:38,755 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 09:53:38,838 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 09:53:38,849 - INFO - Appended data for date 20191211 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:53:38,849 - INFO - Executing SQL query for date 20191212 and interval 0 to 3\n",
      "2024-07-03 09:55:25,710 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 09:55:25,768 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:55:25,781 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 09:55:25,828 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 09:55:25,840 - INFO - Appended data for date 20191212 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:55:25,840 - INFO - Executing SQL query for date 20191212 and interval 3 to 6\n",
      "2024-07-03 09:56:04,468 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 09:56:04,527 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:56:04,544 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 09:56:04,603 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 09:56:04,614 - INFO - Appended data for date 20191212 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:56:04,615 - INFO - Executing SQL query for date 20191212 and interval 6 to 9\n",
      "2024-07-03 09:56:43,792 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 09:56:43,930 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:56:43,947 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 09:56:44,045 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 09:56:44,056 - INFO - Appended data for date 20191212 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:56:44,056 - INFO - Executing SQL query for date 20191212 and interval 9 to 12\n",
      "2024-07-03 09:58:32,268 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 09:58:32,414 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:58:32,433 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 09:58:32,544 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 09:58:32,555 - INFO - Appended data for date 20191212 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:58:32,556 - INFO - Executing SQL query for date 20191212 and interval 12 to 15\n",
      "2024-07-03 09:59:19,414 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 09:59:19,544 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:59:19,561 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 09:59:19,679 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 09:59:19,690 - INFO - Appended data for date 20191212 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 09:59:19,691 - INFO - Executing SQL query for date 20191212 and interval 15 to 18\n",
      "2024-07-03 10:00:46,732 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:00:46,893 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:00:46,910 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:00:47,029 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:00:47,040 - INFO - Appended data for date 20191212 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:00:47,041 - INFO - Executing SQL query for date 20191212 and interval 18 to 21\n",
      "2024-07-03 10:01:56,743 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:01:56,879 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:01:56,893 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:01:57,014 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:01:57,024 - INFO - Appended data for date 20191212 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:01:57,025 - INFO - Executing SQL query for date 20191212 and interval 21 to 24\n",
      "2024-07-03 10:03:17,745 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:03:17,848 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:03:17,867 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:03:17,952 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:03:17,964 - INFO - Appended data for date 20191212 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:03:17,965 - INFO - Executing SQL query for date 20191213 and interval 0 to 3\n",
      "2024-07-03 10:04:01,453 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:04:01,509 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:04:01,528 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:04:01,574 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:04:01,585 - INFO - Appended data for date 20191213 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:04:01,585 - INFO - Executing SQL query for date 20191213 and interval 3 to 6\n",
      "2024-07-03 10:05:37,667 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:05:37,722 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:05:37,735 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:05:37,780 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:05:37,791 - INFO - Appended data for date 20191213 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:05:37,792 - INFO - Executing SQL query for date 20191213 and interval 6 to 9\n",
      "2024-07-03 10:06:21,215 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:06:21,333 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:06:21,350 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:06:21,454 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:06:21,465 - INFO - Appended data for date 20191213 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:06:21,465 - INFO - Executing SQL query for date 20191213 and interval 9 to 12\n",
      "2024-07-03 10:07:07,119 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:07:07,308 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:07:07,323 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:07:07,437 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:07:07,448 - INFO - Appended data for date 20191213 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:07:07,448 - INFO - Executing SQL query for date 20191213 and interval 12 to 15\n",
      "2024-07-03 10:08:15,389 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:08:15,524 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:08:15,540 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:08:15,657 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:08:15,672 - INFO - Appended data for date 20191213 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:08:15,672 - INFO - Executing SQL query for date 20191213 and interval 15 to 18\n",
      "2024-07-03 10:09:32,754 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:09:32,916 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:09:32,933 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:09:33,073 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:09:33,084 - INFO - Appended data for date 20191213 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:09:33,084 - INFO - Executing SQL query for date 20191213 and interval 18 to 21\n",
      "2024-07-03 10:10:24,772 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:10:24,925 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:10:24,944 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:10:25,073 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:10:25,083 - INFO - Appended data for date 20191213 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:10:25,084 - INFO - Executing SQL query for date 20191213 and interval 21 to 24\n",
      "2024-07-03 10:11:33,755 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:11:33,859 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:11:33,873 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:11:33,962 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:11:33,974 - INFO - Appended data for date 20191213 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:11:33,974 - INFO - Executing SQL query for date 20191214 and interval 0 to 3\n",
      "2024-07-03 10:12:43,136 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:12:43,221 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:12:43,234 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:12:43,284 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:12:43,294 - INFO - Appended data for date 20191214 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:12:43,295 - INFO - Executing SQL query for date 20191214 and interval 3 to 6\n",
      "2024-07-03 10:13:22,513 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:13:22,597 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:13:22,611 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:13:22,674 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:13:22,689 - INFO - Appended data for date 20191214 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:13:22,690 - INFO - Executing SQL query for date 20191214 and interval 6 to 9\n",
      "2024-07-03 10:14:02,315 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:14:02,431 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:14:02,445 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:14:02,536 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:14:02,550 - INFO - Appended data for date 20191214 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:14:02,550 - INFO - Executing SQL query for date 20191214 and interval 9 to 12\n",
      "2024-07-03 10:15:40,481 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:15:40,626 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:15:40,650 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:15:40,769 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:15:40,779 - INFO - Appended data for date 20191214 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:15:40,780 - INFO - Executing SQL query for date 20191214 and interval 12 to 15\n",
      "2024-07-03 10:16:29,615 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:16:29,755 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:16:29,774 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:16:29,892 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:16:29,903 - INFO - Appended data for date 20191214 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:16:29,904 - INFO - Executing SQL query for date 20191214 and interval 15 to 18\n",
      "2024-07-03 10:17:15,289 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:17:15,444 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:17:15,459 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:17:15,605 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:17:15,616 - INFO - Appended data for date 20191214 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:17:15,617 - INFO - Executing SQL query for date 20191214 and interval 18 to 21\n",
      "2024-07-03 10:18:28,449 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:18:28,597 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:18:28,613 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:18:28,740 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:18:28,751 - INFO - Appended data for date 20191214 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:18:28,752 - INFO - Executing SQL query for date 20191214 and interval 21 to 24\n",
      "2024-07-03 10:19:41,192 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:19:41,300 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:19:41,315 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:19:41,405 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:19:41,415 - INFO - Appended data for date 20191214 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:19:41,416 - INFO - Executing SQL query for date 20191215 and interval 0 to 3\n",
      "2024-07-03 10:20:16,288 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:20:16,349 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:20:16,365 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:20:16,415 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:20:16,426 - INFO - Appended data for date 20191215 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:20:16,426 - INFO - Executing SQL query for date 20191215 and interval 3 to 6\n",
      "2024-07-03 10:20:59,013 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:20:59,116 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:20:59,129 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:20:59,188 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:20:59,200 - INFO - Appended data for date 20191215 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:20:59,200 - INFO - Executing SQL query for date 20191215 and interval 6 to 9\n",
      "2024-07-03 10:22:06,561 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:22:06,671 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:22:06,692 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:22:06,775 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:22:06,786 - INFO - Appended data for date 20191215 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:22:06,786 - INFO - Executing SQL query for date 20191215 and interval 9 to 12\n",
      "2024-07-03 10:23:21,216 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:23:21,344 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:23:21,363 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:23:21,487 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:23:21,499 - INFO - Appended data for date 20191215 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:23:21,499 - INFO - Executing SQL query for date 20191215 and interval 12 to 15\n",
      "2024-07-03 10:24:16,602 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:24:16,734 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:24:16,751 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:24:16,883 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:24:16,894 - INFO - Appended data for date 20191215 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:24:16,895 - INFO - Executing SQL query for date 20191215 and interval 15 to 18\n",
      "2024-07-03 10:25:44,522 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:25:44,705 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:25:44,725 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:25:44,869 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:25:44,879 - INFO - Appended data for date 20191215 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:25:44,880 - INFO - Executing SQL query for date 20191215 and interval 18 to 21\n",
      "2024-07-03 10:26:44,042 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:26:44,186 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:26:44,199 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:26:44,320 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:26:44,331 - INFO - Appended data for date 20191215 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:26:44,332 - INFO - Executing SQL query for date 20191215 and interval 21 to 24\n",
      "2024-07-03 10:28:00,462 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:28:00,564 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:28:00,580 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:28:00,666 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:28:00,679 - INFO - Appended data for date 20191215 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:28:00,679 - INFO - Executing SQL query for date 20191216 and interval 0 to 3\n",
      "2024-07-03 10:28:41,130 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:28:41,190 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:28:41,205 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:28:41,251 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:28:41,261 - INFO - Appended data for date 20191216 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:28:41,261 - INFO - Executing SQL query for date 20191216 and interval 3 to 6\n",
      "2024-07-03 10:30:14,296 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:30:14,360 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:30:14,375 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:30:14,422 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:30:14,435 - INFO - Appended data for date 20191216 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:30:14,435 - INFO - Executing SQL query for date 20191216 and interval 6 to 9\n",
      "2024-07-03 10:31:11,504 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:31:11,620 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:31:11,634 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:31:11,731 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:31:11,748 - INFO - Appended data for date 20191216 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:31:11,749 - INFO - Executing SQL query for date 20191216 and interval 9 to 12\n",
      "2024-07-03 10:31:57,792 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:31:57,924 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:31:57,939 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:31:58,051 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:31:58,062 - INFO - Appended data for date 20191216 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:31:58,062 - INFO - Executing SQL query for date 20191216 and interval 12 to 15\n",
      "2024-07-03 10:33:05,381 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:33:05,516 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:33:05,530 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:33:05,641 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:33:05,652 - INFO - Appended data for date 20191216 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:33:05,653 - INFO - Executing SQL query for date 20191216 and interval 15 to 18\n",
      "2024-07-03 10:34:21,391 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:34:21,602 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:34:21,617 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:34:21,770 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:34:21,781 - INFO - Appended data for date 20191216 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:34:21,782 - INFO - Executing SQL query for date 20191216 and interval 18 to 21\n",
      "2024-07-03 10:35:08,044 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:35:08,270 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:35:08,285 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:35:08,417 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:35:08,428 - INFO - Appended data for date 20191216 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:35:08,429 - INFO - Executing SQL query for date 20191216 and interval 21 to 24\n",
      "2024-07-03 10:36:15,830 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:36:15,934 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:36:15,948 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:36:16,032 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:36:16,043 - INFO - Appended data for date 20191216 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:36:16,043 - INFO - Executing SQL query for date 20191217 and interval 0 to 3\n",
      "2024-07-03 10:37:23,905 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:37:23,962 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:37:23,976 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:37:24,021 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:37:24,035 - INFO - Appended data for date 20191217 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:37:24,036 - INFO - Executing SQL query for date 20191217 and interval 3 to 6\n",
      "2024-07-03 10:38:06,889 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:38:06,986 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:38:07,003 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:38:07,076 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:38:07,087 - INFO - Appended data for date 20191217 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:38:07,087 - INFO - Executing SQL query for date 20191217 and interval 6 to 9\n",
      "2024-07-03 10:38:55,538 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:38:55,665 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:38:55,683 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:38:55,790 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:38:55,800 - INFO - Appended data for date 20191217 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:38:55,801 - INFO - Executing SQL query for date 20191217 and interval 9 to 12\n",
      "2024-07-03 10:40:25,035 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:40:25,173 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:40:25,191 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:40:25,307 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:40:25,317 - INFO - Appended data for date 20191217 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:40:25,317 - INFO - Executing SQL query for date 20191217 and interval 12 to 15\n",
      "2024-07-03 10:41:15,518 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:41:15,723 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:41:15,740 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:41:15,879 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:41:15,891 - INFO - Appended data for date 20191217 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:41:15,892 - INFO - Executing SQL query for date 20191217 and interval 15 to 18\n",
      "2024-07-03 10:42:06,599 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:42:06,740 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:42:06,756 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:42:06,874 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:42:06,885 - INFO - Appended data for date 20191217 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:42:06,886 - INFO - Executing SQL query for date 20191217 and interval 18 to 21\n",
      "2024-07-03 10:43:54,159 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:43:54,337 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:43:54,350 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:43:54,473 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:43:54,487 - INFO - Appended data for date 20191217 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:43:54,488 - INFO - Executing SQL query for date 20191217 and interval 21 to 24\n",
      "2024-07-03 10:44:40,051 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:44:40,152 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:44:40,169 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:44:40,267 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:44:40,278 - INFO - Appended data for date 20191217 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:44:40,279 - INFO - Executing SQL query for date 20191218 and interval 0 to 3\n",
      "2024-07-03 10:45:48,844 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:45:48,898 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:45:48,915 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:45:48,959 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:45:48,969 - INFO - Appended data for date 20191218 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:45:48,970 - INFO - Executing SQL query for date 20191218 and interval 3 to 6\n",
      "2024-07-03 10:47:06,204 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:47:06,274 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:47:06,290 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:47:06,338 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:47:06,369 - INFO - Appended data for date 20191218 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:47:06,370 - INFO - Executing SQL query for date 20191218 and interval 6 to 9\n",
      "2024-07-03 10:47:52,675 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:47:52,795 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:47:52,818 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:47:52,933 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:47:52,943 - INFO - Appended data for date 20191218 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:47:52,943 - INFO - Executing SQL query for date 20191218 and interval 9 to 12\n",
      "2024-07-03 10:49:34,066 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:49:34,199 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:49:34,214 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:49:34,329 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:49:34,344 - INFO - Appended data for date 20191218 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:49:34,345 - INFO - Executing SQL query for date 20191218 and interval 12 to 15\n",
      "2024-07-03 10:50:38,214 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:50:38,354 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:50:38,368 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:50:38,479 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:50:38,490 - INFO - Appended data for date 20191218 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:50:38,490 - INFO - Executing SQL query for date 20191218 and interval 15 to 18\n",
      "2024-07-03 10:51:55,380 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 10:51:55,541 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:51:55,564 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 10:51:55,702 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 10:51:55,715 - INFO - Appended data for date 20191218 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:51:55,715 - INFO - Executing SQL query for date 20191218 and interval 18 to 21\n",
      "2024-07-03 10:52:45,141 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 10:52:45,292 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:52:45,307 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 10:52:45,435 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 10:52:45,446 - INFO - Appended data for date 20191218 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:52:45,447 - INFO - Executing SQL query for date 20191218 and interval 21 to 24\n",
      "2024-07-03 10:54:35,291 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 10:54:35,420 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:54:35,433 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 10:54:35,534 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 10:54:35,545 - INFO - Appended data for date 20191218 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:54:35,546 - INFO - Executing SQL query for date 20191219 and interval 0 to 3\n",
      "2024-07-03 10:55:14,481 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 10:55:14,540 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:55:14,554 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 10:55:14,602 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 10:55:14,613 - INFO - Appended data for date 20191219 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:55:14,614 - INFO - Executing SQL query for date 20191219 and interval 3 to 6\n",
      "2024-07-03 10:55:55,718 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 10:55:55,776 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:55:55,790 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 10:55:55,846 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 10:55:55,858 - INFO - Appended data for date 20191219 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:55:55,858 - INFO - Executing SQL query for date 20191219 and interval 6 to 9\n",
      "2024-07-03 10:57:30,489 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 10:57:30,616 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:57:30,630 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 10:57:30,751 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 10:57:30,766 - INFO - Appended data for date 20191219 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:57:30,767 - INFO - Executing SQL query for date 20191219 and interval 9 to 12\n",
      "2024-07-03 10:58:19,756 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 10:58:19,914 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:58:19,929 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 10:58:20,066 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 10:58:20,078 - INFO - Appended data for date 20191219 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:58:20,079 - INFO - Executing SQL query for date 20191219 and interval 12 to 15\n",
      "2024-07-03 10:59:05,245 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 10:59:05,396 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:59:05,409 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 10:59:05,535 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 10:59:05,546 - INFO - Appended data for date 20191219 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 10:59:05,547 - INFO - Executing SQL query for date 20191219 and interval 15 to 18\n",
      "2024-07-03 11:00:12,101 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 11:00:12,250 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:00:12,267 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 11:00:12,395 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:00:12,407 - INFO - Appended data for date 20191219 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:00:12,407 - INFO - Executing SQL query for date 20191219 and interval 18 to 21\n",
      "2024-07-03 11:01:30,173 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 11:01:30,373 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:01:30,390 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 11:01:30,566 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:01:30,577 - INFO - Appended data for date 20191219 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:01:30,577 - INFO - Executing SQL query for date 20191219 and interval 21 to 24\n",
      "2024-07-03 11:02:17,501 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 11:02:17,671 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:02:17,684 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 11:02:17,817 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:02:17,828 - INFO - Appended data for date 20191219 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:02:17,829 - INFO - Executing SQL query for date 20191220 and interval 0 to 3\n",
      "2024-07-03 11:03:19,585 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 11:03:19,655 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:03:19,669 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 11:03:19,721 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:03:19,733 - INFO - Appended data for date 20191220 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:03:19,734 - INFO - Executing SQL query for date 20191220 and interval 3 to 6\n",
      "2024-07-03 11:04:38,142 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 11:04:38,209 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:04:38,224 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 11:04:38,282 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:04:38,294 - INFO - Appended data for date 20191220 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:04:38,294 - INFO - Executing SQL query for date 20191220 and interval 6 to 9\n",
      "2024-07-03 11:05:34,335 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:05:34,462 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:05:34,475 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:05:34,584 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:05:34,594 - INFO - Appended data for date 20191220 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:05:34,594 - INFO - Executing SQL query for date 20191220 and interval 9 to 12\n",
      "2024-07-03 11:05:42,389 - INFO - failed after 3 attempts\n",
      "2024-07-03 11:05:43,129 - INFO - failed after 3 attempts\n",
      "2024-07-03 11:05:43,130 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 501, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 821, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 838, in fetch\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 615, in cancel\n",
      "    self._query.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 862, in cancel\n",
      "    self._request.raise_response_error(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "2024-07-03 11:05:43,131 - ERROR - Error while processing data for date 20191220 interval 9 to 12: error 502: bad gateway\n",
      "2024-07-03 11:05:43,132 - INFO - Executing SQL query for date 20191220 and interval 12 to 15\n",
      "2024-07-03 11:05:44,518 - INFO - failed after 3 attempts\n",
      "2024-07-03 11:05:44,519 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 482, in execute\n",
      "    self._prepare_statement(operation, statement_name)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 379, in _prepare_statement\n",
      "    query.execute()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 807, in execute\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 612, in cancel\n",
      "    raise trino.exceptions.OperationalError(\n",
      "trino.exceptions.OperationalError: Cancel query failed; no running query\n",
      "2024-07-03 11:05:45,616 - INFO - failed after 3 attempts\n",
      "2024-07-03 11:05:45,617 - ERROR - Error while processing data for date 20191220 interval 12 to 15: error 502: bad gateway\n",
      "2024-07-03 11:05:45,617 - INFO - Executing SQL query for date 20191220 and interval 15 to 18\n",
      "2024-07-03 11:08:06,924 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 11:08:07,086 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:08:07,101 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 11:08:07,245 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:08:07,256 - INFO - Appended data for date 20191220 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:08:07,257 - INFO - Executing SQL query for date 20191220 and interval 18 to 21\n",
      "2024-07-03 11:08:53,592 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 11:08:53,763 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:08:53,777 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 11:08:53,917 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:08:53,928 - INFO - Appended data for date 20191220 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:08:53,929 - INFO - Executing SQL query for date 20191220 and interval 21 to 24\n",
      "2024-07-03 11:09:36,659 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 11:09:36,786 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:09:36,802 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 11:09:36,909 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:09:36,920 - INFO - Appended data for date 20191220 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:09:36,921 - INFO - Executing SQL query for date 20191221 and interval 0 to 3\n",
      "2024-07-03 11:11:05,524 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 11:11:05,592 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:11:05,606 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 11:11:05,662 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:11:05,674 - INFO - Appended data for date 20191221 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:11:05,675 - INFO - Executing SQL query for date 20191221 and interval 3 to 6\n",
      "2024-07-03 11:11:51,105 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 11:11:51,178 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:11:51,192 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 11:11:51,271 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:11:51,283 - INFO - Appended data for date 20191221 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:11:51,284 - INFO - Executing SQL query for date 20191221 and interval 6 to 9\n",
      "2024-07-03 11:12:56,807 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:12:56,927 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:12:56,945 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:12:57,047 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:12:57,057 - INFO - Appended data for date 20191221 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:12:57,058 - INFO - Executing SQL query for date 20191221 and interval 9 to 12\n",
      "2024-07-03 11:14:36,678 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 11:14:36,836 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:14:36,854 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 11:14:36,991 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:14:37,002 - INFO - Appended data for date 20191221 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:14:37,003 - INFO - Executing SQL query for date 20191221 and interval 12 to 15\n",
      "2024-07-03 11:15:54,549 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 11:15:54,715 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:15:54,733 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 11:15:54,868 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:15:54,881 - INFO - Appended data for date 20191221 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:15:54,881 - INFO - Executing SQL query for date 20191221 and interval 15 to 18\n",
      "2024-07-03 11:17:02,820 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 11:17:03,003 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:17:03,021 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 11:17:03,199 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:17:03,211 - INFO - Appended data for date 20191221 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:17:03,212 - INFO - Executing SQL query for date 20191221 and interval 18 to 21\n",
      "2024-07-03 11:17:54,433 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 11:17:54,600 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:17:54,615 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 11:17:54,756 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:17:54,767 - INFO - Appended data for date 20191221 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:17:54,768 - INFO - Executing SQL query for date 20191221 and interval 21 to 24\n",
      "2024-07-03 11:19:37,463 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 11:19:37,587 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:19:37,602 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 11:19:37,708 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:19:37,720 - INFO - Appended data for date 20191221 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:19:37,721 - INFO - Executing SQL query for date 20191222 and interval 0 to 3\n",
      "2024-07-03 11:20:19,585 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 11:20:19,667 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:20:19,681 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 11:20:19,766 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:20:19,777 - INFO - Appended data for date 20191222 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:20:19,778 - INFO - Executing SQL query for date 20191222 and interval 3 to 6\n",
      "2024-07-03 11:20:58,386 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 11:20:58,450 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:20:58,463 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 11:20:58,514 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:20:58,525 - INFO - Appended data for date 20191222 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:20:58,526 - INFO - Executing SQL query for date 20191222 and interval 6 to 9\n",
      "2024-07-03 11:22:38,807 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:22:38,928 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:22:38,942 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:22:39,044 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:22:39,067 - INFO - Appended data for date 20191222 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:22:39,068 - INFO - Executing SQL query for date 20191222 and interval 9 to 12\n",
      "2024-07-03 11:23:32,068 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 11:23:32,227 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:23:32,264 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 11:23:32,442 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:23:32,454 - INFO - Appended data for date 20191222 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:23:32,454 - INFO - Executing SQL query for date 20191222 and interval 12 to 15\n",
      "2024-07-03 11:24:17,992 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 11:24:18,155 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:24:18,191 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 11:24:18,329 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:24:18,340 - INFO - Appended data for date 20191222 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:24:18,340 - INFO - Executing SQL query for date 20191222 and interval 15 to 18\n",
      "2024-07-03 11:30:51,871 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:30:52,010 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:30:52,025 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:30:52,134 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:30:52,148 - INFO - Appended data for date 20191223 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:30:52,149 - INFO - Executing SQL query for date 20191223 and interval 9 to 12\n",
      "2024-07-03 11:33:23,065 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 11:33:23,228 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:33:23,243 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 11:33:23,419 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:33:23,429 - INFO - Appended data for date 20191223 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:33:23,430 - INFO - Executing SQL query for date 20191223 and interval 15 to 18\n",
      "2024-07-03 11:34:10,890 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 11:34:11,049 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:34:11,064 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 11:34:11,198 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:34:11,209 - INFO - Appended data for date 20191223 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:34:11,210 - INFO - Executing SQL query for date 20191223 and interval 18 to 21\n",
      "2024-07-03 11:35:47,741 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 11:35:47,905 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:35:47,920 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 11:35:48,060 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:35:48,070 - INFO - Appended data for date 20191223 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:35:48,070 - INFO - Executing SQL query for date 20191223 and interval 21 to 24\n",
      "2024-07-03 11:36:31,557 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 11:36:31,678 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:36:31,692 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 11:36:31,794 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:36:31,814 - INFO - Appended data for date 20191223 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:36:31,815 - INFO - Executing SQL query for date 20191224 and interval 0 to 3\n",
      "2024-07-03 11:38:03,632 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 11:38:03,695 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:38:03,708 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 11:38:03,760 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:38:03,776 - INFO - Appended data for date 20191224 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:38:03,777 - INFO - Executing SQL query for date 20191224 and interval 3 to 6\n",
      "2024-07-03 11:38:43,333 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 11:38:43,392 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:38:43,404 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 11:38:43,453 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:38:43,464 - INFO - Appended data for date 20191224 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:38:43,464 - INFO - Executing SQL query for date 20191224 and interval 6 to 9\n",
      "2024-07-03 11:40:21,863 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:40:21,988 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:40:22,003 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:40:22,103 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:40:22,114 - INFO - Appended data for date 20191224 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:40:22,115 - INFO - Executing SQL query for date 20191224 and interval 9 to 12\n",
      "2024-07-03 11:41:13,783 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 11:41:14,036 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:41:14,057 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 11:41:14,272 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:41:14,283 - INFO - Appended data for date 20191224 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:41:14,284 - INFO - Executing SQL query for date 20191224 and interval 12 to 15\n",
      "2024-07-03 11:42:30,388 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 11:42:30,543 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:42:30,560 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 11:42:30,690 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:42:30,701 - INFO - Appended data for date 20191224 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:42:30,702 - INFO - Executing SQL query for date 20191224 and interval 15 to 18\n",
      "2024-07-03 11:43:47,722 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 11:43:47,891 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:43:47,906 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 11:43:48,049 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 11:43:48,060 - INFO - Appended data for date 20191224 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:43:48,061 - INFO - Executing SQL query for date 20191224 and interval 18 to 21\n",
      "2024-07-03 11:44:34,718 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-03 11:44:34,973 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:44:34,991 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-03 11:44:35,181 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 11:44:35,192 - INFO - Appended data for date 20191224 interval 18 to 21 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:44:35,193 - INFO - Executing SQL query for date 20191224 and interval 21 to 24\n",
      "2024-07-03 11:45:39,837 - INFO - Aggregating data for geohash_5 for interval 21 to 24\n",
      "2024-07-03 11:45:39,960 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:45:39,979 - INFO - Aggregating data for geohash_3 for interval 21 to 24\n",
      "2024-07-03 11:45:40,087 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 11:45:40,101 - INFO - Appended data for date 20191224 interval 21 to 24 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:45:40,102 - INFO - Executing SQL query for date 20191225 and interval 0 to 3\n",
      "2024-07-03 11:46:56,766 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-03 11:46:56,836 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:46:56,851 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-03 11:46:56,910 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 11:46:56,921 - INFO - Appended data for date 20191225 interval 0 to 3 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:46:56,922 - INFO - Executing SQL query for date 20191225 and interval 3 to 6\n",
      "2024-07-03 11:47:36,905 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n",
      "2024-07-03 11:47:36,981 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:47:36,997 - INFO - Aggregating data for geohash_3 for interval 3 to 6\n",
      "2024-07-03 11:47:37,047 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 11:47:37,058 - INFO - Appended data for date 20191225 interval 3 to 6 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:47:37,059 - INFO - Executing SQL query for date 20191225 and interval 6 to 9\n",
      "2024-07-03 11:48:23,991 - INFO - Aggregating data for geohash_5 for interval 6 to 9\n",
      "2024-07-03 11:48:24,104 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:48:24,119 - INFO - Aggregating data for geohash_3 for interval 6 to 9\n",
      "2024-07-03 11:48:24,219 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 11:48:24,236 - INFO - Appended data for date 20191225 interval 6 to 9 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:48:24,237 - INFO - Executing SQL query for date 20191225 and interval 9 to 12\n",
      "2024-07-03 11:50:09,053 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 11:50:09,209 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:50:09,240 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 11:50:09,373 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 11:50:09,386 - INFO - Appended data for date 20191225 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:50:09,386 - INFO - Executing SQL query for date 20191225 and interval 12 to 15\n",
      "2024-07-03 11:50:57,238 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 11:50:57,433 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:50:57,454 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 11:50:57,623 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 11:50:57,635 - INFO - Appended data for date 20191225 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 11:50:57,635 - INFO - Executing SQL query for date 20191225 and interval 15 to 18\n",
      "2024-07-03 12:08:34,044 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 12:08:34,208 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 12:08:34,226 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 12:08:34,368 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 12:08:34,378 - INFO - Appended data for date 20191227 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 12:08:34,379 - INFO - Executing SQL query for date 20191227 and interval 18 to 21\n",
      "2024-07-03 12:32:12,384 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-03 12:32:12,536 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 12:32:12,553 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-03 12:32:12,680 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 12:32:12,689 - INFO - Appended data for date 20191230 interval 9 to 12 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 12:32:12,690 - INFO - Executing SQL query for date 20191230 and interval 12 to 15\n",
      "2024-07-03 12:32:57,159 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 12:32:57,318 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 12:32:57,335 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 12:32:57,475 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 12:32:57,485 - INFO - Appended data for date 20191230 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 12:32:57,486 - INFO - Executing SQL query for date 20191230 and interval 15 to 18\n",
      "2024-07-03 12:34:16,104 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-03 12:34:16,256 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 12:34:16,275 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-03 12:34:16,406 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 12:34:16,417 - INFO - Appended data for date 20191230 interval 15 to 18 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 12:34:16,418 - INFO - Executing SQL query for date 20191230 and interval 18 to 21\n",
      "2024-07-03 12:35:27,251 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n"
     ]
    }
   ],
   "source": [
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            for geohash_col, export_file_path, write_header in [\n",
    "                ('geohash_5', export_file_path_5, write_header_5),\n",
    "                ('geohash_3', export_file_path_3, write_header_3)\n",
    "            ]:\n",
    "                # Aggregate data for geohash\n",
    "                logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "                aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                    no_of_points=(geohash_col, 'size'),\n",
    "                    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                    local_time=('3_hour_interval', 'first'),\n",
    "                    local_date=('local_date', 'first')\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Filter rows with no_of_unique_users > 10\n",
    "                filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "                \n",
    "                # Append the DataFrame to the CSV file\n",
    "                logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "                filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "                \n",
    "                # After the first write, set the header flag to False\n",
    "                if geohash_col == 'geohash5':\n",
    "                    write_header_5 = False\n",
    "                else:\n",
    "                    write_header_3 = False\n",
    "                \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_records.append((formatted_current_date, start_hour, end_hour, str(e))) \n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfce0a-3ded-4d71-9216-89e735316522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9376d2-1978-47bb-9623-f91d22e28a11",
   "metadata": {},
   "source": [
    "2024-07-01 15:20:09,850 - INFO - Executing SQL query for date 20190626 and interval 18 to 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746c7a7-93fb-4033-be9f-d8ce25b94506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d477592f-0902-45f7-86ea-04d505c791e7",
   "metadata": {},
   "source": [
    "## Filling dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdaa638-a871-41c4-a391-787516b6192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Inputs\n",
    "inputs = [\n",
    "    # {'date': '20190118', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20190626', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20190710', 'interval': '12:00 - 15:00'},\n",
    "    # {'date': '20190713', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20190726', 'interval': '15:00 - 18:00'},\n",
    "    # {'date': '20190810', 'interval': '9:00 - 12:00'},\n",
    "    # {'date': '20190827', 'interval': '15:00 - 18:00'},\n",
    "    # {'date': '20190830', 'interval': '0:00 - 3:00'},\n",
    "    # {'date': '20190920', 'interval': '6:00 - 9:00'},\n",
    "    # {'date': '20190927', 'interval': '21:00 - 24:00'},\n",
    "    # {'date': '20190928', 'interval': '0:00 - 3:00'},\n",
    "    # {'date': '20190929', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20191009', 'interval': '9:00 - 12:00'},\n",
    "    # {'date': '20191012', 'interval': '9:00 - 12:00'},\n",
    "    # {'date': '20191013', 'interval': '15:00 - 18:00'}\n",
    "    # {'date': '20191014', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20191101', 'interval': '12:00 - 15:00'},\n",
    "    # {'date': '20191122', 'interval': '18:00 - 21:00'},\n",
    "    {'date': '20191220', 'interval': '12:00 - 15:00'}\n",
    "]\n",
    "country_code = 'ID'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "# export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_path = '/home/jovyan/Data/pd3/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "write_header_5 = False\n",
    "write_header_3 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d2a03a-132c-424e-8eca-77f512a4fb4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 20:41:00,654 - INFO - Executing SQL query for date 20191220 and interval 12 to 15\n",
      "2024-07-03 20:42:36,732 - INFO - Aggregating data for geohash_5 for interval 12 to 15\n",
      "2024-07-03 20:42:36,908 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 20:42:36,925 - INFO - Aggregating data for geohash_3 for interval 12 to 15\n",
      "2024-07-03 20:42:37,091 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 20:42:37,101 - INFO - Appended data for date 20191220 interval 12 to 15 to pd_id_2019_agg5_3h.csv and pd_id_2019_agg3_3h.csv\n",
      "2024-07-03 20:42:37,102 - INFO - Error records:\n"
     ]
    }
   ],
   "source": [
    "# List to record errors\n",
    "error_records = []\n",
    "\n",
    "for input in inputs:\n",
    "    input_date = input['date']\n",
    "    input_interval = input['interval']\n",
    "\n",
    "    # Parse the input\n",
    "    formatted_current_date = input_date\n",
    "    start_hour = int(input_interval.split(':')[0])\n",
    "    end_hour = start_hour + 3\n",
    "\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    current_date = datetime.strptime(input_date, '%Y%m%d')\n",
    "    lookback_date = current_date - timedelta(days=1)\n",
    "    lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "    # Format dates for the SQL query in 'yyyymmdd' format\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "    try:\n",
    "        # SQL Query to fetch data for the specified date and interval\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash_3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "            AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime once\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Create 3-hour interval column\n",
    "        interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "        pe_dl_table_gen['3_hour_interval'] = interval\n",
    "        pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "        \n",
    "        for geohash_col, export_file_path, write_header in [\n",
    "            ('geohash_5', export_file_path_5, write_header_5),\n",
    "            ('geohash_3', export_file_path_3, write_header_3)\n",
    "        ]:\n",
    "            # Aggregate data for geohash\n",
    "            logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                no_of_points=(geohash_col, 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file\n",
    "            logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False\n",
    "            if geohash_col == 'geohash_5':\n",
    "                write_header_5 = False\n",
    "            else:\n",
    "                write_header_3 = False\n",
    "            \n",
    "        logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba042a-244e-4f9e-ac06-74e87aec0221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd0a4d0-8ea5-495a-881b-7633a3c85d9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export to schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e34172-ee99-417e-b800-e6a93bc1f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "# output_schema_name = 'presence_data'\n",
    "output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_date = '2019-11-01'\n",
    "end_date = '2019-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "country_abbre = country_code.lower()  \n",
    "master_table_3 = f\"pd_{country_abbre}_2019_3h_agg3\"\n",
    "master_table_5 = f\"pd_{country_abbre}_2019_3h_agg5\"\n",
    "\n",
    "# Create the master tables if they do not exist\n",
    "create_table_query_3 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_3}(\n",
    "    geohash_3 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "create_table_query_5 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_5}(\n",
    "    geohash_5 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "    \n",
    "with con.connect() as connection:\n",
    "    logging.info(f\"Creating master table: {master_table_3}\")\n",
    "    connection.execute(create_table_query_3)\n",
    "    logging.info(f\"Creating master table: {master_table_5}\")\n",
    "    connection.execute(create_table_query_5)\n",
    "\n",
    "# List to record errors\n",
    "error_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536c09d-ea52-406b-9c88-ee97c376a49c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "\n",
    "            # Process for geohash_5\n",
    "            logging.info(f\"Aggregating data for geohash_5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = pe_dl_table_gen.groupby('geohash_5').agg(\n",
    "                no_of_points=('geohash_5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_5.to_sql(master_table_5, con, if_exists='append', index=False)\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into {master_table_3}\")\n",
    "\n",
    "            # Process for geohash_3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = pe_dl_table_gen.groupby('geohash_3').agg(\n",
    "                no_of_points=('geohash_3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_3.to_sql(master_table_3, con, if_exists='append', index=False, method='multi')\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into the {master_table_5}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee53328-3554-4860-8729-bc48a13a89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288c67d-370a-496f-911b-e743e9642d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d2e15f9-3fc3-4141-a782-71054cd873bd",
   "metadata": {},
   "source": [
    "# Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da2b5f41-d736-4644-98cb-b1951ee368ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0rfr</td>\n",
       "      <td>106</td>\n",
       "      <td>21</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20u8</td>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20u9</td>\n",
       "      <td>307</td>\n",
       "      <td>48</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d21nc</td>\n",
       "      <td>1073</td>\n",
       "      <td>170</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d21p1</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232296</th>\n",
       "      <td>d650p</td>\n",
       "      <td>149</td>\n",
       "      <td>27</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232297</th>\n",
       "      <td>d6h1s</td>\n",
       "      <td>634</td>\n",
       "      <td>80</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232298</th>\n",
       "      <td>d6h1t</td>\n",
       "      <td>280</td>\n",
       "      <td>54</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232299</th>\n",
       "      <td>d6h8e</td>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232300</th>\n",
       "      <td>d6h8s</td>\n",
       "      <td>190</td>\n",
       "      <td>29</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231815 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geohash_5 no_of_points no_of_unique_users  \\\n",
       "0          d0rfr          106                 21   \n",
       "1          d20u8           69                 16   \n",
       "2          d20u9          307                 48   \n",
       "3          d21nc         1073                170   \n",
       "4          d21p1           23                 11   \n",
       "...          ...          ...                ...   \n",
       "232296     d650p          149                 27   \n",
       "232297     d6h1s          634                 80   \n",
       "232298     d6h1t          280                 54   \n",
       "232299     d6h8e           87                 21   \n",
       "232300     d6h8s          190                 29   \n",
       "\n",
       "                          local_time  local_date  \n",
       "0       20191101 00:00:00 - 03:00:00    20191101  \n",
       "1       20191101 00:00:00 - 03:00:00    20191101  \n",
       "2       20191101 00:00:00 - 03:00:00    20191101  \n",
       "3       20191101 00:00:00 - 03:00:00    20191101  \n",
       "4       20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                              ...         ...  \n",
       "232296  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232297  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232298  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232299  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232300  20191108 09:00:00 - 12:00:00    20191108  \n",
       "\n",
       "[231815 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the file paths\n",
    "export_file_path_5 = '/home/jovyan/Data/pd3/pd_co_2019_agg5_3h.csv'\n",
    "export_file_path_3 = '/home/jovyan/Data/pd3/pd_co_2019_agg3_3h.csv'\n",
    "\n",
    "# Read the geohash_5 CSV file to get the column names\n",
    "data_5 = pd.read_csv(export_file_path_5)\n",
    "data_5['local_date'] = pd.to_numeric(data_5['local_date'], errors='coerce')\n",
    "data_5 = data_5.dropna(subset=['local_date'])\n",
    "data_5['local_date'] = data_5['local_date'].astype(int)\n",
    "\n",
    "data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28a91dee-d7fc-41a9-8a2b-8e3176eacf22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0r</td>\n",
       "      <td>125</td>\n",
       "      <td>24</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20</td>\n",
       "      <td>678</td>\n",
       "      <td>107</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d21</td>\n",
       "      <td>1498</td>\n",
       "      <td>242</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d22</td>\n",
       "      <td>253</td>\n",
       "      <td>38</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d23</td>\n",
       "      <td>1636</td>\n",
       "      <td>289</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>d3s</td>\n",
       "      <td>210</td>\n",
       "      <td>20</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>d3u</td>\n",
       "      <td>1091</td>\n",
       "      <td>100</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19584</th>\n",
       "      <td>d4j</td>\n",
       "      <td>1140</td>\n",
       "      <td>94</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19585</th>\n",
       "      <td>d65</td>\n",
       "      <td>247</td>\n",
       "      <td>41</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586</th>\n",
       "      <td>d6h</td>\n",
       "      <td>1644</td>\n",
       "      <td>170</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19587 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      geohash_5  no_of_points  no_of_unique_users  \\\n",
       "0           d0r           125                  24   \n",
       "1           d20           678                 107   \n",
       "2           d21          1498                 242   \n",
       "3           d22           253                  38   \n",
       "4           d23          1636                 289   \n",
       "...         ...           ...                 ...   \n",
       "19582       d3s           210                  20   \n",
       "19583       d3u          1091                 100   \n",
       "19584       d4j          1140                  94   \n",
       "19585       d65           247                  41   \n",
       "19586       d6h          1644                 170   \n",
       "\n",
       "                         local_time  local_date  \n",
       "0      20191101 00:00:00 - 03:00:00    20191101  \n",
       "1      20191101 00:00:00 - 03:00:00    20191101  \n",
       "2      20191101 00:00:00 - 03:00:00    20191101  \n",
       "3      20191101 00:00:00 - 03:00:00    20191101  \n",
       "4      20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                             ...         ...  \n",
       "19582  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19583  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19584  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19585  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19586  20191108 09:00:00 - 12:00:00    20191108  \n",
       "\n",
       "[19587 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the geohash_3 CSV file without headers\n",
    "data_3 = pd.read_csv(export_file_path_3, header=None)\n",
    "\n",
    "data_3.columns = data_5.columns # Assign column names from geohash_5 to geohash_3 for CO\n",
    "\n",
    "data_3['local_date'] = pd.to_numeric(data_3['local_date'], errors='coerce')\n",
    "data_3 = data_3.dropna(subset=['local_date'])\n",
    "data_3['local_date'] = data_3['local_date'].astype(int)\n",
    "\n",
    "data_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea29a4-9899-4b1b-92da-97707a4b33f9",
   "metadata": {},
   "source": [
    "## Format Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "515e23c3-c693-4a06-b41f-242070d07922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0r</td>\n",
       "      <td>125</td>\n",
       "      <td>24</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d34</td>\n",
       "      <td>21285</td>\n",
       "      <td>3198</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d35</td>\n",
       "      <td>1191</td>\n",
       "      <td>197</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d36</td>\n",
       "      <td>591</td>\n",
       "      <td>89</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d37</td>\n",
       "      <td>2493</td>\n",
       "      <td>392</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>d3d</td>\n",
       "      <td>7267</td>\n",
       "      <td>695</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>d39</td>\n",
       "      <td>497</td>\n",
       "      <td>43</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19584</th>\n",
       "      <td>d37</td>\n",
       "      <td>3988</td>\n",
       "      <td>462</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19585</th>\n",
       "      <td>d3j</td>\n",
       "      <td>148</td>\n",
       "      <td>13</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586</th>\n",
       "      <td>d23</td>\n",
       "      <td>3353</td>\n",
       "      <td>375</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19587 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      geohash_5  no_of_points  no_of_unique_users  \\\n",
       "0           d0r           125                  24   \n",
       "1           d34         21285                3198   \n",
       "2           d35          1191                 197   \n",
       "3           d36           591                  89   \n",
       "4           d37          2493                 392   \n",
       "...         ...           ...                 ...   \n",
       "19582       d3d          7267                 695   \n",
       "19583       d39           497                  43   \n",
       "19584       d37          3988                 462   \n",
       "19585       d3j           148                  13   \n",
       "19586       d23          3353                 375   \n",
       "\n",
       "                         local_time  local_date          start_time  \n",
       "0      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "1      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "2      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "3      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "4      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "...                             ...         ...                 ...  \n",
       "19582  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19583  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19584  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19585  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19586  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "\n",
       "[19587 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the start time from local_time and convert to datetime\n",
    "data_5['start_time'] = data_5['local_time'].str.split(' - ').str[0]\n",
    "data_3['start_time'] = data_3['local_time'].str.split(' - ').str[0]\n",
    "\n",
    "data_5['start_time'] = pd.to_datetime(data_5['start_time'], format='%Y%m%d %H:%M:%S')\n",
    "data_3['start_time'] = pd.to_datetime(data_3['start_time'], format='%Y%m%d %H:%M:%S')\n",
    "\n",
    "# Sort the data by start_time\n",
    "data_5_sorted = data_5.sort_values(by='start_time').reset_index(drop=True)\n",
    "data_3_sorted = data_3.sort_values(by='start_time').reset_index(drop=True)\n",
    "data_3_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9044d4d-1a7c-43e2-8ceb-ff3e9698f856",
   "metadata": {},
   "source": [
    "## Check output missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6cd81e0-ac68-43d4-b9f5-e3fcfb5346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the full range of 3-hour intervals\n",
    "start_date = '2019-11-01'\n",
    "end_date = '2019-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='3H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d92852e0-2782-4feb-b162-ca25aeb2e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals in geohash_5 data:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing intervals in geohash_5 data\n",
    "data_5_intervals = data_5_sorted['start_time']\n",
    "missing_intervals_5 = date_range.difference(data_5_intervals)\n",
    "print(\"Missing intervals in geohash_5 data:\")\n",
    "print(missing_intervals_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2989532e-7104-4e34-9acc-d33326375f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals in geohash_3 data:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing intervals in geohash_3 data\n",
    "data_3_intervals = data_3_sorted['start_time']\n",
    "missing_intervals_3 = date_range.difference(data_3_intervals)\n",
    "print(\"Missing intervals in geohash_3 data:\")\n",
    "print(missing_intervals_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96a156-6fb7-4e17-99a6-476924af9c1d",
   "metadata": {},
   "source": [
    "## Overwrite output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f90d2e8-7292-49a7-87b0-a81272ef7568",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0rfr</td>\n",
       "      <td>106</td>\n",
       "      <td>21</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d3473</td>\n",
       "      <td>166</td>\n",
       "      <td>40</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d3478</td>\n",
       "      <td>2829</td>\n",
       "      <td>516</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3479</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d347b</td>\n",
       "      <td>2333</td>\n",
       "      <td>420</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231810</th>\n",
       "      <td>d345y</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231811</th>\n",
       "      <td>d345z</td>\n",
       "      <td>294</td>\n",
       "      <td>62</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231812</th>\n",
       "      <td>d3467</td>\n",
       "      <td>304</td>\n",
       "      <td>43</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231813</th>\n",
       "      <td>d3401</td>\n",
       "      <td>165</td>\n",
       "      <td>18</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231814</th>\n",
       "      <td>d2gk0</td>\n",
       "      <td>114</td>\n",
       "      <td>15</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231815 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geohash_5 no_of_points no_of_unique_users  \\\n",
       "0          d0rfr          106                 21   \n",
       "1          d3473          166                 40   \n",
       "2          d3478         2829                516   \n",
       "3          d3479           57                 18   \n",
       "4          d347b         2333                420   \n",
       "...          ...          ...                ...   \n",
       "231810     d345y           30                 11   \n",
       "231811     d345z          294                 62   \n",
       "231812     d3467          304                 43   \n",
       "231813     d3401          165                 18   \n",
       "231814     d2gk0          114                 15   \n",
       "\n",
       "                          local_time  local_date  \n",
       "0       20191101 00:00:00 - 03:00:00    20191101  \n",
       "1       20191101 00:00:00 - 03:00:00    20191101  \n",
       "2       20191101 00:00:00 - 03:00:00    20191101  \n",
       "3       20191101 00:00:00 - 03:00:00    20191101  \n",
       "4       20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                              ...         ...  \n",
       "231810  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231811  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231812  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231813  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231814  20191231 21:00:00 - 24:00:00    20191231  \n",
       "\n",
       "[231815 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3_sorted = data_3_sorted.drop(['start_time'], axis=1)\n",
    "data_5_sorted = data_5_sorted.drop(['start_time'], axis=1)\n",
    "data_5_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "196c5ec9-d14d-454f-b186-143db0bcde2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the cleaned and sorted data back to the original paths, overwriting the existing files\n",
    "data_5_sorted.to_csv(export_file_path_5, index=False)\n",
    "data_3_sorted.to_csv(export_file_path_3, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5b18-b41f-4ade-be91-23cadc1ff2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c6003-99eb-46b3-922c-ed0cfed2cf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344be9b-be6f-4c9a-9445-6063316e8e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd3dda5-cf68-4ef5-bd61-89cc137a60ed",
   "metadata": {},
   "source": [
    "# tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45622ce-9901-45b1-9805-0252db6c76ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Worked query per day. - not working for MX\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fe83-0e29-441c-ae94-3913a9ea56ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055991e-c5d8-44c1-b790-961344e56c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa0f7-2b33-4d15-8970-63048d233858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1175b728-44a5-419f-9d80-2b87bed95a4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# !!!!!!!!!!!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09410c-5fd9-43e4-9e2c-96af3d2afb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "# output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'CO'\n",
    "start_date = 20191101\n",
    "end_date = 20191102\n",
    "# longitude_ranges = [(-82, -74.53125), (-74.53125, -65)]  # CO specific longitude ranges\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "failed_inserts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b9d47-428f-427a-8db5-40e7ef9dbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2019-11-12'\n",
    "end_date = '2019-11-11'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020a18-0fb9-4bee-b368-37b9309b61ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Loop through each day in the date range\n",
    "# for current_date in date_range:\n",
    "#     try:\n",
    "#         formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "#         # Calculate the lookback and lookahead dates\n",
    "#         lookback_date = current_date - timedelta(days=1)\n",
    "#         lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "#         # Format dates for the SQL query in 'yyyymmdd' format\n",
    "#         formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "#         formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "#         formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "#         # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "#         query = f\"\"\"\n",
    "#         SELECT \n",
    "#             cuebiq_id, \n",
    "#             event_zoned_datetime, \n",
    "#             processing_date,\n",
    "#             lat,\n",
    "#             lng,\n",
    "#             TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "#             EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "#             geohash_encode(lat, lng, 5) AS geohash5,\n",
    "#             geohash_encode(lat, lng, 3) AS geohash3\n",
    "#         FROM {pe_dl_table}\n",
    "#         WHERE \n",
    "#             processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "#             AND country_code = '{country_code}' \n",
    "#             AND event_zoned_datetime IS NOT NULL\n",
    "#             AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#             AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "#         \"\"\"\n",
    "        \n",
    "#         logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "#         pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "#         # Convert event_datetime_local to datetime\n",
    "#         pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "#         # Loop through each 3-hour interval\n",
    "#         for start_hour in range(0, 24, 3):\n",
    "#             end_hour = start_hour + 3\n",
    "            \n",
    "#             # Filter data for the current 3-hour interval\n",
    "#             interval_data = pe_dl_table_gen[\n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "#             ].copy()\n",
    "            \n",
    "#             # Create 3-hour interval column\n",
    "#             interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "#             interval_data['3_hour_interval'] = interval\n",
    "            \n",
    "#             # Aggregate data for geohash5\n",
    "#             logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "#                 no_of_points=('geohash5', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash5\n",
    "#             logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash5\n",
    "#             write_header_5 = False\n",
    "            \n",
    "#             # Aggregate data for geohash3\n",
    "#             logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "#                 no_of_points=('geohash3', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash3\n",
    "#             logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash3\n",
    "#             write_header_3 = False\n",
    "            \n",
    "#             logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "#     # Move to the next day\n",
    "#     current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f97b3a-022f-4b9b-9983-213d2dff3fe1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adc895-13b8-4190-8651-894e4bad5697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fb058-0ded-431a-85aa-a3bfd92c51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash5 and geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash5,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 5) AS geohash5,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash5\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL5 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5 = sql_engine.read_sql(query)\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash3,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 3) AS geohash3,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash3\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL3 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3 = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash5\n",
    "        logging.info(f\"Exporting data to {export_file_path_5} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash5\n",
    "        write_header_5 = False\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash3\n",
    "        logging.info(f\"Exporting data to {export_file_path_3} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash3\n",
    "        write_header_3 = False\n",
    "\n",
    "        logging.info(f\"Appended data for date {formatted_current_date} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c587e04-766d-4c83-9234-f6bd93e6fff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5c1fb-bce9-4098-bef6-1b2dc952b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a1127-0a10-4c27-a131-467026fbae94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c274-2c3d-4198-8d89-2c127eb4e280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c91b-b3c2-4e93-9b79-6411d7b47f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8948ce-1ebc-42c4-af54-844999063440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116555cb-e34a-4303-b77d-9a91e2dad215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bcea9-c6aa-46f1-a2c5-2d246d6716dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf4250-178e-474b-8b33-efb5705d612a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "\n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "        # Calculate geohashes\n",
    "        logging.info(f\"Processing geohashes for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "\n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "\n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "\n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "\n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "\n",
    "            # logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e18cd-7712-4ce5-8af2-17132ddfb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c1a10-ca49-4978-80af-bf1b1b436979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f70d6c-14ee-4bb3-a144-8f9d22756993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849c172-25b4-4015-9c3f-4c54c6f9b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f35f29-e38b-4be2-bbc4-b5603656734a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a575-0da2-445d-a070-9d5e3a37309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f2b3f-068f-441d-b29b-785322d09091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101c3db-396b-4176-b0da-f46abb48a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721c5a7-4c76-4278-a9b9-6bc0ab54028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75951-dd0a-4bc0-80ef-2e3cb2a2f8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d343d50-9c6c-4c8b-a774-2cec03b0b627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283036a-b031-4b32-8ae0-3cc1cdad8652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34e23-66c7-485c-807c-3476a4344dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abfca8-eb70-4e28-8fb1-0665f28903e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16774b-eb43-4b9b-b101-955588f36125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8862327-eaba-46a0-8ecb-5e142581315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965534c4-906e-4714-8dae-a12a9a7d83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd686a0-4c83-4154-a4f7-37acd1960ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64342f59-eaf3-41d2-aabd-932a4994830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e038-fb28-4124-a130-d6d2b36350f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54a6f6-e75b-4644-801c-dd37105ee619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b67b-9e18-4c61-8756-3a8aaae6384c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee41c-424a-40dd-8ed8-62a8764d629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed2e44-ad40-4863-9a63-637f70c3775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5481759-bef6-47f8-84fd-bd7bd04d43dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8a3f-9bbe-4abe-9fe0-236738e72528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ef14e-1604-4c31-b723-cddf9aab9947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfdaea-78a1-4a00-9dc3-6fa793f6fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one seemms to be working \n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    try:\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Construct the SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        \n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e495e-3f2b-4872-b595-61c1f8242278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926ef6e-772d-442f-9416-55c3c6c5c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d2b9d-7e31-4ad1-824b-5cd8a23c95e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f16fd-3cfc-4ea4-87ee-baca476c647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0f9e2-49ff-452e-8f32-d84cb913dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51349b29-b674-4f30-83bd-9f6386e5655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd167fd2-7c44-4a94-9b9c-b9b5dca82d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check by single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b7184-2831-4507-8682-3af4bdd08f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46018200-ab33-4c97-bf10-53fbca072c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "import os\n",
    "\n",
    "# Define the export file path\n",
    "export_file_path_3 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv'\n",
    "export_file_path_5 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv'\n",
    "\n",
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Initialize flags to write the headers only once\n",
    "write_header_5 = True\n",
    "write_header_3 = True\n",
    "\n",
    "# Loop through each 3-hour interval\n",
    "formatted_current_date = pd.to_datetime(formatted_current_date)  # Ensure it's a datetime object\n",
    "for start_hour in range(0, 24, 3):\n",
    "    end_hour = start_hour + 3\n",
    "    \n",
    "    # Filter data for the current 3-hour interval\n",
    "    interval_data = pe_dl_table_gen[\n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "    ].copy()\n",
    "    \n",
    "    # Create 3-hour interval column\n",
    "    interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "    interval_data['3_hour_interval'] = interval\n",
    "    \n",
    "    # Aggregate data for geohash5\n",
    "    aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "        no_of_points=('geohash5', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash5\n",
    "    filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash5\n",
    "    write_header_5 = False\n",
    "    \n",
    "    # Aggregate data for geohash3\n",
    "    aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "        no_of_points=('geohash3', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash3\n",
    "    filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash3\n",
    "    write_header_3 = False\n",
    "    \n",
    "    print(f\"Appended data for interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af6712-172d-42e8-92ef-e193c08202ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a73f2-0941-4640-b810-139180fc4924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369faf8d-fc67-4b50-99e2-698af62fb47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696fe4f-4b3c-456f-9862-69ae758fc1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915eb14-07e3-496b-b8b2-733d01d80cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bb1c2a-a79d-4575-823d-d60ae8d68441",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc27da9-ce7c-4c81-9baa-9948cc38d47e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a8dd2-4a7e-4ab0-8b90-ad328598d301",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate geohashes\n",
    "pe_dl_table_gen['geohash5'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "pe_dl_table_gen['geohash3'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44cc33-43de-4484-ba65-c9ee31f13ba2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fc0e4-8076-47f9-8f45-eebbc712092b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3d588-1f07-4eb7-9fde-b9b8e34f6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb012c-6879-460f-b74e-ce3c3afac10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a0b31-b807-4379-a859-4fedd6e15832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab1ede-6b29-4dc0-812d-70013cacb04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ce8cd-9b8e-409c-b680-508c00dc6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17829f-2722-48d6-975c-9a3bb6623f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc2e21-e91d-4109-ac04-0c5a2de5b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0353dd-55c7-42de-837f-119f48be6435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2c057-683c-451d-8970-6b6d8e5f8172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad075e-4927-4a9c-ab40-ed648ec5dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c21ab-230e-4951-998a-2f67576f87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279564a-3432-451d-9f63-bd40672b375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aabfc8-5afd-4644-bde7-5055d567d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964deb-74d2-482b-9cd2-812b2da03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808822b-920e-4541-9c13-b335f8b43a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630359c-25d2-4a9c-be72-973dd6c58e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71e37f-1292-4161-8bbf-b5fdc1114214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
