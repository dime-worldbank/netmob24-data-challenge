{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-geohash in /srv/conda/envs/notebook/lib/python3.9/site-packages (0.8.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import geohash\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab6cd4-e6a8-4554-9d57-3e788f2f9b0d",
   "metadata": {},
   "source": [
    "# Export to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f794a88-314e-4c93-96a8-34e9767fdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12fceee-4db7-47ca-adaf-1ca2009d1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data for a specific date\n",
    "def process_data_for_date(event_date, country_code, export_path):\n",
    "    errors = []  # List to keep track of errors\n",
    "    \n",
    "    # Define the export file paths\n",
    "    csv3_file = f\"od_{country_code.lower()}_agg3_3h.csv\"\n",
    "    csv5_file = f\"od_{country_code.lower()}_agg5_3h.csv\"\n",
    "    csv3_file_path = f\"{export_path}{csv3_file}\"\n",
    "    csv5_file_path = f\"{export_path}{csv5_file}\"\n",
    "\n",
    "    # Loop through 24 hours in increments of 3 hours\n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3  # end hour for the 3-hour interval\n",
    "        logging.info(f\"Querying data for date: {event_date}, hours: {start_hour}-{end_hour}\")\n",
    "\n",
    "        try:\n",
    "            # Fetch the data for the specified event date and country code\n",
    "            pe_tj_df3 = sql_engine.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT \n",
    "                    cuebiq_id,\n",
    "                    duration_minutes,\n",
    "                    length_meters,\n",
    "                    number_of_points,\n",
    "                    TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                    EXTRACT(HOUR FROM TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                    geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "                    geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "                    geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "                    geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "                    DATE_FORMAT(date_parse(substr(start_zoned_datetime, 1, 10), '%Y-%m-%d'), '%Y%m%d') AS local_date\n",
    "                FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "                WHERE \n",
    "                    event_date = {event_date}\n",
    "                    AND end_country = '{country_code}' \n",
    "                    AND start_country = '{country_code}' \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            # Filter the DataFrame for the current 3-hour interval\n",
    "            logging.info(f\"Processing data for date: {event_date}, hours: {start_hour}-{end_hour}\")\n",
    "            filtered_df = pe_tj_df3[(pe_tj_df3['event_hour'] >= start_hour) & (pe_tj_df3['event_hour'] < end_hour)]\n",
    "\n",
    "            # Add user numbers to the aggregated data for geohash3\n",
    "            aggregated_df3 = filtered_df.groupby(['start_geohash3', 'end_geohash3', 'local_date']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std']\n",
    "            }).reset_index()\n",
    "\n",
    "            # Flatten the MultiIndex columns\n",
    "            aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'local_date', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "            # Add the local_time column\n",
    "            aggregated_df3['local_time'] = aggregated_df3['local_date'].astype(str) + \\\n",
    "                                           f\" {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "            # Filter out rows where trip_count is less than or equal to 9\n",
    "            aggregated_df3 = aggregated_df3[aggregated_df3['trip_count'] > 9]\n",
    "\n",
    "            # Append the results to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting aggregated data (geohash3) for date: {event_date}, hours: {start_hour}-{end_hour}\")\n",
    "            if not os.path.isfile(csv3_file_path):\n",
    "                aggregated_df3.to_csv(csv3_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df3.to_csv(csv3_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "            # Add user numbers to the aggregated data for geohash5\n",
    "            aggregated_df5 = filtered_df.groupby(['start_geohash5', 'end_geohash5', 'local_date']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std']\n",
    "            }).reset_index()\n",
    "\n",
    "            # Flatten the MultiIndex columns\n",
    "            aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'local_date', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "            # Add the local_time column\n",
    "            aggregated_df5['local_time'] = aggregated_df5['local_date'].astype(str) + \\\n",
    "                                           f\" {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "            # Filter out rows where trip_count is less than or equal to 9\n",
    "            aggregated_df5 = aggregated_df5[aggregated_df5['trip_count'] > 9]\n",
    "\n",
    "            # Append the results to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting aggregated data (geohash5) for date: {event_date}, hours: {start_hour}-{end_hour}\")\n",
    "            if not os.path.isfile(csv5_file_path):\n",
    "                aggregated_df5.to_csv(csv5_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df5.to_csv(csv5_file_path, mode='a', header=False, index=False)\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing date: {event_date}, hours: {start_hour}-{end_hour} - {str(e)}\"\n",
    "            logging.error(error_message)\n",
    "            errors.append(error_message)\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# Function to process data for a range of dates\n",
    "def process_data_for_date_range(start_date, end_date, country_code, export_path):\n",
    "    current_date = start_date\n",
    "    all_errors = []\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime(\"%Y%m%d\")\n",
    "        logging.info(f\"Processing data for date: {event_date}\")\n",
    "        errors = process_data_for_date(event_date, country_code, export_path)\n",
    "        all_errors.extend(errors)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    # Log any errors that occurred\n",
    "    if all_errors:\n",
    "        logging.info(\"Errors occurred during processing:\")\n",
    "        for error in all_errors:\n",
    "            logging.info(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8559dcca-4b9f-489a-9a06-5527018e5001",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 19:50:40,770 - INFO - Processing data for date: 20190809\n",
      "2024-07-02 19:50:40,771 - INFO - Querying data for date: 20190809, hours: 0-3\n",
      "2024-07-02 19:51:16,147 - INFO - Processing data for date: 20190809, hours: 0-3\n",
      "2024-07-02 19:51:16,159 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 0-3\n",
      "2024-07-02 19:51:16,183 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 0-3\n",
      "2024-07-02 19:51:16,197 - INFO - Querying data for date: 20190809, hours: 3-6\n",
      "2024-07-02 19:51:41,186 - INFO - Processing data for date: 20190809, hours: 3-6\n",
      "2024-07-02 19:51:41,204 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 3-6\n",
      "2024-07-02 19:51:41,233 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 3-6\n",
      "2024-07-02 19:51:41,248 - INFO - Querying data for date: 20190809, hours: 6-9\n",
      "2024-07-02 19:52:06,038 - INFO - Processing data for date: 20190809, hours: 6-9\n",
      "2024-07-02 19:52:06,071 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 6-9\n",
      "2024-07-02 19:52:06,128 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 6-9\n",
      "2024-07-02 19:52:06,148 - INFO - Querying data for date: 20190809, hours: 9-12\n",
      "2024-07-02 19:52:31,051 - INFO - Processing data for date: 20190809, hours: 9-12\n",
      "2024-07-02 19:52:31,095 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 9-12\n",
      "2024-07-02 19:52:31,151 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 9-12\n",
      "2024-07-02 19:52:31,175 - INFO - Querying data for date: 20190809, hours: 12-15\n",
      "2024-07-02 19:52:59,220 - INFO - Processing data for date: 20190809, hours: 12-15\n",
      "2024-07-02 19:52:59,265 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 12-15\n",
      "2024-07-02 19:52:59,315 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 12-15\n",
      "2024-07-02 19:52:59,338 - INFO - Querying data for date: 20190809, hours: 15-18\n",
      "2024-07-02 19:53:30,792 - INFO - Processing data for date: 20190809, hours: 15-18\n",
      "2024-07-02 19:53:30,836 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 15-18\n",
      "2024-07-02 19:53:30,890 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 15-18\n",
      "2024-07-02 19:53:30,911 - INFO - Querying data for date: 20190809, hours: 18-21\n",
      "2024-07-02 19:54:10,160 - INFO - Processing data for date: 20190809, hours: 18-21\n",
      "2024-07-02 19:54:10,199 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 18-21\n",
      "2024-07-02 19:54:10,273 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 18-21\n",
      "2024-07-02 19:54:10,294 - INFO - Querying data for date: 20190809, hours: 21-24\n",
      "2024-07-02 19:54:42,469 - INFO - Processing data for date: 20190809, hours: 21-24\n",
      "2024-07-02 19:54:42,496 - INFO - Exporting aggregated data (geohash3) for date: 20190809, hours: 21-24\n",
      "2024-07-02 19:54:42,529 - INFO - Exporting aggregated data (geohash5) for date: 20190809, hours: 21-24\n",
      "2024-07-02 19:54:42,587 - INFO - Processing data for date: 20190810\n",
      "2024-07-02 19:54:42,588 - INFO - Querying data for date: 20190810, hours: 0-3\n",
      "2024-07-02 19:55:19,783 - INFO - Processing data for date: 20190810, hours: 0-3\n",
      "2024-07-02 19:55:19,795 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 0-3\n",
      "2024-07-02 19:55:19,818 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 0-3\n",
      "2024-07-02 19:55:19,830 - INFO - Querying data for date: 20190810, hours: 3-6\n",
      "2024-07-02 19:55:48,731 - INFO - Processing data for date: 20190810, hours: 3-6\n",
      "2024-07-02 19:55:48,750 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 3-6\n",
      "2024-07-02 19:55:48,779 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 3-6\n",
      "2024-07-02 19:55:48,792 - INFO - Querying data for date: 20190810, hours: 6-9\n",
      "2024-07-02 19:56:13,010 - INFO - Processing data for date: 20190810, hours: 6-9\n",
      "2024-07-02 19:56:13,040 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 6-9\n",
      "2024-07-02 19:56:13,083 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 6-9\n",
      "2024-07-02 19:56:13,101 - INFO - Querying data for date: 20190810, hours: 9-12\n",
      "2024-07-02 19:56:42,506 - INFO - Processing data for date: 20190810, hours: 9-12\n",
      "2024-07-02 19:56:42,546 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 9-12\n",
      "2024-07-02 19:56:42,601 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 9-12\n",
      "2024-07-02 19:56:42,622 - INFO - Querying data for date: 20190810, hours: 12-15\n",
      "2024-07-02 19:57:17,645 - INFO - Processing data for date: 20190810, hours: 12-15\n",
      "2024-07-02 19:57:17,688 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 12-15\n",
      "2024-07-02 19:57:17,738 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 12-15\n",
      "2024-07-02 19:57:17,761 - INFO - Querying data for date: 20190810, hours: 15-18\n",
      "2024-07-02 19:58:06,411 - INFO - Processing data for date: 20190810, hours: 15-18\n",
      "2024-07-02 19:58:06,453 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 15-18\n",
      "2024-07-02 19:58:06,503 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 15-18\n",
      "2024-07-02 19:58:06,526 - INFO - Querying data for date: 20190810, hours: 18-21\n",
      "2024-07-02 19:58:30,722 - INFO - Processing data for date: 20190810, hours: 18-21\n",
      "2024-07-02 19:58:30,761 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 18-21\n",
      "2024-07-02 19:58:30,808 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 18-21\n",
      "2024-07-02 19:58:30,828 - INFO - Querying data for date: 20190810, hours: 21-24\n",
      "2024-07-02 19:58:59,791 - INFO - Processing data for date: 20190810, hours: 21-24\n",
      "2024-07-02 19:58:59,819 - INFO - Exporting aggregated data (geohash3) for date: 20190810, hours: 21-24\n",
      "2024-07-02 19:58:59,850 - INFO - Exporting aggregated data (geohash5) for date: 20190810, hours: 21-24\n",
      "2024-07-02 19:58:59,910 - INFO - Processing data for date: 20190811\n",
      "2024-07-02 19:58:59,911 - INFO - Querying data for date: 20190811, hours: 0-3\n",
      "2024-07-02 19:59:34,521 - INFO - Processing data for date: 20190811, hours: 0-3\n",
      "2024-07-02 19:59:34,534 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 0-3\n",
      "2024-07-02 19:59:34,556 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 0-3\n",
      "2024-07-02 19:59:34,568 - INFO - Querying data for date: 20190811, hours: 3-6\n",
      "2024-07-02 19:59:58,138 - INFO - Processing data for date: 20190811, hours: 3-6\n",
      "2024-07-02 19:59:58,155 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 3-6\n",
      "2024-07-02 19:59:58,185 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 3-6\n",
      "2024-07-02 19:59:58,197 - INFO - Querying data for date: 20190811, hours: 6-9\n",
      "2024-07-02 20:00:28,528 - INFO - Processing data for date: 20190811, hours: 6-9\n",
      "2024-07-02 20:00:28,563 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 6-9\n",
      "2024-07-02 20:00:28,608 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 6-9\n",
      "2024-07-02 20:00:28,626 - INFO - Querying data for date: 20190811, hours: 9-12\n",
      "2024-07-02 20:01:00,810 - INFO - Processing data for date: 20190811, hours: 9-12\n",
      "2024-07-02 20:01:00,851 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 9-12\n",
      "2024-07-02 20:01:00,901 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 9-12\n",
      "2024-07-02 20:01:00,925 - INFO - Querying data for date: 20190811, hours: 12-15\n",
      "2024-07-02 20:01:33,246 - INFO - Processing data for date: 20190811, hours: 12-15\n",
      "2024-07-02 20:01:33,284 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 12-15\n",
      "2024-07-02 20:01:33,336 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 12-15\n",
      "2024-07-02 20:01:33,356 - INFO - Querying data for date: 20190811, hours: 15-18\n",
      "2024-07-02 20:02:04,527 - INFO - Processing data for date: 20190811, hours: 15-18\n",
      "2024-07-02 20:02:04,563 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 15-18\n",
      "2024-07-02 20:02:04,608 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 15-18\n",
      "2024-07-02 20:02:04,627 - INFO - Querying data for date: 20190811, hours: 18-21\n",
      "2024-07-02 20:02:30,645 - INFO - Processing data for date: 20190811, hours: 18-21\n",
      "2024-07-02 20:02:30,678 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 18-21\n",
      "2024-07-02 20:02:30,721 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 18-21\n",
      "2024-07-02 20:02:30,739 - INFO - Querying data for date: 20190811, hours: 21-24\n",
      "2024-07-02 20:02:55,521 - INFO - Processing data for date: 20190811, hours: 21-24\n",
      "2024-07-02 20:02:55,545 - INFO - Exporting aggregated data (geohash3) for date: 20190811, hours: 21-24\n",
      "2024-07-02 20:02:55,609 - INFO - Exporting aggregated data (geohash5) for date: 20190811, hours: 21-24\n",
      "2024-07-02 20:02:55,658 - INFO - Processing data for date: 20190812\n",
      "2024-07-02 20:02:55,659 - INFO - Querying data for date: 20190812, hours: 0-3\n",
      "2024-07-02 20:03:25,972 - INFO - Processing data for date: 20190812, hours: 0-3\n",
      "2024-07-02 20:03:25,984 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 0-3\n",
      "2024-07-02 20:03:26,006 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 0-3\n",
      "2024-07-02 20:03:26,017 - INFO - Querying data for date: 20190812, hours: 3-6\n",
      "2024-07-02 20:03:58,231 - INFO - Processing data for date: 20190812, hours: 3-6\n",
      "2024-07-02 20:03:58,249 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 3-6\n",
      "2024-07-02 20:03:58,282 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 3-6\n",
      "2024-07-02 20:03:58,298 - INFO - Querying data for date: 20190812, hours: 6-9\n",
      "2024-07-02 20:04:53,661 - INFO - Processing data for date: 20190812, hours: 6-9\n",
      "2024-07-02 20:04:53,691 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 6-9\n",
      "2024-07-02 20:04:53,734 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 6-9\n",
      "2024-07-02 20:04:53,755 - INFO - Querying data for date: 20190812, hours: 9-12\n",
      "2024-07-02 20:05:23,427 - INFO - Processing data for date: 20190812, hours: 9-12\n",
      "2024-07-02 20:05:23,469 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 9-12\n",
      "2024-07-02 20:05:23,521 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 9-12\n",
      "2024-07-02 20:05:23,542 - INFO - Querying data for date: 20190812, hours: 12-15\n",
      "2024-07-02 20:05:49,931 - INFO - Processing data for date: 20190812, hours: 12-15\n",
      "2024-07-02 20:05:49,978 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 12-15\n",
      "2024-07-02 20:05:50,042 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 12-15\n",
      "2024-07-02 20:05:50,064 - INFO - Querying data for date: 20190812, hours: 15-18\n",
      "2024-07-02 20:06:22,781 - INFO - Processing data for date: 20190812, hours: 15-18\n",
      "2024-07-02 20:06:22,825 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 15-18\n",
      "2024-07-02 20:06:22,880 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 15-18\n",
      "2024-07-02 20:06:22,905 - INFO - Querying data for date: 20190812, hours: 18-21\n",
      "2024-07-02 20:07:00,829 - INFO - Processing data for date: 20190812, hours: 18-21\n",
      "2024-07-02 20:07:00,867 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 18-21\n",
      "2024-07-02 20:07:00,912 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 18-21\n",
      "2024-07-02 20:07:00,941 - INFO - Querying data for date: 20190812, hours: 21-24\n",
      "2024-07-02 20:07:40,103 - INFO - Processing data for date: 20190812, hours: 21-24\n",
      "2024-07-02 20:07:40,128 - INFO - Exporting aggregated data (geohash3) for date: 20190812, hours: 21-24\n",
      "2024-07-02 20:07:40,159 - INFO - Exporting aggregated data (geohash5) for date: 20190812, hours: 21-24\n",
      "2024-07-02 20:07:40,207 - INFO - Processing data for date: 20190813\n",
      "2024-07-02 20:07:40,208 - INFO - Querying data for date: 20190813, hours: 0-3\n",
      "2024-07-02 20:08:19,744 - INFO - Processing data for date: 20190813, hours: 0-3\n",
      "2024-07-02 20:08:19,756 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 0-3\n",
      "2024-07-02 20:08:19,778 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 0-3\n",
      "2024-07-02 20:08:19,789 - INFO - Querying data for date: 20190813, hours: 3-6\n",
      "2024-07-02 20:08:44,946 - INFO - Processing data for date: 20190813, hours: 3-6\n",
      "2024-07-02 20:08:44,964 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 3-6\n",
      "2024-07-02 20:08:44,992 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 3-6\n",
      "2024-07-02 20:08:45,009 - INFO - Querying data for date: 20190813, hours: 6-9\n",
      "2024-07-02 20:09:14,892 - INFO - Processing data for date: 20190813, hours: 6-9\n",
      "2024-07-02 20:09:14,924 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 6-9\n",
      "2024-07-02 20:09:14,970 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 6-9\n",
      "2024-07-02 20:09:14,991 - INFO - Querying data for date: 20190813, hours: 9-12\n",
      "2024-07-02 20:09:50,126 - INFO - Processing data for date: 20190813, hours: 9-12\n",
      "2024-07-02 20:09:50,167 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 9-12\n",
      "2024-07-02 20:09:50,220 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 9-12\n",
      "2024-07-02 20:09:50,250 - INFO - Querying data for date: 20190813, hours: 12-15\n",
      "2024-07-02 20:10:21,029 - INFO - Processing data for date: 20190813, hours: 12-15\n",
      "2024-07-02 20:10:21,075 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 12-15\n",
      "2024-07-02 20:10:21,126 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 12-15\n",
      "2024-07-02 20:10:21,163 - INFO - Querying data for date: 20190813, hours: 15-18\n",
      "2024-07-02 20:10:48,485 - INFO - Processing data for date: 20190813, hours: 15-18\n",
      "2024-07-02 20:10:48,533 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 15-18\n",
      "2024-07-02 20:10:48,585 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 15-18\n",
      "2024-07-02 20:10:48,615 - INFO - Querying data for date: 20190813, hours: 18-21\n",
      "2024-07-02 20:11:13,590 - INFO - Processing data for date: 20190813, hours: 18-21\n",
      "2024-07-02 20:11:13,629 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 18-21\n",
      "2024-07-02 20:11:13,673 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 18-21\n",
      "2024-07-02 20:11:13,693 - INFO - Querying data for date: 20190813, hours: 21-24\n",
      "2024-07-02 20:11:41,097 - INFO - Processing data for date: 20190813, hours: 21-24\n",
      "2024-07-02 20:11:41,123 - INFO - Exporting aggregated data (geohash3) for date: 20190813, hours: 21-24\n",
      "2024-07-02 20:11:41,154 - INFO - Exporting aggregated data (geohash5) for date: 20190813, hours: 21-24\n",
      "2024-07-02 20:11:41,208 - INFO - Processing data for date: 20190814\n",
      "2024-07-02 20:11:41,208 - INFO - Querying data for date: 20190814, hours: 0-3\n",
      "2024-07-02 20:12:09,495 - INFO - Processing data for date: 20190814, hours: 0-3\n",
      "2024-07-02 20:12:09,506 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 0-3\n",
      "2024-07-02 20:12:09,531 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 0-3\n",
      "2024-07-02 20:12:09,542 - INFO - Querying data for date: 20190814, hours: 3-6\n",
      "2024-07-02 20:12:35,071 - INFO - Processing data for date: 20190814, hours: 3-6\n",
      "2024-07-02 20:12:35,090 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 3-6\n",
      "2024-07-02 20:12:35,123 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 3-6\n",
      "2024-07-02 20:12:35,138 - INFO - Querying data for date: 20190814, hours: 6-9\n",
      "2024-07-02 20:13:00,605 - INFO - Processing data for date: 20190814, hours: 6-9\n",
      "2024-07-02 20:13:00,636 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 6-9\n",
      "2024-07-02 20:13:00,683 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 6-9\n",
      "2024-07-02 20:13:00,701 - INFO - Querying data for date: 20190814, hours: 9-12\n",
      "2024-07-02 20:13:34,493 - INFO - Processing data for date: 20190814, hours: 9-12\n",
      "2024-07-02 20:13:34,537 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 9-12\n",
      "2024-07-02 20:13:34,588 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 9-12\n",
      "2024-07-02 20:13:34,614 - INFO - Querying data for date: 20190814, hours: 12-15\n",
      "2024-07-02 20:14:00,491 - INFO - Processing data for date: 20190814, hours: 12-15\n",
      "2024-07-02 20:14:00,537 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 12-15\n",
      "2024-07-02 20:14:00,592 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 12-15\n",
      "2024-07-02 20:14:00,616 - INFO - Querying data for date: 20190814, hours: 15-18\n",
      "2024-07-02 20:14:22,701 - INFO - Processing data for date: 20190814, hours: 15-18\n",
      "2024-07-02 20:14:22,745 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 15-18\n",
      "2024-07-02 20:14:22,798 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 15-18\n",
      "2024-07-02 20:14:22,820 - INFO - Querying data for date: 20190814, hours: 18-21\n",
      "2024-07-02 20:14:54,671 - INFO - Processing data for date: 20190814, hours: 18-21\n",
      "2024-07-02 20:14:54,709 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 18-21\n",
      "2024-07-02 20:14:54,757 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 18-21\n",
      "2024-07-02 20:14:54,778 - INFO - Querying data for date: 20190814, hours: 21-24\n",
      "2024-07-02 20:15:16,736 - INFO - Processing data for date: 20190814, hours: 21-24\n",
      "2024-07-02 20:15:16,762 - INFO - Exporting aggregated data (geohash3) for date: 20190814, hours: 21-24\n",
      "2024-07-02 20:15:16,814 - INFO - Exporting aggregated data (geohash5) for date: 20190814, hours: 21-24\n",
      "2024-07-02 20:15:16,870 - INFO - Processing data for date: 20190815\n",
      "2024-07-02 20:15:16,871 - INFO - Querying data for date: 20190815, hours: 0-3\n",
      "2024-07-02 20:15:55,018 - INFO - Processing data for date: 20190815, hours: 0-3\n",
      "2024-07-02 20:15:55,031 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 0-3\n",
      "2024-07-02 20:15:55,054 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 0-3\n",
      "2024-07-02 20:15:55,068 - INFO - Querying data for date: 20190815, hours: 3-6\n",
      "2024-07-02 20:16:23,685 - INFO - Processing data for date: 20190815, hours: 3-6\n",
      "2024-07-02 20:16:23,703 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 3-6\n",
      "2024-07-02 20:16:23,730 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 3-6\n",
      "2024-07-02 20:16:23,743 - INFO - Querying data for date: 20190815, hours: 6-9\n",
      "2024-07-02 20:16:57,709 - INFO - Processing data for date: 20190815, hours: 6-9\n",
      "2024-07-02 20:16:57,742 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 6-9\n",
      "2024-07-02 20:16:57,791 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 6-9\n",
      "2024-07-02 20:16:57,811 - INFO - Querying data for date: 20190815, hours: 9-12\n",
      "2024-07-02 20:17:23,895 - INFO - Processing data for date: 20190815, hours: 9-12\n",
      "2024-07-02 20:17:23,940 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 9-12\n",
      "2024-07-02 20:17:23,992 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 9-12\n",
      "2024-07-02 20:17:24,014 - INFO - Querying data for date: 20190815, hours: 12-15\n",
      "2024-07-02 20:17:52,821 - INFO - Processing data for date: 20190815, hours: 12-15\n",
      "2024-07-02 20:17:52,869 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 12-15\n",
      "2024-07-02 20:17:52,923 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 12-15\n",
      "2024-07-02 20:17:52,952 - INFO - Querying data for date: 20190815, hours: 15-18\n",
      "2024-07-02 20:18:19,034 - INFO - Processing data for date: 20190815, hours: 15-18\n",
      "2024-07-02 20:18:19,082 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 15-18\n",
      "2024-07-02 20:18:19,137 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 15-18\n",
      "2024-07-02 20:18:19,164 - INFO - Querying data for date: 20190815, hours: 18-21\n",
      "2024-07-02 20:18:47,976 - INFO - Processing data for date: 20190815, hours: 18-21\n",
      "2024-07-02 20:18:48,022 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 18-21\n",
      "2024-07-02 20:18:48,075 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 18-21\n",
      "2024-07-02 20:18:48,098 - INFO - Querying data for date: 20190815, hours: 21-24\n",
      "2024-07-02 20:19:24,755 - INFO - Processing data for date: 20190815, hours: 21-24\n",
      "2024-07-02 20:19:24,786 - INFO - Exporting aggregated data (geohash3) for date: 20190815, hours: 21-24\n",
      "2024-07-02 20:19:24,821 - INFO - Exporting aggregated data (geohash5) for date: 20190815, hours: 21-24\n",
      "2024-07-02 20:19:24,881 - INFO - Processing data for date: 20190816\n",
      "2024-07-02 20:19:24,882 - INFO - Querying data for date: 20190816, hours: 0-3\n",
      "2024-07-02 20:20:44,970 - INFO - Processing data for date: 20190816, hours: 0-3\n",
      "2024-07-02 20:20:44,984 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 0-3\n",
      "2024-07-02 20:20:45,007 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 0-3\n",
      "2024-07-02 20:20:45,018 - INFO - Querying data for date: 20190816, hours: 3-6\n",
      "2024-07-02 20:21:30,988 - INFO - Processing data for date: 20190816, hours: 3-6\n",
      "2024-07-02 20:21:31,008 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 3-6\n",
      "2024-07-02 20:21:31,040 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 3-6\n",
      "2024-07-02 20:21:31,054 - INFO - Querying data for date: 20190816, hours: 6-9\n",
      "2024-07-02 20:22:04,429 - INFO - Processing data for date: 20190816, hours: 6-9\n",
      "2024-07-02 20:22:04,478 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 6-9\n",
      "2024-07-02 20:22:04,562 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 6-9\n",
      "2024-07-02 20:22:04,589 - INFO - Querying data for date: 20190816, hours: 9-12\n",
      "2024-07-02 20:22:31,538 - INFO - Processing data for date: 20190816, hours: 9-12\n",
      "2024-07-02 20:22:31,594 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 9-12\n",
      "2024-07-02 20:22:31,660 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 9-12\n",
      "2024-07-02 20:22:31,687 - INFO - Querying data for date: 20190816, hours: 12-15\n",
      "2024-07-02 20:23:06,292 - INFO - Processing data for date: 20190816, hours: 12-15\n",
      "2024-07-02 20:23:06,349 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 12-15\n",
      "2024-07-02 20:23:06,410 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 12-15\n",
      "2024-07-02 20:23:06,440 - INFO - Querying data for date: 20190816, hours: 15-18\n",
      "2024-07-02 20:23:46,446 - INFO - Processing data for date: 20190816, hours: 15-18\n",
      "2024-07-02 20:23:46,499 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 15-18\n",
      "2024-07-02 20:23:46,556 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 15-18\n",
      "2024-07-02 20:23:46,591 - INFO - Querying data for date: 20190816, hours: 18-21\n",
      "2024-07-02 20:24:19,689 - INFO - Processing data for date: 20190816, hours: 18-21\n",
      "2024-07-02 20:24:19,740 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 18-21\n",
      "2024-07-02 20:24:19,793 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 18-21\n",
      "2024-07-02 20:24:19,824 - INFO - Querying data for date: 20190816, hours: 21-24\n",
      "2024-07-02 20:24:58,089 - INFO - Processing data for date: 20190816, hours: 21-24\n",
      "2024-07-02 20:24:58,124 - INFO - Exporting aggregated data (geohash3) for date: 20190816, hours: 21-24\n",
      "2024-07-02 20:24:58,162 - INFO - Exporting aggregated data (geohash5) for date: 20190816, hours: 21-24\n",
      "2024-07-02 20:24:58,235 - INFO - Processing data for date: 20190817\n",
      "2024-07-02 20:24:58,236 - INFO - Querying data for date: 20190817, hours: 0-3\n",
      "2024-07-02 20:25:36,555 - INFO - Processing data for date: 20190817, hours: 0-3\n",
      "2024-07-02 20:25:36,568 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 0-3\n",
      "2024-07-02 20:25:36,593 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 0-3\n",
      "2024-07-02 20:25:36,605 - INFO - Querying data for date: 20190817, hours: 3-6\n",
      "2024-07-02 20:26:00,974 - INFO - Processing data for date: 20190817, hours: 3-6\n",
      "2024-07-02 20:26:00,994 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 3-6\n",
      "2024-07-02 20:26:01,024 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 3-6\n",
      "2024-07-02 20:26:01,038 - INFO - Querying data for date: 20190817, hours: 6-9\n",
      "2024-07-02 20:26:26,402 - INFO - Processing data for date: 20190817, hours: 6-9\n",
      "2024-07-02 20:26:26,439 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 6-9\n",
      "2024-07-02 20:26:26,490 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 6-9\n",
      "2024-07-02 20:26:26,512 - INFO - Querying data for date: 20190817, hours: 9-12\n",
      "2024-07-02 20:26:58,815 - INFO - Processing data for date: 20190817, hours: 9-12\n",
      "2024-07-02 20:26:58,863 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 9-12\n",
      "2024-07-02 20:26:58,922 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 9-12\n",
      "2024-07-02 20:26:58,947 - INFO - Querying data for date: 20190817, hours: 12-15\n",
      "2024-07-02 20:27:36,860 - INFO - Processing data for date: 20190817, hours: 12-15\n",
      "2024-07-02 20:27:36,909 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 12-15\n",
      "2024-07-02 20:27:36,964 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 12-15\n",
      "2024-07-02 20:27:37,002 - INFO - Querying data for date: 20190817, hours: 15-18\n",
      "2024-07-02 20:28:14,114 - INFO - Processing data for date: 20190817, hours: 15-18\n",
      "2024-07-02 20:28:14,199 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 15-18\n",
      "2024-07-02 20:28:14,267 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 15-18\n",
      "2024-07-02 20:28:14,328 - INFO - Querying data for date: 20190817, hours: 18-21\n",
      "2024-07-02 20:28:40,080 - INFO - Processing data for date: 20190817, hours: 18-21\n",
      "2024-07-02 20:28:40,128 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 18-21\n",
      "2024-07-02 20:28:40,188 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 18-21\n",
      "2024-07-02 20:28:40,213 - INFO - Querying data for date: 20190817, hours: 21-24\n",
      "2024-07-02 20:29:05,158 - INFO - Processing data for date: 20190817, hours: 21-24\n",
      "2024-07-02 20:29:05,192 - INFO - Exporting aggregated data (geohash3) for date: 20190817, hours: 21-24\n",
      "2024-07-02 20:29:05,228 - INFO - Exporting aggregated data (geohash5) for date: 20190817, hours: 21-24\n",
      "2024-07-02 20:29:05,297 - INFO - Processing data for date: 20190818\n",
      "2024-07-02 20:29:05,298 - INFO - Querying data for date: 20190818, hours: 0-3\n",
      "2024-07-02 20:29:52,714 - INFO - Processing data for date: 20190818, hours: 0-3\n",
      "2024-07-02 20:29:52,728 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 0-3\n",
      "2024-07-02 20:29:52,755 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 0-3\n",
      "2024-07-02 20:29:52,768 - INFO - Querying data for date: 20190818, hours: 3-6\n",
      "2024-07-02 20:30:23,728 - INFO - Processing data for date: 20190818, hours: 3-6\n",
      "2024-07-02 20:30:23,748 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 3-6\n",
      "2024-07-02 20:30:23,779 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 3-6\n",
      "2024-07-02 20:30:23,795 - INFO - Querying data for date: 20190818, hours: 6-9\n",
      "2024-07-02 20:30:54,903 - INFO - Processing data for date: 20190818, hours: 6-9\n",
      "2024-07-02 20:30:54,937 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 6-9\n",
      "2024-07-02 20:30:54,985 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 6-9\n",
      "2024-07-02 20:30:55,007 - INFO - Querying data for date: 20190818, hours: 9-12\n",
      "2024-07-02 20:31:25,572 - INFO - Processing data for date: 20190818, hours: 9-12\n",
      "2024-07-02 20:31:25,618 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 9-12\n",
      "2024-07-02 20:31:25,674 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 9-12\n",
      "2024-07-02 20:31:25,705 - INFO - Querying data for date: 20190818, hours: 12-15\n",
      "2024-07-02 20:32:14,592 - INFO - Processing data for date: 20190818, hours: 12-15\n",
      "2024-07-02 20:32:14,642 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 12-15\n",
      "2024-07-02 20:32:14,699 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 12-15\n",
      "2024-07-02 20:32:14,726 - INFO - Querying data for date: 20190818, hours: 15-18\n",
      "2024-07-02 20:32:42,397 - INFO - Processing data for date: 20190818, hours: 15-18\n",
      "2024-07-02 20:32:42,446 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 15-18\n",
      "2024-07-02 20:32:42,504 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 15-18\n",
      "2024-07-02 20:32:42,526 - INFO - Querying data for date: 20190818, hours: 18-21\n",
      "2024-07-02 20:33:06,946 - INFO - Processing data for date: 20190818, hours: 18-21\n",
      "2024-07-02 20:33:07,002 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 18-21\n",
      "2024-07-02 20:33:07,054 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 18-21\n",
      "2024-07-02 20:33:07,079 - INFO - Querying data for date: 20190818, hours: 21-24\n",
      "2024-07-02 20:33:30,984 - INFO - Processing data for date: 20190818, hours: 21-24\n",
      "2024-07-02 20:33:31,016 - INFO - Exporting aggregated data (geohash3) for date: 20190818, hours: 21-24\n",
      "2024-07-02 20:33:31,056 - INFO - Exporting aggregated data (geohash5) for date: 20190818, hours: 21-24\n",
      "2024-07-02 20:33:31,120 - INFO - Processing data for date: 20190819\n",
      "2024-07-02 20:33:31,121 - INFO - Querying data for date: 20190819, hours: 0-3\n",
      "2024-07-02 20:34:28,202 - INFO - Processing data for date: 20190819, hours: 0-3\n",
      "2024-07-02 20:34:28,214 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 0-3\n",
      "2024-07-02 20:34:28,238 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 0-3\n",
      "2024-07-02 20:34:28,250 - INFO - Querying data for date: 20190819, hours: 3-6\n",
      "2024-07-02 20:35:12,612 - INFO - Processing data for date: 20190819, hours: 3-6\n",
      "2024-07-02 20:35:12,632 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 3-6\n",
      "2024-07-02 20:35:12,668 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 3-6\n",
      "2024-07-02 20:35:12,683 - INFO - Querying data for date: 20190819, hours: 6-9\n",
      "2024-07-02 20:35:42,976 - INFO - Processing data for date: 20190819, hours: 6-9\n",
      "2024-07-02 20:35:43,014 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 6-9\n",
      "2024-07-02 20:35:43,071 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 6-9\n",
      "2024-07-02 20:35:43,092 - INFO - Querying data for date: 20190819, hours: 9-12\n",
      "2024-07-02 20:36:11,851 - INFO - Processing data for date: 20190819, hours: 9-12\n",
      "2024-07-02 20:36:11,903 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 9-12\n",
      "2024-07-02 20:36:11,964 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 9-12\n",
      "2024-07-02 20:36:11,988 - INFO - Querying data for date: 20190819, hours: 12-15\n",
      "2024-07-02 20:36:41,246 - INFO - Processing data for date: 20190819, hours: 12-15\n",
      "2024-07-02 20:36:41,338 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 12-15\n",
      "2024-07-02 20:36:41,416 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 12-15\n",
      "2024-07-02 20:36:41,446 - INFO - Querying data for date: 20190819, hours: 15-18\n",
      "2024-07-02 20:37:18,926 - INFO - Processing data for date: 20190819, hours: 15-18\n",
      "2024-07-02 20:37:18,980 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 15-18\n",
      "2024-07-02 20:37:19,042 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 15-18\n",
      "2024-07-02 20:37:19,071 - INFO - Querying data for date: 20190819, hours: 18-21\n",
      "2024-07-02 20:37:57,906 - INFO - Processing data for date: 20190819, hours: 18-21\n",
      "2024-07-02 20:37:57,953 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 18-21\n",
      "2024-07-02 20:37:58,004 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 18-21\n",
      "2024-07-02 20:37:58,029 - INFO - Querying data for date: 20190819, hours: 21-24\n",
      "2024-07-02 20:38:35,426 - INFO - Processing data for date: 20190819, hours: 21-24\n",
      "2024-07-02 20:38:35,456 - INFO - Exporting aggregated data (geohash3) for date: 20190819, hours: 21-24\n",
      "2024-07-02 20:38:35,491 - INFO - Exporting aggregated data (geohash5) for date: 20190819, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date range\n",
    "start_date = datetime.strptime(\"20190809\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20190819\", \"%Y%m%d\")\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "\n",
    "# Process data for the specified date range\n",
    "process_data_for_date_range(start_date, end_date, country_code, export_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898ef46-3a75-4d8f-bf2a-66ab932251d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d65f3-5b6a-402b-8d08-92b5c3878d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5efb5b1-0518-4a68-a728-c19d5db39dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ab6357-f150-4bf2-8ebb-7952b8782800",
   "metadata": {},
   "source": [
    "# Fillin Gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce5e04-9099-4510-80b7-cbc4be4b2bd9",
   "metadata": {},
   "source": [
    "Missing info: \n",
    "Missing hours for each date:\n",
    "         Date        Missing Hours\n",
    "0  2019-01-02              [0, 21]\n",
    "1  2019-01-03                  [0]\n",
    "2  2019-01-07                  [0]\n",
    "3  2019-01-09                  [0]\n",
    "4  2019-01-10                  [0]\n",
    "5  2019-01-11                  [0]\n",
    "6  2019-01-14                  [0]\n",
    "7  2019-01-15                  [0]\n",
    "8  2019-01-16                  [0]\n",
    "9  2019-01-18                  [0]\n",
    "10 2019-01-21                  [0]\n",
    "11 2019-01-22                  [0]\n",
    "12 2019-01-25                  [0]\n",
    "13 2019-01-28                  [0]\n",
    "14 2019-01-29                  [0]\n",
    "15 2019-02-02                  [0]\n",
    "16 2019-04-04                  [0]\n",
    "17 2019-04-12                  [3]\n",
    "18 2019-04-29                  [0]\n",
    "19 2019-05-26                 [21]\n",
    "20 2019-06-06                 [12]\n",
    "21 2019-08-01             [18, 21]\n",
    "22 2019-08-31                 [18]\n",
    "23 2019-09-08                  [3]\n",
    "24 2019-10-17                 [21]\n",
    "25 2019-10-22  [9, 12, 15, 18, 21]\n",
    "26 2019-10-28                  [6]\n",
    "27 2019-11-26                  [0]\n",
    "28 2019-11-27                  [0]\n",
    "29 2019-11-28                  [0]\n",
    "30 2019-12-02                  [0]\n",
    "31 2019-12-03                  [0]\n",
    "32 2019-12-05                  [0]\n",
    "33 2019-12-06                  [0]\n",
    "34 2019-12-07                  [0]\n",
    "35 2019-12-08                  [0]\n",
    "36 2019-12-09                  [0]\n",
    "37 2019-12-10                  [0]\n",
    "38 2019-12-11                  [0]\n",
    "39 2019-12-12                  [0]\n",
    "40 2019-12-13                  [0]\n",
    "41 2019-12-17                  [0]\n",
    "42 2019-12-18                  [0]\n",
    "43 2019-12-19                  [0]\n",
    "44 2019-12-20                  [0]\n",
    "45 2019-12-21                  [0]\n",
    "46 2019-12-23                  [0]\n",
    "47 2019-12-24                  [0]\n",
    "48 2019-12-25                  [0]\n",
    "49 2019-12-26                  [0]\n",
    "50 2019-12-27                  [0]\n",
    "51 2019-12-28                  [0]\n",
    "52 2019-12-29                  [0]\n",
    "53 2019-12-30                  [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277d1d3e-ab95-44c8-a4f9-e8929691170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a specific date and time intervals\n",
    "def process_data_for_date(event_date, country_code, export_path, start_hour, end_hour):\n",
    "    # Define the export file paths\n",
    "    csv3_file = f\"od_{country_code.lower()}3_agg3_3h.csv\"\n",
    "    csv5_file = f\"od_{country_code.lower()}3_agg5_3h.csv\"\n",
    "    csv3_file_path = f\"{export_path}{csv3_file}\"\n",
    "    csv5_file_path = f\"{export_path}{csv5_file}\"\n",
    "\n",
    "    # Loop through the specified hours in increments of 3 hours\n",
    "    for hour in range(start_hour, end_hour, 3):\n",
    "        next_hour = hour + 3  # end hour for the 3-hour interval\n",
    "        logging.info(f\"Querying data for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "\n",
    "        # Fetch the data for the specified event date and country code\n",
    "        pe_tj_df3 = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points,\n",
    "                TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "                geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "                geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "                geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "                DATE_FORMAT(date_parse(substr(start_zoned_datetime, 1, 10), '%Y-%m-%d'), '%Y%m%d') AS local_date\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Filter the DataFrame for the current 3-hour interval\n",
    "        logging.info(f\"Processing data for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "        filtered_df = pe_tj_df3[(pe_tj_df3['event_hour'] >= hour) & (pe_tj_df3['event_hour'] < next_hour)]\n",
    "\n",
    "        # Add user numbers to the aggregated data for geohash3\n",
    "        aggregated_df3 = filtered_df.groupby(['start_geohash3', 'end_geohash3', 'local_date']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten the MultiIndex columns\n",
    "        aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'local_date', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "        # Add the local_time column\n",
    "        aggregated_df3['local_time'] = aggregated_df3['local_date'].astype(str) + \\\n",
    "                                       f\" {hour:02d}:00:00 - {next_hour:02d}:00:00\"\n",
    "\n",
    "        # Filter out rows where trip_count is less than or equal to 9\n",
    "        aggregated_df3 = aggregated_df3[aggregated_df3['trip_count'] > 9]\n",
    "\n",
    "        # Append the results to the CSV file for geohash3\n",
    "        logging.info(f\"Exporting aggregated data (geohash3) for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "        if not os.path.isfile(csv3_file_path):\n",
    "            aggregated_df3.to_csv(csv3_file_path, index=False)\n",
    "        else:\n",
    "            aggregated_df3.to_csv(csv3_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "        # Add user numbers to the aggregated data for geohash5\n",
    "        aggregated_df5 = filtered_df.groupby(['start_geohash5', 'end_geohash5', 'local_date']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten the MultiIndex columns\n",
    "        aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'local_date', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "        # Add the local_time column\n",
    "        aggregated_df5['local_time'] = aggregated_df5['local_date'].astype(str) + \\\n",
    "                                       f\" {hour:02d}:00:00 - {next_hour:02d}:00:00\"\n",
    "\n",
    "        # Filter out rows where trip_count is less than or equal to 9\n",
    "        aggregated_df5 = aggregated_df5[aggregated_df5['trip_count'] > 9]\n",
    "\n",
    "        # Append the results to the CSV file for geohash5\n",
    "        logging.info(f\"Exporting aggregated data (geohash5) for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "        if not os.path.isfile(csv5_file_path):\n",
    "            aggregated_df5.to_csv(csv5_file_path, index=False)\n",
    "        else:\n",
    "            aggregated_df5.to_csv(csv5_file_path, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f91819-1047-44ed-80ed-bf9a196ccf92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:27:07,056 - INFO - Querying data for date: 20190102, hours: 0-3\n",
      "2024-07-02 22:27:56,958 - INFO - Processing data for date: 20190102, hours: 0-3\n",
      "2024-07-02 22:27:56,969 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 0-3\n",
      "2024-07-02 22:27:56,989 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 0-3\n",
      "2024-07-02 22:27:56,991 - INFO - Querying data for date: 20190102, hours: 3-6\n",
      "2024-07-02 22:28:17,642 - INFO - Processing data for date: 20190102, hours: 3-6\n",
      "2024-07-02 22:28:17,655 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 3-6\n",
      "2024-07-02 22:28:17,678 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 3-6\n",
      "2024-07-02 22:28:17,684 - INFO - Querying data for date: 20190102, hours: 6-9\n",
      "2024-07-02 22:28:36,134 - INFO - Processing data for date: 20190102, hours: 6-9\n",
      "2024-07-02 22:28:36,156 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 6-9\n",
      "2024-07-02 22:28:36,192 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 6-9\n",
      "2024-07-02 22:28:36,203 - INFO - Querying data for date: 20190102, hours: 9-12\n",
      "2024-07-02 22:28:54,889 - INFO - Processing data for date: 20190102, hours: 9-12\n",
      "2024-07-02 22:28:54,910 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 9-12\n",
      "2024-07-02 22:28:54,944 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 9-12\n",
      "2024-07-02 22:28:54,956 - INFO - Querying data for date: 20190102, hours: 12-15\n",
      "2024-07-02 22:29:14,306 - INFO - Processing data for date: 20190102, hours: 12-15\n",
      "2024-07-02 22:29:14,330 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 12-15\n",
      "2024-07-02 22:29:14,364 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 12-15\n",
      "2024-07-02 22:29:14,384 - INFO - Querying data for date: 20190102, hours: 15-18\n",
      "2024-07-02 22:29:34,560 - INFO - Processing data for date: 20190102, hours: 15-18\n",
      "2024-07-02 22:29:34,584 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 15-18\n",
      "2024-07-02 22:29:34,633 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 15-18\n",
      "2024-07-02 22:29:34,657 - INFO - Querying data for date: 20190102, hours: 18-21\n",
      "2024-07-02 22:29:55,432 - INFO - Processing data for date: 20190102, hours: 18-21\n",
      "2024-07-02 22:29:55,453 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 18-21\n",
      "2024-07-02 22:29:55,497 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 18-21\n",
      "2024-07-02 22:29:55,515 - INFO - Querying data for date: 20190102, hours: 21-24\n",
      "2024-07-02 22:30:16,259 - INFO - Processing data for date: 20190102, hours: 21-24\n",
      "2024-07-02 22:30:16,274 - INFO - Exporting aggregated data (geohash3) for date: 20190102, hours: 21-24\n",
      "2024-07-02 22:30:16,298 - INFO - Exporting aggregated data (geohash5) for date: 20190102, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190102\"\n",
    "start_hour = 0\n",
    "end_hour = 24 \n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9c6566-6f70-4c8a-8ede-613e74bd3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:30:16,327 - INFO - Querying data for date: 20190412, hours: 3-6\n",
      "2024-07-02 22:31:01,187 - INFO - Processing data for date: 20190412, hours: 3-6\n",
      "2024-07-02 22:31:01,206 - INFO - Exporting aggregated data (geohash3) for date: 20190412, hours: 3-6\n",
      "2024-07-02 22:31:01,238 - INFO - Exporting aggregated data (geohash5) for date: 20190412, hours: 3-6\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190412\"\n",
    "start_hour = 3\n",
    "end_hour = 6\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "292baedd-4a3e-4b14-a0b6-81d4854522dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:31:01,296 - INFO - Querying data for date: 20190526, hours: 21-24\n",
      "2024-07-02 22:31:33,634 - INFO - Processing data for date: 20190526, hours: 21-24\n",
      "2024-07-02 22:31:33,651 - INFO - Exporting aggregated data (geohash3) for date: 20190526, hours: 21-24\n",
      "2024-07-02 22:31:33,678 - INFO - Exporting aggregated data (geohash5) for date: 20190526, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190526\"\n",
    "start_hour = 21\n",
    "end_hour = 24\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c26216b-f176-4b18-81db-f49fa9a1ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:31:33,717 - INFO - Querying data for date: 20190606, hours: 21-24\n",
      "2024-07-02 22:32:09,983 - INFO - Processing data for date: 20190606, hours: 21-24\n",
      "2024-07-02 22:32:10,003 - INFO - Exporting aggregated data (geohash3) for date: 20190606, hours: 21-24\n",
      "2024-07-02 22:32:10,052 - INFO - Exporting aggregated data (geohash5) for date: 20190606, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190606\"\n",
    "start_hour = 12\n",
    "end_hour = 15\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e677f02-98c6-4dc3-bf2e-04ca5e202fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:32:10,096 - INFO - Querying data for date: 20190801, hours: 18-21\n",
      "2024-07-02 22:32:50,603 - INFO - Processing data for date: 20190801, hours: 18-21\n",
      "2024-07-02 22:32:50,635 - INFO - Exporting aggregated data (geohash3) for date: 20190801, hours: 18-21\n",
      "2024-07-02 22:32:50,676 - INFO - Exporting aggregated data (geohash5) for date: 20190801, hours: 18-21\n",
      "2024-07-02 22:32:50,699 - INFO - Querying data for date: 20190801, hours: 21-24\n",
      "2024-07-02 22:33:26,361 - INFO - Processing data for date: 20190801, hours: 21-24\n",
      "2024-07-02 22:33:26,388 - INFO - Exporting aggregated data (geohash3) for date: 20190801, hours: 21-24\n",
      "2024-07-02 22:33:26,417 - INFO - Exporting aggregated data (geohash5) for date: 20190801, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190801\"\n",
    "start_hour = 18\n",
    "end_hour = 24\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd841aee-6f33-4689-bb67-38654623c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:33:26,475 - INFO - Querying data for date: 20190831, hours: 18-21\n",
      "2024-07-02 22:34:08,674 - INFO - Processing data for date: 20190831, hours: 18-21\n",
      "2024-07-02 22:34:08,715 - INFO - Exporting aggregated data (geohash3) for date: 20190831, hours: 18-21\n",
      "2024-07-02 22:34:08,773 - INFO - Exporting aggregated data (geohash5) for date: 20190831, hours: 18-21\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190831\"\n",
    "start_hour = 18\n",
    "end_hour = 21\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88c3d86-57c1-4425-8d18-29c05f0913cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:34:08,858 - INFO - Querying data for date: 20190908, hours: 3-6\n",
      "2024-07-02 22:34:50,203 - INFO - Processing data for date: 20190908, hours: 3-6\n",
      "2024-07-02 22:34:50,223 - INFO - Exporting aggregated data (geohash3) for date: 20190908, hours: 3-6\n",
      "2024-07-02 22:34:50,265 - INFO - Exporting aggregated data (geohash5) for date: 20190908, hours: 3-6\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20190908\"\n",
    "start_hour = 3\n",
    "end_hour = 6\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "168294de-4db5-4733-b990-8d6463f7cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:34:50,343 - INFO - Querying data for date: 20191017, hours: 21-24\n",
      "2024-07-02 22:35:33,092 - INFO - Processing data for date: 20191017, hours: 21-24\n",
      "2024-07-02 22:35:33,115 - INFO - Exporting aggregated data (geohash3) for date: 20191017, hours: 21-24\n",
      "2024-07-02 22:35:33,154 - INFO - Exporting aggregated data (geohash5) for date: 20191017, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20191017\"\n",
    "start_hour = 21\n",
    "end_hour = 24\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880c3bbc-d260-4119-83f4-8b4b0e55ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:35:33,233 - INFO - Querying data for date: 20191022, hours: 9-12\n",
      "2024-07-02 22:35:55,764 - INFO - Processing data for date: 20191022, hours: 9-12\n",
      "2024-07-02 22:35:55,774 - INFO - Exporting aggregated data (geohash3) for date: 20191022, hours: 9-12\n",
      "2024-07-02 22:35:55,784 - INFO - Exporting aggregated data (geohash5) for date: 20191022, hours: 9-12\n",
      "2024-07-02 22:35:55,788 - INFO - Querying data for date: 20191022, hours: 12-15\n",
      "2024-07-02 22:36:15,461 - INFO - Processing data for date: 20191022, hours: 12-15\n",
      "2024-07-02 22:36:15,471 - INFO - Exporting aggregated data (geohash3) for date: 20191022, hours: 12-15\n",
      "2024-07-02 22:36:15,480 - INFO - Exporting aggregated data (geohash5) for date: 20191022, hours: 12-15\n",
      "2024-07-02 22:36:15,483 - INFO - Querying data for date: 20191022, hours: 15-18\n",
      "2024-07-02 22:36:32,597 - INFO - Processing data for date: 20191022, hours: 15-18\n",
      "2024-07-02 22:36:32,607 - INFO - Exporting aggregated data (geohash3) for date: 20191022, hours: 15-18\n",
      "2024-07-02 22:36:32,617 - INFO - Exporting aggregated data (geohash5) for date: 20191022, hours: 15-18\n",
      "2024-07-02 22:36:32,619 - INFO - Querying data for date: 20191022, hours: 18-21\n",
      "2024-07-02 22:36:51,243 - INFO - Processing data for date: 20191022, hours: 18-21\n",
      "2024-07-02 22:36:51,255 - INFO - Exporting aggregated data (geohash3) for date: 20191022, hours: 18-21\n",
      "2024-07-02 22:36:51,265 - INFO - Exporting aggregated data (geohash5) for date: 20191022, hours: 18-21\n",
      "2024-07-02 22:36:51,266 - INFO - Querying data for date: 20191022, hours: 21-24\n",
      "2024-07-02 22:37:02,625 - INFO - Processing data for date: 20191022, hours: 21-24\n",
      "2024-07-02 22:37:02,636 - INFO - Exporting aggregated data (geohash3) for date: 20191022, hours: 21-24\n",
      "2024-07-02 22:37:02,647 - INFO - Exporting aggregated data (geohash5) for date: 20191022, hours: 21-24\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20191022\"\n",
    "start_hour = 9\n",
    "end_hour = 24\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e63cf53a-b412-45ac-9852-eef1213e83c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 22:37:02,658 - INFO - Querying data for date: 20191028, hours: 6-9\n",
      "2024-07-02 22:37:36,733 - INFO - Processing data for date: 20191028, hours: 6-9\n",
      "2024-07-02 22:37:36,769 - INFO - Exporting aggregated data (geohash3) for date: 20191028, hours: 6-9\n",
      "2024-07-02 22:37:36,820 - INFO - Exporting aggregated data (geohash5) for date: 20191028, hours: 6-9\n"
     ]
    }
   ],
   "source": [
    "# Specify the date and initial time interval\n",
    "event_date = \"20191028\"\n",
    "start_hour = 6\n",
    "end_hour = 9\n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified date and time intervals\n",
    "process_data_for_date(event_date, country_code, export_path, start_hour, end_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9948bc-9413-438d-9ddd-6c101e46d87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15288d55-5613-472a-b1ab-da879e20c3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4174e6-3951-4234-be53-e28c56906050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fb966-fcab-4a86-9516-af09247ca204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7159c211-e0a9-4aa3-8f88-931269922469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a specific date and time intervals\n",
    "def process_data_for_date(event_dates, country_code, export_path, start_hour, end_hour):\n",
    "    # Define the export file paths\n",
    "    csv3_file = f\"od_{country_code.lower()}3_agg3_3h.csv\"\n",
    "    csv5_file = f\"od_{country_code.lower()}3_agg5_3h.csv\"\n",
    "    csv3_file_path = f\"{export_path}{csv3_file}\"\n",
    "    csv5_file_path = f\"{export_path}{csv5_file}\"\n",
    "\n",
    "    # Loop through each date\n",
    "    for event_date in event_dates:\n",
    "        # Loop through the specified hours in increments of 3 hours\n",
    "        for hour in range(start_hour, end_hour, 3):\n",
    "            next_hour = hour + 3  # end hour for the 3-hour interval\n",
    "            logging.info(f\"Querying data for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "\n",
    "            # Fetch the data for the specified event date and country code\n",
    "            pe_tj_df3 = sql_engine.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT \n",
    "                    cuebiq_id,\n",
    "                    duration_minutes,\n",
    "                    length_meters,\n",
    "                    number_of_points,\n",
    "                    TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                    EXTRACT(HOUR FROM TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                    geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "                    geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "                    geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "                    geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "                    DATE_FORMAT(date_parse(substr(start_zoned_datetime, 1, 10), '%Y-%m-%d'), '%Y%m%d') AS local_date\n",
    "                FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "                WHERE \n",
    "                    event_date = {event_date}\n",
    "                    AND end_country = '{country_code}' \n",
    "                    AND start_country = '{country_code}' \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            # Filter the DataFrame for the current 3-hour interval\n",
    "            logging.info(f\"Processing data for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "            filtered_df = pe_tj_df3[(pe_tj_df3['event_hour'] >= hour) & (pe_tj_df3['event_hour'] < next_hour)]\n",
    "\n",
    "            # Add user numbers to the aggregated data for geohash3\n",
    "            aggregated_df3 = filtered_df.groupby(['start_geohash3', 'end_geohash3', 'local_date']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std']\n",
    "            }).reset_index()\n",
    "\n",
    "            # Flatten the MultiIndex columns\n",
    "            aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'local_date', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "            # Add the local_time column\n",
    "            aggregated_df3['local_time'] = aggregated_df3['local_date'].astype(str) + \\\n",
    "                                           f\" {hour:02d}:00:00 - {next_hour:02d}:00:00\"\n",
    "\n",
    "            # Filter out rows where trip_count is less than or equal to 9\n",
    "            aggregated_df3 = aggregated_df3[aggregated_df3['trip_count'] > 9]\n",
    "\n",
    "            # Append the results to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting aggregated data (geohash3) for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "            if not os.path.isfile(csv3_file_path):\n",
    "                aggregated_df3.to_csv(csv3_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df3.to_csv(csv3_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "            # Add user numbers to the aggregated data for geohash5\n",
    "            aggregated_df5 = filtered_df.groupby(['start_geohash5', 'end_geohash5', 'local_date']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std']\n",
    "            }).reset_index()\n",
    "\n",
    "            # Flatten the MultiIndex columns\n",
    "            aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'local_date', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "            # Add the local_time column\n",
    "            aggregated_df5['local_time'] = aggregated_df5['local_date'].astype(str) + \\\n",
    "                                           f\" {hour:02d}:00:00 - {next_hour:02d}:00:00\"\n",
    "\n",
    "            # Filter out rows where trip_count is less than or equal to 9\n",
    "            aggregated_df5 = aggregated_df5[aggregated_df5['trip_count'] > 9]\n",
    "\n",
    "            # Append the results to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting aggregated data (geohash5) for date: {event_date}, hours: {hour}-{next_hour}\")\n",
    "            if not os.path.isfile(csv5_file_path):\n",
    "                aggregated_df5.to_csv(csv5_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df5.to_csv(csv5_file_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d20b8d0-be7b-44e6-95a5-ba70650ede5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 21:59:54,830 - INFO - Querying data for date: 20190122, hours: 0-3\n",
      "2024-07-02 22:00:09,080 - INFO - Processing data for date: 20190122, hours: 0-3\n",
      "2024-07-02 22:00:09,092 - INFO - Exporting aggregated data (geohash3) for date: 20190122, hours: 0-3\n",
      "2024-07-02 22:00:09,112 - INFO - Exporting aggregated data (geohash5) for date: 20190122, hours: 0-3\n",
      "2024-07-02 22:00:09,114 - INFO - Querying data for date: 20190125, hours: 0-3\n",
      "2024-07-02 22:00:34,751 - INFO - Processing data for date: 20190125, hours: 0-3\n",
      "2024-07-02 22:00:34,763 - INFO - Exporting aggregated data (geohash3) for date: 20190125, hours: 0-3\n",
      "2024-07-02 22:00:34,779 - INFO - Exporting aggregated data (geohash5) for date: 20190125, hours: 0-3\n",
      "2024-07-02 22:00:34,782 - INFO - Querying data for date: 20190128, hours: 0-3\n",
      "2024-07-02 22:01:15,562 - INFO - Processing data for date: 20190128, hours: 0-3\n",
      "2024-07-02 22:01:15,574 - INFO - Exporting aggregated data (geohash3) for date: 20190128, hours: 0-3\n",
      "2024-07-02 22:01:15,591 - INFO - Exporting aggregated data (geohash5) for date: 20190128, hours: 0-3\n",
      "2024-07-02 22:01:15,592 - INFO - Querying data for date: 20190129, hours: 0-3\n",
      "2024-07-02 22:01:38,593 - INFO - Processing data for date: 20190129, hours: 0-3\n",
      "2024-07-02 22:01:38,605 - INFO - Exporting aggregated data (geohash3) for date: 20190129, hours: 0-3\n",
      "2024-07-02 22:01:38,621 - INFO - Exporting aggregated data (geohash5) for date: 20190129, hours: 0-3\n",
      "2024-07-02 22:01:38,623 - INFO - Querying data for date: 20190202, hours: 0-3\n",
      "2024-07-02 22:01:50,426 - INFO - Processing data for date: 20190202, hours: 0-3\n",
      "2024-07-02 22:01:50,438 - INFO - Exporting aggregated data (geohash3) for date: 20190202, hours: 0-3\n",
      "2024-07-02 22:01:50,455 - INFO - Exporting aggregated data (geohash5) for date: 20190202, hours: 0-3\n",
      "2024-07-02 22:01:50,457 - INFO - Querying data for date: 20190404, hours: 0-3\n",
      "2024-07-02 22:02:03,003 - INFO - Processing data for date: 20190404, hours: 0-3\n",
      "2024-07-02 22:02:03,016 - INFO - Exporting aggregated data (geohash3) for date: 20190404, hours: 0-3\n",
      "2024-07-02 22:02:03,032 - INFO - Exporting aggregated data (geohash5) for date: 20190404, hours: 0-3\n",
      "2024-07-02 22:02:03,034 - INFO - Querying data for date: 20190429, hours: 0-3\n",
      "2024-07-02 22:02:16,016 - INFO - Processing data for date: 20190429, hours: 0-3\n",
      "2024-07-02 22:02:16,029 - INFO - Exporting aggregated data (geohash3) for date: 20190429, hours: 0-3\n",
      "2024-07-02 22:02:16,048 - INFO - Exporting aggregated data (geohash5) for date: 20190429, hours: 0-3\n",
      "2024-07-02 22:02:16,050 - INFO - Querying data for date: 20191126, hours: 0-3\n",
      "2024-07-02 22:02:26,120 - INFO - Processing data for date: 20191126, hours: 0-3\n",
      "2024-07-02 22:02:26,132 - INFO - Exporting aggregated data (geohash3) for date: 20191126, hours: 0-3\n",
      "2024-07-02 22:02:26,148 - INFO - Exporting aggregated data (geohash5) for date: 20191126, hours: 0-3\n",
      "2024-07-02 22:02:26,150 - INFO - Querying data for date: 20191127, hours: 0-3\n",
      "2024-07-02 22:02:36,339 - INFO - Processing data for date: 20191127, hours: 0-3\n",
      "2024-07-02 22:02:36,350 - INFO - Exporting aggregated data (geohash3) for date: 20191127, hours: 0-3\n",
      "2024-07-02 22:02:36,366 - INFO - Exporting aggregated data (geohash5) for date: 20191127, hours: 0-3\n",
      "2024-07-02 22:02:36,368 - INFO - Querying data for date: 20191128, hours: 0-3\n",
      "2024-07-02 22:03:09,792 - INFO - Processing data for date: 20191128, hours: 0-3\n",
      "2024-07-02 22:03:09,803 - INFO - Exporting aggregated data (geohash3) for date: 20191128, hours: 0-3\n",
      "2024-07-02 22:03:09,819 - INFO - Exporting aggregated data (geohash5) for date: 20191128, hours: 0-3\n",
      "2024-07-02 22:03:09,821 - INFO - Querying data for date: 20191202, hours: 0-3\n",
      "2024-07-02 22:03:50,511 - INFO - Processing data for date: 20191202, hours: 0-3\n",
      "2024-07-02 22:03:50,523 - INFO - Exporting aggregated data (geohash3) for date: 20191202, hours: 0-3\n",
      "2024-07-02 22:03:50,540 - INFO - Exporting aggregated data (geohash5) for date: 20191202, hours: 0-3\n",
      "2024-07-02 22:03:50,542 - INFO - Querying data for date: 20191203, hours: 0-3\n",
      "2024-07-02 22:04:23,685 - INFO - Processing data for date: 20191203, hours: 0-3\n",
      "2024-07-02 22:04:23,696 - INFO - Exporting aggregated data (geohash3) for date: 20191203, hours: 0-3\n",
      "2024-07-02 22:04:23,712 - INFO - Exporting aggregated data (geohash5) for date: 20191203, hours: 0-3\n",
      "2024-07-02 22:04:23,714 - INFO - Querying data for date: 20191205, hours: 0-3\n",
      "2024-07-02 22:04:52,945 - INFO - Processing data for date: 20191205, hours: 0-3\n",
      "2024-07-02 22:04:52,956 - INFO - Exporting aggregated data (geohash3) for date: 20191205, hours: 0-3\n",
      "2024-07-02 22:04:52,972 - INFO - Exporting aggregated data (geohash5) for date: 20191205, hours: 0-3\n",
      "2024-07-02 22:04:52,974 - INFO - Querying data for date: 20191206, hours: 0-3\n",
      "2024-07-02 22:05:26,785 - INFO - Processing data for date: 20191206, hours: 0-3\n",
      "2024-07-02 22:05:26,797 - INFO - Exporting aggregated data (geohash3) for date: 20191206, hours: 0-3\n",
      "2024-07-02 22:05:26,816 - INFO - Exporting aggregated data (geohash5) for date: 20191206, hours: 0-3\n",
      "2024-07-02 22:05:26,818 - INFO - Querying data for date: 20191207, hours: 0-3\n",
      "2024-07-02 22:06:08,510 - INFO - Processing data for date: 20191207, hours: 0-3\n",
      "2024-07-02 22:06:08,522 - INFO - Exporting aggregated data (geohash3) for date: 20191207, hours: 0-3\n",
      "2024-07-02 22:06:08,539 - INFO - Exporting aggregated data (geohash5) for date: 20191207, hours: 0-3\n",
      "2024-07-02 22:06:08,541 - INFO - Querying data for date: 20191208, hours: 0-3\n",
      "2024-07-02 22:06:39,461 - INFO - Processing data for date: 20191208, hours: 0-3\n",
      "2024-07-02 22:06:39,472 - INFO - Exporting aggregated data (geohash3) for date: 20191208, hours: 0-3\n",
      "2024-07-02 22:06:39,488 - INFO - Exporting aggregated data (geohash5) for date: 20191208, hours: 0-3\n",
      "2024-07-02 22:06:39,490 - INFO - Querying data for date: 20191209, hours: 0-3\n",
      "2024-07-02 22:07:09,266 - INFO - Processing data for date: 20191209, hours: 0-3\n",
      "2024-07-02 22:07:09,278 - INFO - Exporting aggregated data (geohash3) for date: 20191209, hours: 0-3\n",
      "2024-07-02 22:07:09,294 - INFO - Exporting aggregated data (geohash5) for date: 20191209, hours: 0-3\n",
      "2024-07-02 22:07:09,296 - INFO - Querying data for date: 20191210, hours: 0-3\n",
      "2024-07-02 22:07:40,251 - INFO - Processing data for date: 20191210, hours: 0-3\n",
      "2024-07-02 22:07:40,262 - INFO - Exporting aggregated data (geohash3) for date: 20191210, hours: 0-3\n",
      "2024-07-02 22:07:40,278 - INFO - Exporting aggregated data (geohash5) for date: 20191210, hours: 0-3\n",
      "2024-07-02 22:07:40,280 - INFO - Querying data for date: 20191211, hours: 0-3\n",
      "2024-07-02 22:08:12,549 - INFO - Processing data for date: 20191211, hours: 0-3\n",
      "2024-07-02 22:08:12,560 - INFO - Exporting aggregated data (geohash3) for date: 20191211, hours: 0-3\n",
      "2024-07-02 22:08:12,576 - INFO - Exporting aggregated data (geohash5) for date: 20191211, hours: 0-3\n",
      "2024-07-02 22:08:12,578 - INFO - Querying data for date: 20191212, hours: 0-3\n",
      "2024-07-02 22:08:54,522 - INFO - Processing data for date: 20191212, hours: 0-3\n",
      "2024-07-02 22:08:54,533 - INFO - Exporting aggregated data (geohash3) for date: 20191212, hours: 0-3\n",
      "2024-07-02 22:08:54,549 - INFO - Exporting aggregated data (geohash5) for date: 20191212, hours: 0-3\n",
      "2024-07-02 22:08:54,552 - INFO - Querying data for date: 20191213, hours: 0-3\n",
      "2024-07-02 22:09:26,025 - INFO - Processing data for date: 20191213, hours: 0-3\n",
      "2024-07-02 22:09:26,038 - INFO - Exporting aggregated data (geohash3) for date: 20191213, hours: 0-3\n",
      "2024-07-02 22:09:26,054 - INFO - Exporting aggregated data (geohash5) for date: 20191213, hours: 0-3\n",
      "2024-07-02 22:09:26,056 - INFO - Querying data for date: 20191217, hours: 0-3\n",
      "2024-07-02 22:09:54,051 - INFO - Processing data for date: 20191217, hours: 0-3\n",
      "2024-07-02 22:09:54,062 - INFO - Exporting aggregated data (geohash3) for date: 20191217, hours: 0-3\n",
      "2024-07-02 22:09:54,082 - INFO - Exporting aggregated data (geohash5) for date: 20191217, hours: 0-3\n",
      "2024-07-02 22:09:54,084 - INFO - Querying data for date: 20191218, hours: 0-3\n",
      "2024-07-02 22:10:22,712 - INFO - Processing data for date: 20191218, hours: 0-3\n",
      "2024-07-02 22:10:22,724 - INFO - Exporting aggregated data (geohash3) for date: 20191218, hours: 0-3\n",
      "2024-07-02 22:10:22,740 - INFO - Exporting aggregated data (geohash5) for date: 20191218, hours: 0-3\n",
      "2024-07-02 22:10:22,742 - INFO - Querying data for date: 20191219, hours: 0-3\n",
      "2024-07-02 22:10:58,356 - INFO - Processing data for date: 20191219, hours: 0-3\n",
      "2024-07-02 22:10:58,368 - INFO - Exporting aggregated data (geohash3) for date: 20191219, hours: 0-3\n",
      "2024-07-02 22:10:58,425 - INFO - Exporting aggregated data (geohash5) for date: 20191219, hours: 0-3\n",
      "2024-07-02 22:10:58,427 - INFO - Querying data for date: 20191220, hours: 0-3\n",
      "2024-07-02 22:11:32,660 - INFO - Processing data for date: 20191220, hours: 0-3\n",
      "2024-07-02 22:11:32,675 - INFO - Exporting aggregated data (geohash3) for date: 20191220, hours: 0-3\n",
      "2024-07-02 22:11:32,695 - INFO - Exporting aggregated data (geohash5) for date: 20191220, hours: 0-3\n",
      "2024-07-02 22:11:32,698 - INFO - Querying data for date: 20191221, hours: 0-3\n",
      "2024-07-02 22:12:08,295 - INFO - Processing data for date: 20191221, hours: 0-3\n",
      "2024-07-02 22:12:08,307 - INFO - Exporting aggregated data (geohash3) for date: 20191221, hours: 0-3\n",
      "2024-07-02 22:12:08,323 - INFO - Exporting aggregated data (geohash5) for date: 20191221, hours: 0-3\n",
      "2024-07-02 22:12:08,325 - INFO - Querying data for date: 20191223, hours: 0-3\n",
      "2024-07-02 22:12:40,484 - INFO - Processing data for date: 20191223, hours: 0-3\n",
      "2024-07-02 22:12:40,495 - INFO - Exporting aggregated data (geohash3) for date: 20191223, hours: 0-3\n",
      "2024-07-02 22:12:40,511 - INFO - Exporting aggregated data (geohash5) for date: 20191223, hours: 0-3\n",
      "2024-07-02 22:12:40,513 - INFO - Querying data for date: 20191224, hours: 0-3\n",
      "2024-07-02 22:13:18,280 - INFO - Processing data for date: 20191224, hours: 0-3\n",
      "2024-07-02 22:13:18,292 - INFO - Exporting aggregated data (geohash3) for date: 20191224, hours: 0-3\n",
      "2024-07-02 22:13:18,322 - INFO - Exporting aggregated data (geohash5) for date: 20191224, hours: 0-3\n",
      "2024-07-02 22:13:18,324 - INFO - Querying data for date: 20191225, hours: 0-3\n",
      "2024-07-02 22:13:44,806 - INFO - Processing data for date: 20191225, hours: 0-3\n",
      "2024-07-02 22:13:44,817 - INFO - Exporting aggregated data (geohash3) for date: 20191225, hours: 0-3\n",
      "2024-07-02 22:13:44,839 - INFO - Exporting aggregated data (geohash5) for date: 20191225, hours: 0-3\n",
      "2024-07-02 22:13:44,841 - INFO - Querying data for date: 20191226, hours: 0-3\n",
      "2024-07-02 22:14:25,757 - INFO - Processing data for date: 20191226, hours: 0-3\n",
      "2024-07-02 22:14:25,768 - INFO - Exporting aggregated data (geohash3) for date: 20191226, hours: 0-3\n",
      "2024-07-02 22:14:25,788 - INFO - Exporting aggregated data (geohash5) for date: 20191226, hours: 0-3\n",
      "2024-07-02 22:14:25,790 - INFO - Querying data for date: 20191227, hours: 0-3\n",
      "2024-07-02 22:14:58,989 - INFO - Processing data for date: 20191227, hours: 0-3\n",
      "2024-07-02 22:14:59,001 - INFO - Exporting aggregated data (geohash3) for date: 20191227, hours: 0-3\n",
      "2024-07-02 22:14:59,025 - INFO - Exporting aggregated data (geohash5) for date: 20191227, hours: 0-3\n",
      "2024-07-02 22:14:59,027 - INFO - Querying data for date: 20191228, hours: 0-3\n",
      "2024-07-02 22:15:32,923 - INFO - Processing data for date: 20191228, hours: 0-3\n",
      "2024-07-02 22:15:32,935 - INFO - Exporting aggregated data (geohash3) for date: 20191228, hours: 0-3\n",
      "2024-07-02 22:15:32,954 - INFO - Exporting aggregated data (geohash5) for date: 20191228, hours: 0-3\n",
      "2024-07-02 22:15:32,956 - INFO - Querying data for date: 20191229, hours: 0-3\n",
      "2024-07-02 22:15:57,552 - INFO - Processing data for date: 20191229, hours: 0-3\n",
      "2024-07-02 22:15:57,564 - INFO - Exporting aggregated data (geohash3) for date: 20191229, hours: 0-3\n",
      "2024-07-02 22:15:57,585 - INFO - Exporting aggregated data (geohash5) for date: 20191229, hours: 0-3\n",
      "2024-07-02 22:15:57,587 - INFO - Querying data for date: 20191230, hours: 0-3\n",
      "2024-07-02 22:16:36,674 - INFO - Processing data for date: 20191230, hours: 0-3\n",
      "2024-07-02 22:16:36,686 - INFO - Exporting aggregated data (geohash3) for date: 20191230, hours: 0-3\n",
      "2024-07-02 22:16:36,706 - INFO - Exporting aggregated data (geohash5) for date: 20191230, hours: 0-3\n"
     ]
    }
   ],
   "source": [
    "# For all 0s:\n",
    "\n",
    "# event_dates = [\"20190103\", \"20190107\", \"20190109\", \"20190110\", \"20190111\", \"20190114\"]\n",
    "event_dates = [\n",
    "     # \"20190115\", \"20190116\", \"20190118\", \"20190121\", \n",
    "    \"20190122\", \n",
    "     \"20190125\", \"20190128\", \"20190129\", \"20190202\", \"20190404\", \n",
    "     \"20190429\", \"20191126\", \"20191127\", \"20191128\", \"20191202\", \n",
    "     \"20191203\", \"20191205\", \"20191206\", \"20191207\", \"20191208\", \n",
    "     \"20191209\", \"20191210\", \"20191211\", \"20191212\", \"20191213\", \n",
    "     \"20191217\", \"20191218\", \"20191219\", \"20191220\", \"20191221\", \n",
    "     \"20191223\", \"20191224\", \"20191225\", \"20191226\", \"20191227\", \n",
    "     \"20191228\", \"20191229\", \"20191230\"]\n",
    "\n",
    "start_hour = 0\n",
    "end_hour = 3 \n",
    "country_code = 'ID'\n",
    "export_path = '/home/jovyan/Data/TJ/3h/'\n",
    "\n",
    "# Process data for the specified dates and time intervals\n",
    "process_data_for_date(event_dates, country_code, export_path, start_hour, end_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
