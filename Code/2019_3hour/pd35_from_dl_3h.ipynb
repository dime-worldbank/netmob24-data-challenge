{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from geohash2) (0.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c200a594-7174-439d-9374-317cc03ed652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a044108c-c3e8-4325-8f00-6fcbe62ea69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277f112-2f83-42fc-9b8c-601e7b6f5536",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export to jupyter notebook\n",
    "should be working for CO, ID, IN, MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a883a0ca-4e38-476c-843f-8d41e0b28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-01-02'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'MX'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b381a613-35a2-4ea3-8ab0-3215806ca7d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 21:17:14,876 - INFO - Executing SQL query for date 20190101 and interval 0 to 3\n",
      "2024-06-28 21:26:34,620 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-06-28 21:26:35,335 - INFO - Exporting data for interval 0 to 3\n",
      "2024-06-28 21:26:35,359 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-06-28 21:26:35,967 - INFO - Exporting data for interval 0 to 3\n",
      "2024-06-28 21:26:35,973 - INFO - Appended data for date 20190101 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-06-28 21:26:35,974 - INFO - Executing SQL query for date 20190101 and interval 3 to 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 47\u001b[0m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    cuebiq_id, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124m    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%Y-%m-%dT%H:%i:%s\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))) BETWEEN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_hour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_hour\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     46\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting SQL query for date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_current_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and interval \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_hour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_hour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m pe_dl_table_gen \u001b[38;5;241m=\u001b[39m \u001b[43msql_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Convert event_datetime_local to datetime once\u001b[39;00m\n\u001b[1;32m     50\u001b[0m pe_dl_table_gen[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_datetime_local\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(pe_dl_table_gen[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_datetime_local\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn [5], line 23\u001b[0m, in \u001b[0;36mTrinoEngine.read_sql\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_sql\u001b[39m(\u001b[38;5;28mself\u001b[39m, query:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame: \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Select and insert into operations.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/sql.py:590\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    582\u001b[0m         sql,\n\u001b[1;32m    583\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/util/deprecations.py:402\u001b[0m, in \u001b[0;36m_decorate_with_warning.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_warning:\n\u001b[1;32m    401\u001b[0m     _warn_with_version(message, version, wtype, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3257\u001b[0m, in \u001b[0;36mEngine.execute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;124;03m\"\"\"Executes the given construct and returns a\u001b[39;00m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;124;03m:class:`_engine.CursorResult`.\u001b[39;00m\n\u001b[1;32m   3241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \n\u001b[1;32m   3255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect(close_with_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 3257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1365\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(statement, util\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[1;32m   1357\u001b[0m     util\u001b[38;5;241m.\u001b[39mwarn_deprecated_20(\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a string to Connection.execute() is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in version 2.0.  Use the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver-level SQL string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_EMPTY_EXECUTION_OPTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1669\u001b[0m, in \u001b[0;36mConnection._exec_driver_sql\u001b[0;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         (\n\u001b[1;32m   1660\u001b[0m             statement,\n\u001b[1;32m   1661\u001b[0m             distilled_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m             statement, distilled_parameters, execution_options\n\u001b[1;32m   1666\u001b[0m         )\n\u001b[1;32m   1668\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[0;32m-> 1669\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future:\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1943\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1940\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1943\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2128\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   2125\u001b[0m             sqlalchemy_exception, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2128\u001b[0m         \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/util/compat.py:211\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    208\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1900\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1900\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1907\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1912\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py:365\u001b[0m, in \u001b[0;36mTrinoDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m, cursor: Cursor, statement: \u001b[38;5;28mstr\u001b[39m, parameters: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], context: DefaultExecutionContext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    364\u001b[0m ):\n\u001b[0;32m--> 365\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py:501\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, params)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query \u001b[38;5;241m=\u001b[39m trino\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mTrinoQuery(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, query\u001b[38;5;241m=\u001b[39moperation,\n\u001b[1;32m    500\u001b[0m                                           legacy_primitive_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_primitive_types)\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:821\u001b[0m, in \u001b[0;36mTrinoQuery.execute\u001b[0;34m(self, additional_http_headers)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Execute should block until at least one row is received or query is finished or cancelled\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancelled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mrows) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mrows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:837\u001b[0m, in \u001b[0;36mTrinoQuery.fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[Any]]:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;124;03m\"\"\"Continue fetching data for the current query_id\"\"\"\u001b[39;00m\n\u001b[0;32m--> 837\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request\u001b[38;5;241m.\u001b[39mprocess(response)\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state(status)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:579\u001b[0m, in \u001b[0;36mTrinoRequest.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROXIES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:886\u001b[0m, in \u001b[0;36m_retry_with.<locals>.wrapper.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_attempts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 886\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(guard(result) \u001b[38;5;28;01mfor\u001b[39;00m guard \u001b[38;5;129;01min\u001b[39;00m conditions):\n\u001b[1;32m    888\u001b[0m             handle_retry\u001b[38;5;241m.\u001b[39mretry(func, args, kwargs, \u001b[38;5;28;01mNone\u001b[39;00m, attempt)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00\"\n",
    "\n",
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            for geohash_col, export_file_path, write_header in [\n",
    "                ('geohash5', export_file_path_5, write_header_5),\n",
    "                ('geohash3', export_file_path_3, write_header_3)\n",
    "            ]:\n",
    "                # Aggregate data for geohash\n",
    "                logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "                aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                    no_of_points=(geohash_col, 'size'),\n",
    "                    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                    local_time=('3_hour_interval', 'first'),\n",
    "                    local_date=('local_date', 'first')\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Filter rows with no_of_unique_users > 10\n",
    "                filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "                \n",
    "                # Append the DataFrame to the CSV file\n",
    "                logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "                filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "                \n",
    "                # After the first write, set the header flag to False\n",
    "                if geohash_col == 'geohash5':\n",
    "                    write_header_5 = False\n",
    "                else:\n",
    "                    write_header_3 = False\n",
    "                \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "# test query by 3 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0a4d0-8ea5-495a-881b-7633a3c85d9b",
   "metadata": {},
   "source": [
    "# Export to schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e34172-ee99-417e-b800-e6a93bc1f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 23:06:54,052 - INFO - Creating master table: test_pd_co_2019_3h_agg3\n",
      "2024-06-28 23:06:54,873 - INFO - Creating master table: test_pd_co_2019_3h_agg5\n"
     ]
    }
   ],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "# output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_date = '2019-11-01'\n",
    "end_date = '2019-11-02'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "country_abbre = country_code.lower()  \n",
    "master_table_3 = f\"test_pd_{country_abbre}_2019_3h_agg3\"\n",
    "master_table_5 = f\"test_pd_{country_abbre}_2019_3h_agg5\"\n",
    "\n",
    "# Create the master tables if they do not exist\n",
    "create_table_query_3 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_3}(\n",
    "    geohash_3 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "create_table_query_5 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_5}(\n",
    "    geohash_5 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "# with engine.connect() as connection:\n",
    "#     connection.execute(create_table_query_3)\n",
    "#     connection.execute(create_table_query_5)\n",
    "    \n",
    "with con.connect() as connection:\n",
    "    logging.info(f\"Creating master table: {master_table_3}\")\n",
    "    connection.execute(create_table_query_3)\n",
    "    logging.info(f\"Creating master table: {master_table_5}\")\n",
    "    connection.execute(create_table_query_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536c09d-ea52-406b-9c88-ee97c376a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 23:06:55,862 - INFO - Executing SQL query for date 20191101 and interval 0 to 3\n",
      "2024-06-28 23:08:02,072 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-06-28 23:12:15,179 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-06-28 23:12:16,368 - INFO - Inserted aggregated data for date 20191101 interval 0 to 3 into the master tables\n",
      "2024-06-28 23:12:16,369 - INFO - Executing SQL query for date 20191101 and interval 3 to 6\n",
      "2024-06-28 23:12:38,950 - INFO - Aggregating data for geohash_5 for interval 3 to 6\n"
     ]
    }
   ],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Process for geohash_5\n",
    "            logging.info(f\"Aggregating data for geohash_5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = pe_dl_table_gen.groupby('geohash_5').agg(\n",
    "                no_of_points=('geohash_5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_5.to_sql(master_table_5, con, if_exists='append', index=False)\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into {master_table_3}\")\n",
    "\n",
    "            # Process for geohash_3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = pe_dl_table_gen.groupby('geohash_3').agg(\n",
    "                no_of_points=('geohash_3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_3.to_sql(master_table_3, con, if_exists='append', index=False, method='multi')\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into the {master tables}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee53328-3554-4860-8729-bc48a13a89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288c67d-370a-496f-911b-e743e9642d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3ee3a-65b4-402d-9523-c66a0b60bd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45622ce-9901-45b1-9805-0252db6c76ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 18:42:23,805 - INFO - Executing SQL query for date 20190101\n"
     ]
    }
   ],
   "source": [
    "# Worked query per day. - not working for MX\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fe83-0e29-441c-ae94-3913a9ea56ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055991e-c5d8-44c1-b790-961344e56c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa0f7-2b33-4d15-8970-63048d233858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1175b728-44a5-419f-9d80-2b87bed95a4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# !!!!!!!!!!!!!!! 这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09410c-5fd9-43e4-9e2c-96af3d2afb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "# output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'CO'\n",
    "start_date = 20191101\n",
    "end_date = 20191102\n",
    "# longitude_ranges = [(-82, -74.53125), (-74.53125, -65)]  # CO specific longitude ranges\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "failed_inserts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02b9d47-428f-427a-8db5-40e7ef9dbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2019-11-12'\n",
    "end_date = '2019-11-11'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10020a18-0fb9-4bee-b368-37b9309b61ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Loop through each day in the date range\n",
    "# for current_date in date_range:\n",
    "#     try:\n",
    "#         formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "#         # Calculate the lookback and lookahead dates\n",
    "#         lookback_date = current_date - timedelta(days=1)\n",
    "#         lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "#         # Format dates for the SQL query in 'yyyymmdd' format\n",
    "#         formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "#         formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "#         formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "#         # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "#         query = f\"\"\"\n",
    "#         SELECT \n",
    "#             cuebiq_id, \n",
    "#             event_zoned_datetime, \n",
    "#             processing_date,\n",
    "#             lat,\n",
    "#             lng,\n",
    "#             TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "#             EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "#             geohash_encode(lat, lng, 5) AS geohash5,\n",
    "#             geohash_encode(lat, lng, 3) AS geohash3\n",
    "#         FROM {pe_dl_table}\n",
    "#         WHERE \n",
    "#             processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "#             AND country_code = '{country_code}' \n",
    "#             AND event_zoned_datetime IS NOT NULL\n",
    "#             AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#             AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "#         \"\"\"\n",
    "        \n",
    "#         logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "#         pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "#         # Convert event_datetime_local to datetime\n",
    "#         pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "#         # Loop through each 3-hour interval\n",
    "#         for start_hour in range(0, 24, 3):\n",
    "#             end_hour = start_hour + 3\n",
    "            \n",
    "#             # Filter data for the current 3-hour interval\n",
    "#             interval_data = pe_dl_table_gen[\n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "#             ].copy()\n",
    "            \n",
    "#             # Create 3-hour interval column\n",
    "#             interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "#             interval_data['3_hour_interval'] = interval\n",
    "            \n",
    "#             # Aggregate data for geohash5\n",
    "#             logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "#                 no_of_points=('geohash5', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash5\n",
    "#             logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash5\n",
    "#             write_header_5 = False\n",
    "            \n",
    "#             # Aggregate data for geohash3\n",
    "#             logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "#                 no_of_points=('geohash3', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash3\n",
    "#             logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash3\n",
    "#             write_header_3 = False\n",
    "            \n",
    "#             logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "#     # Move to the next day\n",
    "#     current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f97b3a-022f-4b9b-9983-213d2dff3fe1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adc895-13b8-4190-8651-894e4bad5697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55fb058-0ded-431a-85aa-a3bfd92c51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 17:15:28,436 - INFO - Executing SQL query for date 20191110\n",
      "2024-06-28 17:16:27,608 - INFO - Executing SQL query for date 20191110\n",
      "2024-06-28 17:17:30,670 - INFO - Exporting data to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv for date 20191110\n",
      "2024-06-28 17:17:30,715 - INFO - Exporting data to /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv for date 20191110\n",
      "2024-06-28 17:17:30,725 - INFO - Appended data for date 20191110 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 17:17:30,726 - INFO - Executing SQL query for date 20191111\n",
      "2024-06-28 17:18:26,612 - INFO - Executing SQL query for date 20191111\n",
      "2024-06-28 17:19:16,483 - INFO - Exporting data to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv for date 20191111\n",
      "2024-06-28 17:19:16,531 - INFO - Exporting data to /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv for date 20191111\n",
      "2024-06-28 17:19:16,549 - INFO - Appended data for date 20191111 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash5 and geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash5,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 5) AS geohash5,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash5\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL5 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5 = sql_engine.read_sql(query)\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash3,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 3) AS geohash3,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash3\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL3 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3 = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash5\n",
    "        logging.info(f\"Exporting data to {export_file_path_5} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash5\n",
    "        write_header_5 = False\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash3\n",
    "        logging.info(f\"Exporting data to {export_file_path_3} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash3\n",
    "        write_header_3 = False\n",
    "\n",
    "        logging.info(f\"Appended data for date {formatted_current_date} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c587e04-766d-4c83-9234-f6bd93e6fff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5c1fb-bce9-4098-bef6-1b2dc952b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a1127-0a10-4c27-a131-467026fbae94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c274-2c3d-4198-8d89-2c127eb4e280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c91b-b3c2-4e93-9b79-6411d7b47f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8948ce-1ebc-42c4-af54-844999063440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116555cb-e34a-4303-b77d-9a91e2dad215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bcea9-c6aa-46f1-a2c5-2d246d6716dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faaf4250-178e-474b-8b33-efb5705d612a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 16:37:15,928 - INFO - Executing SQL query for date 20191106\n",
      "2024-06-28 16:38:45,487 - INFO - Processing geohashes for date 20191106\n",
      "2024-06-28 16:40:17,660 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-06-28 16:40:17,804 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 0 to 3\n",
      "2024-06-28 16:40:17,817 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-06-28 16:40:17,855 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 0 to 3\n",
      "2024-06-28 16:40:17,862 - INFO - Appended data for date 20191106 interval 0 to 3 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:18,370 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-06-28 16:40:18,546 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 3 to 6\n",
      "2024-06-28 16:40:18,565 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-06-28 16:40:18,608 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 3 to 6\n",
      "2024-06-28 16:40:18,614 - INFO - Appended data for date 20191106 interval 3 to 6 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:19,170 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-06-28 16:40:19,496 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 6 to 9\n",
      "2024-06-28 16:40:19,507 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-06-28 16:40:19,621 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 6 to 9\n",
      "2024-06-28 16:40:19,627 - INFO - Appended data for date 20191106 interval 6 to 9 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:20,203 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-06-28 16:40:20,549 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 9 to 12\n",
      "2024-06-28 16:40:20,560 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-06-28 16:40:20,687 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 9 to 12\n",
      "2024-06-28 16:40:20,693 - INFO - Appended data for date 20191106 interval 9 to 12 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:21,276 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-06-28 16:40:21,625 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 12 to 15\n",
      "2024-06-28 16:40:21,637 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-06-28 16:40:21,781 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 12 to 15\n",
      "2024-06-28 16:40:21,787 - INFO - Appended data for date 20191106 interval 12 to 15 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:22,377 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-06-28 16:40:22,748 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 15 to 18\n",
      "2024-06-28 16:40:22,760 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-06-28 16:40:22,906 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 15 to 18\n",
      "2024-06-28 16:40:22,911 - INFO - Appended data for date 20191106 interval 15 to 18 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:23,508 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-06-28 16:40:23,841 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 18 to 21\n",
      "2024-06-28 16:40:23,855 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-06-28 16:40:23,989 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 18 to 21\n",
      "2024-06-28 16:40:23,996 - INFO - Appended data for date 20191106 interval 18 to 21 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:24,558 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-06-28 16:40:24,788 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 21 to 24\n",
      "2024-06-28 16:40:24,799 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-06-28 16:40:24,877 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 21 to 24\n",
      "2024-06-28 16:40:24,887 - INFO - Appended data for date 20191106 interval 21 to 24 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:40:24,888 - INFO - Executing SQL query for date 20191107\n",
      "2024-06-28 16:41:58,352 - INFO - Processing geohashes for date 20191107\n",
      "2024-06-28 16:43:31,332 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-06-28 16:43:31,488 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 0 to 3\n",
      "2024-06-28 16:43:31,500 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-06-28 16:43:31,540 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 0 to 3\n",
      "2024-06-28 16:43:31,545 - INFO - Appended data for date 20191107 interval 0 to 3 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:32,068 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-06-28 16:43:32,246 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 3 to 6\n",
      "2024-06-28 16:43:32,258 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-06-28 16:43:32,301 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 3 to 6\n",
      "2024-06-28 16:43:32,307 - INFO - Appended data for date 20191107 interval 3 to 6 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:32,888 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-06-28 16:43:33,234 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 6 to 9\n",
      "2024-06-28 16:43:33,253 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-06-28 16:43:33,369 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 6 to 9\n",
      "2024-06-28 16:43:33,375 - INFO - Appended data for date 20191107 interval 6 to 9 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:33,984 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-06-28 16:43:34,349 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 9 to 12\n",
      "2024-06-28 16:43:34,361 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-06-28 16:43:34,491 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 9 to 12\n",
      "2024-06-28 16:43:34,497 - INFO - Appended data for date 20191107 interval 9 to 12 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:35,119 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-06-28 16:43:35,497 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 12 to 15\n",
      "2024-06-28 16:43:35,510 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-06-28 16:43:35,651 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 12 to 15\n",
      "2024-06-28 16:43:35,657 - INFO - Appended data for date 20191107 interval 12 to 15 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:36,284 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-06-28 16:43:36,663 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 15 to 18\n",
      "2024-06-28 16:43:36,678 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-06-28 16:43:36,824 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 15 to 18\n",
      "2024-06-28 16:43:36,843 - INFO - Appended data for date 20191107 interval 15 to 18 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:37,444 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-06-28 16:43:37,817 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 18 to 21\n",
      "2024-06-28 16:43:37,830 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-06-28 16:43:37,970 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 18 to 21\n",
      "2024-06-28 16:43:37,980 - INFO - Appended data for date 20191107 interval 18 to 21 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n",
      "2024-06-28 16:43:38,550 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-06-28 16:43:38,788 - INFO - Exporting data to pd_co_2019_agg5_3h.csv for interval 21 to 24\n",
      "2024-06-28 16:43:38,800 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-06-28 16:43:38,880 - INFO - Exporting data to pd_co_2019_agg3_3h.csv for interval 21 to 24\n",
      "2024-06-28 16:43:38,891 - INFO - Appended data for date 20191107 interval 21 to 24 to /home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv and /home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "\n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "        # Calculate geohashes\n",
    "        logging.info(f\"Processing geohashes for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "\n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "\n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "\n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "\n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "\n",
    "            # logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e18cd-7712-4ce5-8af2-17132ddfb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c1a10-ca49-4978-80af-bf1b1b436979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f70d6c-14ee-4bb3-a144-8f9d22756993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849c172-25b4-4015-9c3f-4c54c6f9b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f35f29-e38b-4be2-bbc4-b5603656734a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a575-0da2-445d-a070-9d5e3a37309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f2b3f-068f-441d-b29b-785322d09091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101c3db-396b-4176-b0da-f46abb48a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721c5a7-4c76-4278-a9b9-6bc0ab54028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75951-dd0a-4bc0-80ef-2e3cb2a2f8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d343d50-9c6c-4c8b-a774-2cec03b0b627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283036a-b031-4b32-8ae0-3cc1cdad8652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34e23-66c7-485c-807c-3476a4344dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abfca8-eb70-4e28-8fb1-0665f28903e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16774b-eb43-4b9b-b101-955588f36125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8862327-eaba-46a0-8ecb-5e142581315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965534c4-906e-4714-8dae-a12a9a7d83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd686a0-4c83-4154-a4f7-37acd1960ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64342f59-eaf3-41d2-aabd-932a4994830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e038-fb28-4124-a130-d6d2b36350f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54a6f6-e75b-4644-801c-dd37105ee619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b67b-9e18-4c61-8756-3a8aaae6384c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee41c-424a-40dd-8ed8-62a8764d629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed2e44-ad40-4863-9a63-637f70c3775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5481759-bef6-47f8-84fd-bd7bd04d43dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8a3f-9bbe-4abe-9fe0-236738e72528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ef14e-1604-4c31-b723-cddf9aab9947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfdaea-78a1-4a00-9dc3-6fa793f6fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one seemms to be working \n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    try:\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Construct the SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        \n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e495e-3f2b-4872-b595-61c1f8242278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926ef6e-772d-442f-9416-55c3c6c5c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d2b9d-7e31-4ad1-824b-5cd8a23c95e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f16fd-3cfc-4ea4-87ee-baca476c647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0f9e2-49ff-452e-8f32-d84cb913dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51349b29-b674-4f30-83bd-9f6386e5655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd167fd2-7c44-4a94-9b9c-b9b5dca82d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check by single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491b7184-2831-4507-8682-3af4bdd08f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>event_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1557947231</td>\n",
       "      <td>2019-03-04T21:50:50+07:00</td>\n",
       "      <td>20190304</td>\n",
       "      <td>-7.314244</td>\n",
       "      <td>112.718623</td>\n",
       "      <td>2019-03-04 21:50:50</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1557947231</td>\n",
       "      <td>2019-03-04T22:08:02+07:00</td>\n",
       "      <td>20190304</td>\n",
       "      <td>-7.314244</td>\n",
       "      <td>112.718623</td>\n",
       "      <td>2019-03-04 22:08:02</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1557947231</td>\n",
       "      <td>2019-03-04T22:08:03+07:00</td>\n",
       "      <td>20190304</td>\n",
       "      <td>-7.314244</td>\n",
       "      <td>112.718623</td>\n",
       "      <td>2019-03-04 22:08:03</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1557947231</td>\n",
       "      <td>2019-03-04T22:16:41+07:00</td>\n",
       "      <td>20190304</td>\n",
       "      <td>-7.314244</td>\n",
       "      <td>112.718623</td>\n",
       "      <td>2019-03-04 22:16:41</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1557947231</td>\n",
       "      <td>2019-03-04T22:32:34+07:00</td>\n",
       "      <td>20190304</td>\n",
       "      <td>-7.314244</td>\n",
       "      <td>112.718623</td>\n",
       "      <td>2019-03-04 22:32:34</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610153</th>\n",
       "      <td>1860654372</td>\n",
       "      <td>2019-03-04T18:16:46+08:00</td>\n",
       "      <td>20190305</td>\n",
       "      <td>-8.655824</td>\n",
       "      <td>115.126922</td>\n",
       "      <td>2019-03-04 18:16:46</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610154</th>\n",
       "      <td>1860654372</td>\n",
       "      <td>2019-03-04T19:53:50+08:00</td>\n",
       "      <td>20190305</td>\n",
       "      <td>-8.655824</td>\n",
       "      <td>115.126922</td>\n",
       "      <td>2019-03-04 19:53:50</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610155</th>\n",
       "      <td>1860654372</td>\n",
       "      <td>2019-03-04T15:56:32+08:00</td>\n",
       "      <td>20190305</td>\n",
       "      <td>-8.655472</td>\n",
       "      <td>115.126692</td>\n",
       "      <td>2019-03-04 15:56:32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610156</th>\n",
       "      <td>1860654372</td>\n",
       "      <td>2019-03-04T17:18:17+08:00</td>\n",
       "      <td>20190305</td>\n",
       "      <td>-8.655308</td>\n",
       "      <td>115.126956</td>\n",
       "      <td>2019-03-04 17:18:17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610157</th>\n",
       "      <td>1860654372</td>\n",
       "      <td>2019-03-04T18:02:59+08:00</td>\n",
       "      <td>20190305</td>\n",
       "      <td>-8.655199</td>\n",
       "      <td>115.127370</td>\n",
       "      <td>2019-03-04 18:02:59</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9610158 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cuebiq_id       event_zoned_datetime  processing_date       lat  \\\n",
       "0        1557947231  2019-03-04T21:50:50+07:00         20190304 -7.314244   \n",
       "1        1557947231  2019-03-04T22:08:02+07:00         20190304 -7.314244   \n",
       "2        1557947231  2019-03-04T22:08:03+07:00         20190304 -7.314244   \n",
       "3        1557947231  2019-03-04T22:16:41+07:00         20190304 -7.314244   \n",
       "4        1557947231  2019-03-04T22:32:34+07:00         20190304 -7.314244   \n",
       "...             ...                        ...              ...       ...   \n",
       "9610153  1860654372  2019-03-04T18:16:46+08:00         20190305 -8.655824   \n",
       "9610154  1860654372  2019-03-04T19:53:50+08:00         20190305 -8.655824   \n",
       "9610155  1860654372  2019-03-04T15:56:32+08:00         20190305 -8.655472   \n",
       "9610156  1860654372  2019-03-04T17:18:17+08:00         20190305 -8.655308   \n",
       "9610157  1860654372  2019-03-04T18:02:59+08:00         20190305 -8.655199   \n",
       "\n",
       "                lng event_datetime_local  event_hour  \n",
       "0        112.718623  2019-03-04 21:50:50          21  \n",
       "1        112.718623  2019-03-04 22:08:02          22  \n",
       "2        112.718623  2019-03-04 22:08:03          22  \n",
       "3        112.718623  2019-03-04 22:16:41          22  \n",
       "4        112.718623  2019-03-04 22:32:34          22  \n",
       "...             ...                  ...         ...  \n",
       "9610153  115.126922  2019-03-04 18:16:46          18  \n",
       "9610154  115.126922  2019-03-04 19:53:50          19  \n",
       "9610155  115.126692  2019-03-04 15:56:32          15  \n",
       "9610156  115.126956  2019-03-04 17:18:17          17  \n",
       "9610157  115.127370  2019-03-04 18:02:59          18  \n",
       "\n",
       "[9610158 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46018200-ab33-4c97-bf10-53fbca072c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "import os\n",
    "\n",
    "# Define the export file path\n",
    "export_file_path_3 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv'\n",
    "export_file_path_5 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv'\n",
    "\n",
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Initialize flags to write the headers only once\n",
    "write_header_5 = True\n",
    "write_header_3 = True\n",
    "\n",
    "# Loop through each 3-hour interval\n",
    "formatted_current_date = pd.to_datetime(formatted_current_date)  # Ensure it's a datetime object\n",
    "for start_hour in range(0, 24, 3):\n",
    "    end_hour = start_hour + 3\n",
    "    \n",
    "    # Filter data for the current 3-hour interval\n",
    "    interval_data = pe_dl_table_gen[\n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "    ].copy()\n",
    "    \n",
    "    # Create 3-hour interval column\n",
    "    interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "    interval_data['3_hour_interval'] = interval\n",
    "    \n",
    "    # Aggregate data for geohash5\n",
    "    aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "        no_of_points=('geohash5', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash5\n",
    "    filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash5\n",
    "    write_header_5 = False\n",
    "    \n",
    "    # Aggregate data for geohash3\n",
    "    aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "        no_of_points=('geohash3', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash3\n",
    "    filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash3\n",
    "    write_header_3 = False\n",
    "    \n",
    "    print(f\"Appended data for interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af6712-172d-42e8-92ef-e193c08202ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a73f2-0941-4640-b810-139180fc4924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369faf8d-fc67-4b50-99e2-698af62fb47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696fe4f-4b3c-456f-9862-69ae758fc1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915eb14-07e3-496b-b8b2-733d01d80cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bb1c2a-a79d-4575-823d-d60ae8d68441",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abc27da9-ce7c-4c81-9baa-9948cc38d47e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 00:36:53,634 - INFO - Executing SQL query for date 20191101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>event_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2488437649</td>\n",
       "      <td>2019-11-01T00:55:26-05:00</td>\n",
       "      <td>20191103</td>\n",
       "      <td>3.449487</td>\n",
       "      <td>-76.541778</td>\n",
       "      <td>2019-11-01 00:55:26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2488437649</td>\n",
       "      <td>2019-11-01T00:57:29-05:00</td>\n",
       "      <td>20191103</td>\n",
       "      <td>3.450623</td>\n",
       "      <td>-76.540825</td>\n",
       "      <td>2019-11-01 00:57:29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2488437649</td>\n",
       "      <td>2019-11-01T00:57:29-05:00</td>\n",
       "      <td>20191103</td>\n",
       "      <td>3.450623</td>\n",
       "      <td>-76.540825</td>\n",
       "      <td>2019-11-01 00:57:29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2488423975</td>\n",
       "      <td>2019-11-01T00:26:27-05:00</td>\n",
       "      <td>20191103</td>\n",
       "      <td>3.018457</td>\n",
       "      <td>-76.478375</td>\n",
       "      <td>2019-11-01 00:26:27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2488423975</td>\n",
       "      <td>2019-11-01T00:01:01-05:00</td>\n",
       "      <td>20191103</td>\n",
       "      <td>3.018794</td>\n",
       "      <td>-76.478607</td>\n",
       "      <td>2019-11-01 00:01:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125556</th>\n",
       "      <td>2492387999</td>\n",
       "      <td>2019-11-01T00:25:02-05:00</td>\n",
       "      <td>20191120</td>\n",
       "      <td>3.423894</td>\n",
       "      <td>-76.528569</td>\n",
       "      <td>2019-11-01 00:25:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125557</th>\n",
       "      <td>2492387999</td>\n",
       "      <td>2019-11-01T00:25:02-05:00</td>\n",
       "      <td>20191120</td>\n",
       "      <td>3.423894</td>\n",
       "      <td>-76.528569</td>\n",
       "      <td>2019-11-01 00:25:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125558</th>\n",
       "      <td>2492387999</td>\n",
       "      <td>2019-11-01T00:20:54-05:00</td>\n",
       "      <td>20191120</td>\n",
       "      <td>3.424044</td>\n",
       "      <td>-76.528218</td>\n",
       "      <td>2019-11-01 00:20:54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125559</th>\n",
       "      <td>2489786886</td>\n",
       "      <td>2019-11-01T01:59:57-05:00</td>\n",
       "      <td>20191120</td>\n",
       "      <td>4.757085</td>\n",
       "      <td>-74.035650</td>\n",
       "      <td>2019-11-01 01:59:57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125560</th>\n",
       "      <td>2489786886</td>\n",
       "      <td>2019-11-01T01:59:57-05:00</td>\n",
       "      <td>20191120</td>\n",
       "      <td>4.757085</td>\n",
       "      <td>-74.035650</td>\n",
       "      <td>2019-11-01 01:59:57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125561 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cuebiq_id       event_zoned_datetime  processing_date       lat  \\\n",
       "0       2488437649  2019-11-01T00:55:26-05:00         20191103  3.449487   \n",
       "1       2488437649  2019-11-01T00:57:29-05:00         20191103  3.450623   \n",
       "2       2488437649  2019-11-01T00:57:29-05:00         20191103  3.450623   \n",
       "3       2488423975  2019-11-01T00:26:27-05:00         20191103  3.018457   \n",
       "4       2488423975  2019-11-01T00:01:01-05:00         20191103  3.018794   \n",
       "...            ...                        ...              ...       ...   \n",
       "125556  2492387999  2019-11-01T00:25:02-05:00         20191120  3.423894   \n",
       "125557  2492387999  2019-11-01T00:25:02-05:00         20191120  3.423894   \n",
       "125558  2492387999  2019-11-01T00:20:54-05:00         20191120  3.424044   \n",
       "125559  2489786886  2019-11-01T01:59:57-05:00         20191120  4.757085   \n",
       "125560  2489786886  2019-11-01T01:59:57-05:00         20191120  4.757085   \n",
       "\n",
       "              lng event_datetime_local  event_hour  \n",
       "0      -76.541778  2019-11-01 00:55:26           0  \n",
       "1      -76.540825  2019-11-01 00:57:29           0  \n",
       "2      -76.540825  2019-11-01 00:57:29           0  \n",
       "3      -76.478375  2019-11-01 00:26:27           0  \n",
       "4      -76.478607  2019-11-01 00:01:01           0  \n",
       "...           ...                  ...         ...  \n",
       "125556 -76.528569  2019-11-01 00:25:02           0  \n",
       "125557 -76.528569  2019-11-01 00:25:02           0  \n",
       "125558 -76.528218  2019-11-01 00:20:54           0  \n",
       "125559 -74.035650  2019-11-01 01:59:57           1  \n",
       "125560 -74.035650  2019-11-01 01:59:57           1  \n",
       "\n",
       "[125561 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "537a8dd2-4a7e-4ab0-8b90-ad328598d301",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>event_hour</th>\n",
       "      <th>geohash5</th>\n",
       "      <th>geohash3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517207294</td>\n",
       "      <td>2019-03-04T00:52:01+08:00</td>\n",
       "      <td>-8.528349</td>\n",
       "      <td>115.243144</td>\n",
       "      <td>2019-03-04 00:52:01</td>\n",
       "      <td>0</td>\n",
       "      <td>qw3zq</td>\n",
       "      <td>qw3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517207294</td>\n",
       "      <td>2019-03-04T00:45:32+08:00</td>\n",
       "      <td>-8.223645</td>\n",
       "      <td>115.396520</td>\n",
       "      <td>2019-03-04 00:45:32</td>\n",
       "      <td>0</td>\n",
       "      <td>qwd11</td>\n",
       "      <td>qwd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1522013587</td>\n",
       "      <td>2019-03-04T01:51:05+07:00</td>\n",
       "      <td>-7.669646</td>\n",
       "      <td>111.107331</td>\n",
       "      <td>2019-03-04 01:51:05</td>\n",
       "      <td>1</td>\n",
       "      <td>qqxh2</td>\n",
       "      <td>qqx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1522013587</td>\n",
       "      <td>2019-03-04T01:59:12+07:00</td>\n",
       "      <td>-7.669308</td>\n",
       "      <td>111.107703</td>\n",
       "      <td>2019-03-04 01:59:12</td>\n",
       "      <td>1</td>\n",
       "      <td>qqxh2</td>\n",
       "      <td>qqx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1818627892</td>\n",
       "      <td>2019-03-04T01:07:10+07:00</td>\n",
       "      <td>-6.189541</td>\n",
       "      <td>106.813124</td>\n",
       "      <td>2019-03-04 01:07:10</td>\n",
       "      <td>1</td>\n",
       "      <td>qqguy</td>\n",
       "      <td>qqg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599273</th>\n",
       "      <td>1750154524</td>\n",
       "      <td>2019-03-04T02:25:59+07:00</td>\n",
       "      <td>-6.151897</td>\n",
       "      <td>106.895090</td>\n",
       "      <td>2019-03-04 02:25:59</td>\n",
       "      <td>2</td>\n",
       "      <td>qquj0</td>\n",
       "      <td>qqu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599274</th>\n",
       "      <td>1750154524</td>\n",
       "      <td>2019-03-04T02:59:32+07:00</td>\n",
       "      <td>-6.151897</td>\n",
       "      <td>106.895090</td>\n",
       "      <td>2019-03-04 02:59:32</td>\n",
       "      <td>2</td>\n",
       "      <td>qquj0</td>\n",
       "      <td>qqu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599275</th>\n",
       "      <td>1752656232</td>\n",
       "      <td>2019-03-04T00:16:04+07:00</td>\n",
       "      <td>-0.057598</td>\n",
       "      <td>109.356632</td>\n",
       "      <td>2019-03-04 00:16:04</td>\n",
       "      <td>0</td>\n",
       "      <td>qrvz8</td>\n",
       "      <td>qrv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599276</th>\n",
       "      <td>1583178821</td>\n",
       "      <td>2019-03-04T02:07:45+08:00</td>\n",
       "      <td>-8.211994</td>\n",
       "      <td>124.528087</td>\n",
       "      <td>2019-03-04 02:07:45</td>\n",
       "      <td>2</td>\n",
       "      <td>qy893</td>\n",
       "      <td>qy8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599277</th>\n",
       "      <td>1559566155</td>\n",
       "      <td>2019-03-04T01:27:03+08:00</td>\n",
       "      <td>-8.216223</td>\n",
       "      <td>124.528631</td>\n",
       "      <td>2019-03-04 01:27:03</td>\n",
       "      <td>1</td>\n",
       "      <td>qy893</td>\n",
       "      <td>qy8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599278 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cuebiq_id       event_zoned_datetime       lat         lng  \\\n",
       "0       1517207294  2019-03-04T00:52:01+08:00 -8.528349  115.243144   \n",
       "1       1517207294  2019-03-04T00:45:32+08:00 -8.223645  115.396520   \n",
       "2       1522013587  2019-03-04T01:51:05+07:00 -7.669646  111.107331   \n",
       "3       1522013587  2019-03-04T01:59:12+07:00 -7.669308  111.107703   \n",
       "4       1818627892  2019-03-04T01:07:10+07:00 -6.189541  106.813124   \n",
       "...            ...                        ...       ...         ...   \n",
       "599273  1750154524  2019-03-04T02:25:59+07:00 -6.151897  106.895090   \n",
       "599274  1750154524  2019-03-04T02:59:32+07:00 -6.151897  106.895090   \n",
       "599275  1752656232  2019-03-04T00:16:04+07:00 -0.057598  109.356632   \n",
       "599276  1583178821  2019-03-04T02:07:45+08:00 -8.211994  124.528087   \n",
       "599277  1559566155  2019-03-04T01:27:03+08:00 -8.216223  124.528631   \n",
       "\n",
       "       event_datetime_local  event_hour geohash5 geohash3  \n",
       "0       2019-03-04 00:52:01           0    qw3zq      qw3  \n",
       "1       2019-03-04 00:45:32           0    qwd11      qwd  \n",
       "2       2019-03-04 01:51:05           1    qqxh2      qqx  \n",
       "3       2019-03-04 01:59:12           1    qqxh2      qqx  \n",
       "4       2019-03-04 01:07:10           1    qqguy      qqg  \n",
       "...                     ...         ...      ...      ...  \n",
       "599273  2019-03-04 02:25:59           2    qquj0      qqu  \n",
       "599274  2019-03-04 02:59:32           2    qquj0      qqu  \n",
       "599275  2019-03-04 00:16:04           0    qrvz8      qrv  \n",
       "599276  2019-03-04 02:07:45           2    qy893      qy8  \n",
       "599277  2019-03-04 01:27:03           1    qy893      qy8  \n",
       "\n",
       "[599278 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate geohashes\n",
    "pe_dl_table_gen['geohash5'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "pe_dl_table_gen['geohash3'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea44cc33-43de-4484-ba65-c9ee31f13ba2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>intervals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6rgpz</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6rgrg</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6rgru</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6rgrv</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6rgry</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>d650p</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>d6h1s</td>\n",
       "      <td>199</td>\n",
       "      <td>27</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>d6h1t</td>\n",
       "      <td>66</td>\n",
       "      <td>16</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>d6h8e</td>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>d6h8s</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     geohash5  no_of_points  no_of_unique_users  \\\n",
       "30      6rgpz            26                  12   \n",
       "34      6rgrg            21                  15   \n",
       "35      6rgru            36                  23   \n",
       "36      6rgrv            27                  19   \n",
       "37      6rgry            32                  14   \n",
       "...       ...           ...                 ...   \n",
       "2353    d650p            65                  12   \n",
       "2359    d6h1s           199                  27   \n",
       "2360    d6h1t            66                  16   \n",
       "2363    d6h8e            59                  15   \n",
       "2365    d6h8s           100                  17   \n",
       "\n",
       "                                    intervals  \n",
       "30    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "34    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "35    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "36    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "37    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "...                                       ...  \n",
       "2353  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2359  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2360  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2363  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2365  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "\n",
       "[279 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0fc0e4-8076-47f9-8f45-eebbc712092b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>event_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517207294</td>\n",
       "      <td>2019-03-04T00:52:01+08:00</td>\n",
       "      <td>-8.528349</td>\n",
       "      <td>115.243144</td>\n",
       "      <td>2019-03-04 00:52:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517207294</td>\n",
       "      <td>2019-03-04T00:45:32+08:00</td>\n",
       "      <td>-8.223645</td>\n",
       "      <td>115.396520</td>\n",
       "      <td>2019-03-04 00:45:32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1522013587</td>\n",
       "      <td>2019-03-04T01:51:05+07:00</td>\n",
       "      <td>-7.669646</td>\n",
       "      <td>111.107331</td>\n",
       "      <td>2019-03-04 01:51:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1522013587</td>\n",
       "      <td>2019-03-04T01:59:12+07:00</td>\n",
       "      <td>-7.669308</td>\n",
       "      <td>111.107703</td>\n",
       "      <td>2019-03-04 01:59:12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1818627892</td>\n",
       "      <td>2019-03-04T01:07:10+07:00</td>\n",
       "      <td>-6.189541</td>\n",
       "      <td>106.813124</td>\n",
       "      <td>2019-03-04 01:07:10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599273</th>\n",
       "      <td>1750154524</td>\n",
       "      <td>2019-03-04T02:25:59+07:00</td>\n",
       "      <td>-6.151897</td>\n",
       "      <td>106.895090</td>\n",
       "      <td>2019-03-04 02:25:59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599274</th>\n",
       "      <td>1750154524</td>\n",
       "      <td>2019-03-04T02:59:32+07:00</td>\n",
       "      <td>-6.151897</td>\n",
       "      <td>106.895090</td>\n",
       "      <td>2019-03-04 02:59:32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599275</th>\n",
       "      <td>1752656232</td>\n",
       "      <td>2019-03-04T00:16:04+07:00</td>\n",
       "      <td>-0.057598</td>\n",
       "      <td>109.356632</td>\n",
       "      <td>2019-03-04 00:16:04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599276</th>\n",
       "      <td>1583178821</td>\n",
       "      <td>2019-03-04T02:07:45+08:00</td>\n",
       "      <td>-8.211994</td>\n",
       "      <td>124.528087</td>\n",
       "      <td>2019-03-04 02:07:45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599277</th>\n",
       "      <td>1559566155</td>\n",
       "      <td>2019-03-04T01:27:03+08:00</td>\n",
       "      <td>-8.216223</td>\n",
       "      <td>124.528631</td>\n",
       "      <td>2019-03-04 01:27:03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599278 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cuebiq_id       event_zoned_datetime       lat         lng  \\\n",
       "0       1517207294  2019-03-04T00:52:01+08:00 -8.528349  115.243144   \n",
       "1       1517207294  2019-03-04T00:45:32+08:00 -8.223645  115.396520   \n",
       "2       1522013587  2019-03-04T01:51:05+07:00 -7.669646  111.107331   \n",
       "3       1522013587  2019-03-04T01:59:12+07:00 -7.669308  111.107703   \n",
       "4       1818627892  2019-03-04T01:07:10+07:00 -6.189541  106.813124   \n",
       "...            ...                        ...       ...         ...   \n",
       "599273  1750154524  2019-03-04T02:25:59+07:00 -6.151897  106.895090   \n",
       "599274  1750154524  2019-03-04T02:59:32+07:00 -6.151897  106.895090   \n",
       "599275  1752656232  2019-03-04T00:16:04+07:00 -0.057598  109.356632   \n",
       "599276  1583178821  2019-03-04T02:07:45+08:00 -8.211994  124.528087   \n",
       "599277  1559566155  2019-03-04T01:27:03+08:00 -8.216223  124.528631   \n",
       "\n",
       "       event_datetime_local  event_hour  \n",
       "0       2019-03-04 00:52:01           0  \n",
       "1       2019-03-04 00:45:32           0  \n",
       "2       2019-03-04 01:51:05           1  \n",
       "3       2019-03-04 01:59:12           1  \n",
       "4       2019-03-04 01:07:10           1  \n",
       "...                     ...         ...  \n",
       "599273  2019-03-04 02:25:59           2  \n",
       "599274  2019-03-04 02:59:32           2  \n",
       "599275  2019-03-04 00:16:04           0  \n",
       "599276  2019-03-04 02:07:45           2  \n",
       "599277  2019-03-04 01:27:03           1  \n",
       "\n",
       "[599278 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f3d588-1f07-4eb7-9fde-b9b8e34f6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     geohash5  no_of_points  no_of_unique_users  \\\n",
      "30      6rgpz            26                  12   \n",
      "34      6rgrg            21                  15   \n",
      "35      6rgru            36                  23   \n",
      "36      6rgrv            27                  19   \n",
      "37      6rgry            32                  14   \n",
      "...       ...           ...                 ...   \n",
      "2353    d650p            65                  12   \n",
      "2359    d6h1s           199                  27   \n",
      "2360    d6h1t            66                  16   \n",
      "2363    d6h8e            59                  15   \n",
      "2365    d6h8s           100                  17   \n",
      "\n",
      "                                    intervals  \n",
      "30    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "34    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "35    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "36    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "37    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "...                                       ...  \n",
      "2353  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "2359  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "2360  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "2363  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "2365  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
      "\n",
      "[279 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fbb012c-6879-460f-b74e-ce3c3afac10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>intervals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6rgpz</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6rgrg</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6rgru</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6rgrv</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6rgry</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>d650p</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>d6h1s</td>\n",
       "      <td>199</td>\n",
       "      <td>27</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>d6h1t</td>\n",
       "      <td>66</td>\n",
       "      <td>16</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>d6h8e</td>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>d6h8s</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>2019-11-01 00:00:00/2019-11-01 03:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     geohash5  no_of_points  no_of_unique_users  \\\n",
       "30      6rgpz            26                  12   \n",
       "34      6rgrg            21                  15   \n",
       "35      6rgru            36                  23   \n",
       "36      6rgrv            27                  19   \n",
       "37      6rgry            32                  14   \n",
       "...       ...           ...                 ...   \n",
       "2353    d650p            65                  12   \n",
       "2359    d6h1s           199                  27   \n",
       "2360    d6h1t            66                  16   \n",
       "2363    d6h8e            59                  15   \n",
       "2365    d6h8s           100                  17   \n",
       "\n",
       "                                    intervals  \n",
       "30    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "34    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "35    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "36    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "37    2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "...                                       ...  \n",
       "2353  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2359  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2360  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2363  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "2365  2019-11-01 00:00:00/2019-11-01 03:00:00  \n",
       "\n",
       "[279 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a0b31-b807-4379-a859-4fedd6e15832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab1ede-6b29-4dc0-812d-70013cacb04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ce8cd-9b8e-409c-b680-508c00dc6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17829f-2722-48d6-975c-9a3bb6623f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc2e21-e91d-4109-ac04-0c5a2de5b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0353dd-55c7-42de-837f-119f48be6435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2c057-683c-451d-8970-6b6d8e5f8172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad075e-4927-4a9c-ab40-ed648ec5dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c21ab-230e-4951-998a-2f67576f87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279564a-3432-451d-9f63-bd40672b375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aabfc8-5afd-4644-bde7-5055d567d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964deb-74d2-482b-9cd2-812b2da03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808822b-920e-4541-9c13-b335f8b43a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630359c-25d2-4a9c-be72-973dd6c58e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71e37f-1292-4161-8bbf-b5fdc1114214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
