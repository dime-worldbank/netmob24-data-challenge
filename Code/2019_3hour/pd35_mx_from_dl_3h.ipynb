{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from geohash2) (0.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c200a594-7174-439d-9374-317cc03ed652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a044108c-c3e8-4325-8f00-6fcbe62ea69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277f112-2f83-42fc-9b8c-601e7b6f5536",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export to jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a883a0ca-4e38-476c-843f-8d41e0b28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-03-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'MX'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "export_path = '/home/jovyan/Data/pd3/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381a613-35a2-4ea3-8ab0-3215806ca7d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:06:27,745 - INFO - Executing SQL query for date 20190101 and interval 0 to 3\n",
      "2024-07-03 16:12:15,329 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 16:12:16,025 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 16:12:16,053 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 16:12:16,681 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 16:12:16,689 - INFO - Appended data for date 20190101 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:12:16,690 - INFO - Executing SQL query for date 20190101 and interval 3 to 6\n",
      "2024-07-03 16:17:09,696 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 16:17:10,067 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 16:17:10,091 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 16:17:10,422 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 16:17:10,428 - INFO - Appended data for date 20190101 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:17:10,429 - INFO - Executing SQL query for date 20190101 and interval 6 to 9\n",
      "2024-07-03 16:22:01,080 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 16:22:01,462 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 16:22:01,486 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 16:22:01,796 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 16:22:01,802 - INFO - Appended data for date 20190101 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:22:01,803 - INFO - Executing SQL query for date 20190101 and interval 9 to 12\n",
      "2024-07-03 16:25:48,561 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 16:25:49,282 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 16:25:49,306 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 16:25:49,928 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 16:25:49,934 - INFO - Appended data for date 20190101 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:25:49,935 - INFO - Executing SQL query for date 20190101 and interval 12 to 15\n",
      "2024-07-03 16:29:40,132 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 16:29:41,405 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 16:29:41,444 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 16:29:42,570 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 16:29:42,577 - INFO - Appended data for date 20190101 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:29:42,577 - INFO - Executing SQL query for date 20190101 and interval 15 to 18\n",
      "2024-07-03 16:35:34,115 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 16:35:35,366 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 16:35:35,397 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 16:35:36,549 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 16:35:36,555 - INFO - Appended data for date 20190101 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:35:36,556 - INFO - Executing SQL query for date 20190101 and interval 18 to 21\n",
      "2024-07-03 16:42:03,072 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 16:42:04,218 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 16:42:04,256 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 16:42:05,268 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 16:42:05,275 - INFO - Appended data for date 20190101 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:42:05,276 - INFO - Executing SQL query for date 20190101 and interval 21 to 24\n",
      "2024-07-03 16:46:57,902 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 16:46:58,653 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 16:46:58,675 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 16:46:59,318 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 16:46:59,330 - INFO - Appended data for date 20190101 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:46:59,330 - INFO - Executing SQL query for date 20190102 and interval 0 to 3\n",
      "2024-07-03 16:50:40,707 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 16:50:41,035 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 16:50:41,056 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 16:50:41,352 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 16:50:41,360 - INFO - Appended data for date 20190102 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:50:41,361 - INFO - Executing SQL query for date 20190102 and interval 3 to 6\n",
      "2024-07-03 16:55:15,334 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 16:55:15,605 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 16:55:15,622 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 16:55:15,854 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 16:55:15,860 - INFO - Appended data for date 20190102 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 16:55:15,861 - INFO - Executing SQL query for date 20190102 and interval 6 to 9\n",
      "2024-07-03 17:00:02,227 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 17:00:02,881 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:00:02,908 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 17:00:03,460 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:00:03,467 - INFO - Appended data for date 20190102 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:00:03,468 - INFO - Executing SQL query for date 20190102 and interval 9 to 12\n",
      "2024-07-03 17:04:22,448 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 17:04:23,578 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 17:04:23,610 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 17:04:24,619 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 17:04:24,627 - INFO - Appended data for date 20190102 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:04:24,627 - INFO - Executing SQL query for date 20190102 and interval 12 to 15\n",
      "2024-07-03 17:11:10,762 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 17:11:12,213 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 17:11:12,246 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 17:11:13,574 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 17:11:13,582 - INFO - Appended data for date 20190102 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:11:13,583 - INFO - Executing SQL query for date 20190102 and interval 15 to 18\n",
      "2024-07-03 17:17:33,963 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 17:17:35,469 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 17:17:35,504 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 17:17:36,882 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 17:17:36,900 - INFO - Appended data for date 20190102 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:17:36,901 - INFO - Executing SQL query for date 20190102 and interval 18 to 21\n",
      "2024-07-03 17:18:32,276 - INFO - failed after 3 attempts\n",
      "2024-07-03 17:18:33,207 - INFO - failed after 3 attempts\n",
      "2024-07-03 17:18:33,207 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 501, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 821, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 838, in fetch\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 615, in cancel\n",
      "    self._query.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 862, in cancel\n",
      "    self._request.raise_response_error(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "2024-07-03 17:18:33,210 - ERROR - Error while processing data for date 20190102 interval 18 to 21: error 502: bad gateway\n",
      "2024-07-03 17:18:33,211 - INFO - Executing SQL query for date 20190102 and interval 21 to 24\n",
      "2024-07-03 17:18:34,704 - INFO - failed after 3 attempts\n",
      "2024-07-03 17:18:34,705 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 482, in execute\n",
      "    self._prepare_statement(operation, statement_name)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 379, in _prepare_statement\n",
      "    query.execute()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 807, in execute\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 612, in cancel\n",
      "    raise trino.exceptions.OperationalError(\n",
      "trino.exceptions.OperationalError: Cancel query failed; no running query\n",
      "2024-07-03 17:18:35,314 - INFO - failed after 3 attempts\n",
      "2024-07-03 17:18:35,315 - ERROR - Error while processing data for date 20190102 interval 21 to 24: error 502: bad gateway\n",
      "2024-07-03 17:18:35,315 - INFO - Executing SQL query for date 20190103 and interval 0 to 3\n",
      "2024-07-03 17:22:35,002 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 17:22:35,332 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 17:22:35,354 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 17:22:35,667 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 17:22:35,677 - INFO - Appended data for date 20190103 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:22:35,678 - INFO - Executing SQL query for date 20190103 and interval 3 to 6\n",
      "2024-07-03 17:25:31,961 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 17:25:32,230 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 17:25:32,251 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 17:25:32,492 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 17:25:32,502 - INFO - Appended data for date 20190103 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:25:32,503 - INFO - Executing SQL query for date 20190103 and interval 6 to 9\n",
      "2024-07-03 17:28:11,581 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 17:28:12,347 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:28:12,373 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 17:28:13,012 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:28:13,023 - INFO - Appended data for date 20190103 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:28:13,029 - INFO - Executing SQL query for date 20190103 and interval 9 to 12\n",
      "2024-07-03 17:32:35,534 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 17:32:36,788 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 17:32:36,816 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 17:32:37,926 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 17:32:37,936 - INFO - Appended data for date 20190103 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:32:37,937 - INFO - Executing SQL query for date 20190103 and interval 12 to 15\n",
      "2024-07-03 17:37:05,925 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 17:37:07,314 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 17:37:07,346 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 17:37:08,648 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 17:37:08,659 - INFO - Appended data for date 20190103 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:37:08,660 - INFO - Executing SQL query for date 20190103 and interval 15 to 18\n",
      "2024-07-03 17:41:52,713 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 17:41:54,155 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 17:41:54,184 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 17:41:55,501 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 17:41:55,513 - INFO - Appended data for date 20190103 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:41:55,513 - INFO - Executing SQL query for date 20190103 and interval 18 to 21\n",
      "2024-07-03 17:46:03,591 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 17:46:04,918 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 17:46:04,944 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 17:46:06,155 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 17:46:06,165 - INFO - Appended data for date 20190103 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:46:06,166 - INFO - Executing SQL query for date 20190103 and interval 21 to 24\n",
      "2024-07-03 17:48:51,731 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 17:48:52,524 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 17:48:52,553 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 17:48:53,264 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 17:48:53,273 - INFO - Appended data for date 20190103 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:48:53,274 - INFO - Executing SQL query for date 20190104 and interval 0 to 3\n",
      "2024-07-03 17:51:40,498 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 17:51:40,850 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 17:51:40,867 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 17:51:41,174 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 17:51:41,184 - INFO - Appended data for date 20190104 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:51:41,185 - INFO - Executing SQL query for date 20190104 and interval 3 to 6\n",
      "2024-07-03 17:54:51,763 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 17:54:52,038 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 17:54:52,056 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 17:54:52,298 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 17:54:52,308 - INFO - Appended data for date 20190104 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:54:52,308 - INFO - Executing SQL query for date 20190104 and interval 6 to 9\n",
      "2024-07-03 17:57:37,310 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 17:57:38,052 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:57:38,076 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 17:57:38,739 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 17:57:38,749 - INFO - Appended data for date 20190104 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 17:57:38,750 - INFO - Executing SQL query for date 20190104 and interval 9 to 12\n",
      "2024-07-03 18:00:59,546 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 18:01:00,718 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:01:00,744 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 18:01:01,793 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:01:01,809 - INFO - Appended data for date 20190104 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:01:01,810 - INFO - Executing SQL query for date 20190104 and interval 12 to 15\n",
      "2024-07-03 18:05:31,076 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 18:05:32,496 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 18:05:32,530 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 18:05:33,830 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 18:05:33,848 - INFO - Appended data for date 20190104 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:05:33,849 - INFO - Executing SQL query for date 20190104 and interval 15 to 18\n",
      "2024-07-03 18:09:19,792 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 18:09:21,260 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 18:09:21,288 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 18:09:22,596 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 18:09:22,607 - INFO - Appended data for date 20190104 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:09:22,608 - INFO - Executing SQL query for date 20190104 and interval 18 to 21\n",
      "2024-07-03 18:13:55,131 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 18:13:56,482 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 18:13:56,509 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 18:13:57,738 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 18:13:57,750 - INFO - Appended data for date 20190104 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:13:57,750 - INFO - Executing SQL query for date 20190104 and interval 21 to 24\n",
      "2024-07-03 18:17:37,882 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 18:17:38,802 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 18:17:38,826 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 18:17:39,604 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 18:17:39,614 - INFO - Appended data for date 20190104 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:17:39,616 - INFO - Executing SQL query for date 20190105 and interval 0 to 3\n",
      "2024-07-03 18:20:35,441 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 18:20:35,847 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 18:20:35,870 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 18:20:36,233 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 18:20:36,243 - INFO - Appended data for date 20190105 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:20:36,244 - INFO - Executing SQL query for date 20190105 and interval 3 to 6\n",
      "2024-07-03 18:23:38,872 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 18:23:39,148 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 18:23:39,165 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 18:23:39,414 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 18:23:39,428 - INFO - Appended data for date 20190105 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:23:39,428 - INFO - Executing SQL query for date 20190105 and interval 6 to 9\n",
      "2024-07-03 18:26:23,901 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 18:26:24,480 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 18:26:24,505 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 18:26:25,010 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 18:26:25,021 - INFO - Appended data for date 20190105 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:26:25,022 - INFO - Executing SQL query for date 20190105 and interval 9 to 12\n",
      "2024-07-03 18:31:02,914 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 18:31:04,043 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:31:04,076 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 18:31:05,065 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:31:05,076 - INFO - Appended data for date 20190105 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:31:05,077 - INFO - Executing SQL query for date 20190105 and interval 12 to 15\n",
      "2024-07-03 18:35:40,509 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 18:35:41,968 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 18:35:41,999 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 18:35:43,319 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 18:35:43,329 - INFO - Appended data for date 20190105 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:35:43,330 - INFO - Executing SQL query for date 20190105 and interval 15 to 18\n",
      "2024-07-03 18:40:14,616 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 18:40:16,064 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 18:40:16,096 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 18:40:17,407 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 18:40:17,418 - INFO - Appended data for date 20190105 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:40:17,418 - INFO - Executing SQL query for date 20190105 and interval 18 to 21\n",
      "2024-07-03 18:44:44,006 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 18:44:45,237 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 18:44:45,268 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 18:44:46,363 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 18:44:46,374 - INFO - Appended data for date 20190105 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:44:46,374 - INFO - Executing SQL query for date 20190105 and interval 21 to 24\n",
      "2024-07-03 18:47:26,399 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 18:47:27,274 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 18:47:27,295 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 18:47:28,078 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 18:47:28,088 - INFO - Appended data for date 20190105 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:47:28,088 - INFO - Executing SQL query for date 20190106 and interval 0 to 3\n",
      "2024-07-03 18:50:03,655 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 18:50:04,114 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 18:50:04,134 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 18:50:04,542 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 18:50:04,555 - INFO - Appended data for date 20190106 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:50:04,556 - INFO - Executing SQL query for date 20190106 and interval 3 to 6\n",
      "2024-07-03 18:53:02,388 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 18:53:02,659 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 18:53:02,678 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 18:53:02,911 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 18:53:02,929 - INFO - Appended data for date 20190106 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:53:02,929 - INFO - Executing SQL query for date 20190106 and interval 6 to 9\n",
      "2024-07-03 18:55:56,419 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 18:55:56,861 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 18:55:56,884 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 18:55:57,263 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 18:55:57,272 - INFO - Appended data for date 20190106 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:55:57,273 - INFO - Executing SQL query for date 20190106 and interval 9 to 12\n",
      "2024-07-03 18:59:04,662 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 18:59:05,641 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:59:05,674 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 18:59:06,504 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 18:59:06,514 - INFO - Appended data for date 20190106 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 18:59:06,515 - INFO - Executing SQL query for date 20190106 and interval 12 to 15\n",
      "2024-07-03 19:02:40,478 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 19:02:41,768 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 19:02:41,796 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 19:02:42,925 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 19:02:42,935 - INFO - Appended data for date 20190106 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:02:42,936 - INFO - Executing SQL query for date 20190106 and interval 15 to 18\n",
      "2024-07-03 19:06:51,680 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 19:06:52,889 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 19:06:52,916 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 19:06:53,974 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 19:06:53,984 - INFO - Appended data for date 20190106 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:06:53,984 - INFO - Executing SQL query for date 20190106 and interval 18 to 21\n",
      "2024-07-03 19:10:58,991 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 19:11:00,085 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 19:11:00,113 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 19:11:01,112 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 19:11:01,122 - INFO - Appended data for date 20190106 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:11:01,122 - INFO - Executing SQL query for date 20190106 and interval 21 to 24\n",
      "2024-07-03 19:14:11,948 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 19:14:12,683 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 19:14:12,704 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 19:14:13,344 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 19:14:13,353 - INFO - Appended data for date 20190106 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:14:13,354 - INFO - Executing SQL query for date 20190107 and interval 0 to 3\n",
      "2024-07-03 19:16:58,827 - INFO - Aggregating data for geohash5 for interval 0 to 3\n",
      "2024-07-03 19:16:59,115 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 19:16:59,134 - INFO - Aggregating data for geohash3 for interval 0 to 3\n",
      "2024-07-03 19:16:59,387 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-03 19:16:59,410 - INFO - Appended data for date 20190107 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:16:59,410 - INFO - Executing SQL query for date 20190107 and interval 3 to 6\n",
      "2024-07-03 19:19:59,167 - INFO - Aggregating data for geohash5 for interval 3 to 6\n",
      "2024-07-03 19:19:59,415 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 19:19:59,431 - INFO - Aggregating data for geohash3 for interval 3 to 6\n",
      "2024-07-03 19:19:59,650 - INFO - Exporting data for interval 3 to 6\n",
      "2024-07-03 19:19:59,660 - INFO - Appended data for date 20190107 interval 3 to 6 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:19:59,661 - INFO - Executing SQL query for date 20190107 and interval 6 to 9\n",
      "2024-07-03 19:23:30,589 - INFO - Aggregating data for geohash5 for interval 6 to 9\n",
      "2024-07-03 19:23:31,339 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 19:23:31,363 - INFO - Aggregating data for geohash3 for interval 6 to 9\n",
      "2024-07-03 19:23:32,040 - INFO - Exporting data for interval 6 to 9\n",
      "2024-07-03 19:23:32,050 - INFO - Appended data for date 20190107 interval 6 to 9 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:23:32,051 - INFO - Executing SQL query for date 20190107 and interval 9 to 12\n",
      "2024-07-03 19:27:07,274 - INFO - Aggregating data for geohash5 for interval 9 to 12\n",
      "2024-07-03 19:27:08,360 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 19:27:08,385 - INFO - Aggregating data for geohash3 for interval 9 to 12\n",
      "2024-07-03 19:27:09,397 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-03 19:27:09,408 - INFO - Appended data for date 20190107 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:27:09,408 - INFO - Executing SQL query for date 20190107 and interval 12 to 15\n",
      "2024-07-03 19:31:37,329 - INFO - Aggregating data for geohash5 for interval 12 to 15\n",
      "2024-07-03 19:31:38,628 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 19:31:38,655 - INFO - Aggregating data for geohash3 for interval 12 to 15\n",
      "2024-07-03 19:31:39,773 - INFO - Exporting data for interval 12 to 15\n",
      "2024-07-03 19:31:39,783 - INFO - Appended data for date 20190107 interval 12 to 15 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:31:39,784 - INFO - Executing SQL query for date 20190107 and interval 15 to 18\n",
      "2024-07-03 19:35:46,720 - INFO - Aggregating data for geohash5 for interval 15 to 18\n",
      "2024-07-03 19:35:48,001 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 19:35:48,029 - INFO - Aggregating data for geohash3 for interval 15 to 18\n",
      "2024-07-03 19:35:49,141 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-03 19:35:49,151 - INFO - Appended data for date 20190107 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:35:49,152 - INFO - Executing SQL query for date 20190107 and interval 18 to 21\n",
      "2024-07-03 19:39:25,328 - INFO - Aggregating data for geohash5 for interval 18 to 21\n",
      "2024-07-03 19:39:26,516 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 19:39:26,542 - INFO - Aggregating data for geohash3 for interval 18 to 21\n",
      "2024-07-03 19:39:27,625 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-03 19:39:27,642 - INFO - Appended data for date 20190107 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:39:27,643 - INFO - Executing SQL query for date 20190107 and interval 21 to 24\n",
      "2024-07-03 19:43:10,163 - INFO - Aggregating data for geohash5 for interval 21 to 24\n",
      "2024-07-03 19:43:10,854 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 19:43:10,878 - INFO - Aggregating data for geohash3 for interval 21 to 24\n",
      "2024-07-03 19:43:11,494 - INFO - Exporting data for interval 21 to 24\n",
      "2024-07-03 19:43:11,504 - INFO - Appended data for date 20190107 interval 21 to 24 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-03 19:43:11,504 - INFO - Executing SQL query for date 20190108 and interval 0 to 3\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00\"\n",
    "\n",
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            for geohash_col, export_file_path, write_header in [\n",
    "                ('geohash5', export_file_path_5, write_header_5),\n",
    "                ('geohash3', export_file_path_3, write_header_3)\n",
    "            ]:\n",
    "                # Aggregate data for geohash\n",
    "                logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "                aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                    no_of_points=(geohash_col, 'size'),\n",
    "                    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                    local_time=('3_hour_interval', 'first'),\n",
    "                    local_date=('local_date', 'first')\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Filter rows with no_of_unique_users > 10\n",
    "                filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "                \n",
    "                # Append the DataFrame to the CSV file\n",
    "                logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "                filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "                \n",
    "                # After the first write, set the header flag to False\n",
    "                if geohash_col == 'geohash5':\n",
    "                    write_header_5 = False\n",
    "                else:\n",
    "                    write_header_3 = False\n",
    "                \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a1127-0a10-4c27-a131-467026fbae94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c274-2c3d-4198-8d89-2c127eb4e280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c91b-b3c2-4e93-9b79-6411d7b47f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8948ce-1ebc-42c4-af54-844999063440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd0a4d0-8ea5-495a-881b-7633a3c85d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export to schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e34172-ee99-417e-b800-e6a93bc1f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:03:54,106 - INFO - Creating master table: pd_mx_2019_3h_agg3\n",
      "2024-07-03 16:03:54,158 - INFO - Creating master table: pd_mx_2019_3h_agg5\n"
     ]
    }
   ],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "# output_schema_name = 'presence_data'\n",
    "output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-03-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'MX'\n",
    "country_abbre = country_code.lower()  \n",
    "master_table_3 = f\"pd_{country_abbre}_2019_3h_agg3\"\n",
    "master_table_5 = f\"pd_{country_abbre}_2019_3h_agg5\"\n",
    "\n",
    "# Create the master tables if they do not exist\n",
    "create_table_query_3 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_3}(\n",
    "    geohash_3 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "create_table_query_5 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_5}(\n",
    "    geohash_5 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "    \n",
    "with con.connect() as connection:\n",
    "    logging.info(f\"Creating master table: {master_table_3}\")\n",
    "    connection.execute(create_table_query_3)\n",
    "    logging.info(f\"Creating master table: {master_table_5}\")\n",
    "    connection.execute(create_table_query_5)\n",
    "\n",
    "# List to record errors\n",
    "error_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536c09d-ea52-406b-9c88-ee97c376a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Process for geohash_5\n",
    "            logging.info(f\"Aggregating data for geohash_5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = pe_dl_table_gen.groupby('geohash_5').agg(\n",
    "                no_of_points=('geohash_5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_5.to_sql(master_table_5, con, if_exists='append', index=False)\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into {master_table_5}\")\n",
    "\n",
    "            # Process for geohash_3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = pe_dl_table_gen.groupby('geohash_3').agg(\n",
    "                no_of_points=('geohash_3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_3.to_sql(master_table_3, con, if_exists='append', index=False, method='multi')\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into the {master_table_3}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee53328-3554-4860-8729-bc48a13a89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288c67d-370a-496f-911b-e743e9642d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fe83-0e29-441c-ae94-3913a9ea56ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055991e-c5d8-44c1-b790-961344e56c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa0f7-2b33-4d15-8970-63048d233858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c587e04-766d-4c83-9234-f6bd93e6fff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bcea9-c6aa-46f1-a2c5-2d246d6716dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e18cd-7712-4ce5-8af2-17132ddfb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c1a10-ca49-4978-80af-bf1b1b436979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f70d6c-14ee-4bb3-a144-8f9d22756993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849c172-25b4-4015-9c3f-4c54c6f9b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a575-0da2-445d-a070-9d5e3a37309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f2b3f-068f-441d-b29b-785322d09091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101c3db-396b-4176-b0da-f46abb48a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721c5a7-4c76-4278-a9b9-6bc0ab54028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75951-dd0a-4bc0-80ef-2e3cb2a2f8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d343d50-9c6c-4c8b-a774-2cec03b0b627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283036a-b031-4b32-8ae0-3cc1cdad8652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34e23-66c7-485c-807c-3476a4344dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abfca8-eb70-4e28-8fb1-0665f28903e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16774b-eb43-4b9b-b101-955588f36125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8862327-eaba-46a0-8ecb-5e142581315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965534c4-906e-4714-8dae-a12a9a7d83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd686a0-4c83-4154-a4f7-37acd1960ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64342f59-eaf3-41d2-aabd-932a4994830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e038-fb28-4124-a130-d6d2b36350f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54a6f6-e75b-4644-801c-dd37105ee619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b67b-9e18-4c61-8756-3a8aaae6384c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee41c-424a-40dd-8ed8-62a8764d629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed2e44-ad40-4863-9a63-637f70c3775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5481759-bef6-47f8-84fd-bd7bd04d43dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8a3f-9bbe-4abe-9fe0-236738e72528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ef14e-1604-4c31-b723-cddf9aab9947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e495e-3f2b-4872-b595-61c1f8242278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926ef6e-772d-442f-9416-55c3c6c5c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d2b9d-7e31-4ad1-824b-5cd8a23c95e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f16fd-3cfc-4ea4-87ee-baca476c647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0f9e2-49ff-452e-8f32-d84cb913dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51349b29-b674-4f30-83bd-9f6386e5655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af6712-172d-42e8-92ef-e193c08202ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a73f2-0941-4640-b810-139180fc4924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369faf8d-fc67-4b50-99e2-698af62fb47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696fe4f-4b3c-456f-9862-69ae758fc1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915eb14-07e3-496b-b8b2-733d01d80cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a0b31-b807-4379-a859-4fedd6e15832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab1ede-6b29-4dc0-812d-70013cacb04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ce8cd-9b8e-409c-b680-508c00dc6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17829f-2722-48d6-975c-9a3bb6623f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc2e21-e91d-4109-ac04-0c5a2de5b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0353dd-55c7-42de-837f-119f48be6435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2c057-683c-451d-8970-6b6d8e5f8172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad075e-4927-4a9c-ab40-ed648ec5dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c21ab-230e-4951-998a-2f67576f87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279564a-3432-451d-9f63-bd40672b375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aabfc8-5afd-4644-bde7-5055d567d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964deb-74d2-482b-9cd2-812b2da03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808822b-920e-4541-9c13-b335f8b43a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630359c-25d2-4a9c-be72-973dd6c58e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71e37f-1292-4161-8bbf-b5fdc1114214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
