{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-geohash in /srv/conda/envs/notebook/lib/python3.9/site-packages (0.8.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f56262-6bce-45d7-8ce8-5c1b8e423558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'od_matrix'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9771058-c43d-4cf3-97eb-3f91d2ca61ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 04:37:26,473 - INFO - Executing SQL query for date 20190110\n",
      "2024-06-14 04:37:44,740 - INFO - Inserted data into table od_in_20190110_agg3\n",
      "2024-06-14 04:37:59,259 - INFO - Executing SQL query for date 20190111\n",
      "2024-06-14 04:38:17,426 - INFO - Inserted data into table od_in_20190111_agg3\n",
      "2024-06-14 04:38:30,771 - INFO - Executing SQL query for date 20190112\n",
      "2024-06-14 04:38:44,432 - INFO - Inserted data into table od_in_20190112_agg3\n",
      "2024-06-14 04:38:57,106 - INFO - Executing SQL query for date 20190113\n",
      "2024-06-14 04:39:10,710 - INFO - Inserted data into table od_in_20190113_agg3\n",
      "2024-06-14 04:39:10,711 - INFO - Total processing time: 123.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=3), axis=1)\n",
    "        pe_tj_df['end_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=3), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist3 = sql_engine.read_sql(f\"SELECT geohash3 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg3\")\n",
    "            geohash_dict3 = celllist3.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash3 data for date {event_date}: {e}\")\n",
    "            geohash_dict3 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "        aggregated_df3['start_geohash_user'] = aggregated_df3['start_geohash3'].map(geohash_dict3)\n",
    "        aggregated_df3['end_geohash_user'] = aggregated_df3['end_geohash3'].map(geohash_dict3)\n",
    "\n",
    "        # Filter aggregated data and reorder columns\n",
    "        filtered_df3 = aggregated_df3.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        filtered_df3 = filtered_df3[['start_geohash3', 'start_geohash_user', 'end_geohash3', 'end_geohash_user', 'trip_count', \n",
    "                                     'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                     'm_points_no', 'mdn_points_no', 'sd_points_no']]\n",
    "        return filtered_df3\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Main processing loop\n",
    "def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "        \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        filtered_df3 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "        # Insert data into the database\n",
    "        if not filtered_df3.empty:\n",
    "            table_name = f\"od_{country_code.lower()}_{event_date}_agg3\"\n",
    "            insert_data_with_retry(filtered_df3, table_name, con)\n",
    "        else:\n",
    "            logging.info(f\"No data to insert for date {event_date}\")\n",
    "\n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Example usage:\n",
    "process_date_range(datetime(2019, 1, 11), datetime(2019, 12, 31), 'IN', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94494c01-03d4-4b31-85ce-6f36fd7bb4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd26763-3250-4c11-a0f2-5bc6b1bc9c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0e731-ca45-483d-b5a2-4e84d0425d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba15bf9-7bd3-4950-8315-878df1634a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca435a9f-d6f9-4566-92a6-34fc21f9e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 02:20:43,371 - INFO - Executing SQL query for date 20190421\n",
      "2024-06-15 02:21:01,023 - INFO - Inserted data into table od_in_20190421_agg3\n",
      "2024-06-15 02:21:20,811 - INFO - Executing SQL query for date 20190422\n",
      "2024-06-15 02:21:39,192 - INFO - Inserted data into table od_in_20190422_agg3\n",
      "2024-06-15 02:21:58,097 - INFO - Executing SQL query for date 20190423\n",
      "2024-06-15 02:22:16,496 - INFO - Inserted data into table od_in_20190423_agg3\n",
      "2024-06-15 02:22:32,988 - INFO - Executing SQL query for date 20190630\n",
      "2024-06-15 02:22:50,139 - INFO - Inserted data into table od_in_20190630_agg3\n",
      "2024-06-15 02:23:11,262 - INFO - Executing SQL query for date 20190815\n",
      "2024-06-15 02:23:28,262 - INFO - Inserted data into table od_in_20190815_agg3\n",
      "2024-06-15 02:23:48,846 - INFO - Executing SQL query for date 20190816\n",
      "2024-06-15 02:24:06,995 - INFO - Inserted data into table od_in_20190816_agg3\n",
      "2024-06-15 02:24:23,760 - INFO - Executing SQL query for date 20191111\n",
      "2024-06-15 02:24:37,765 - INFO - Inserted data into table od_in_20191111_agg3\n",
      "2024-06-15 02:24:53,176 - INFO - Executing SQL query for date 20191112\n",
      "2024-06-15 02:25:12,363 - INFO - Inserted data into table od_in_20191112_agg3\n",
      "2024-06-15 02:25:30,273 - INFO - Executing SQL query for date 20191113\n",
      "2024-06-15 02:25:49,823 - INFO - Inserted data into table od_in_20191113_agg3\n",
      "2024-06-15 02:26:02,591 - INFO - Executing SQL query for date 20191224\n",
      "2024-06-15 02:26:15,877 - INFO - Inserted data into table od_in_20191224_agg3\n",
      "2024-06-15 02:26:30,480 - INFO - Executing SQL query for date 20190825\n",
      "2024-06-15 02:26:47,469 - INFO - Inserted data into table od_in_20190825_agg3\n",
      "2024-06-15 02:27:03,301 - INFO - Executing SQL query for date 20190826\n",
      "2024-06-15 02:27:20,755 - INFO - Inserted data into table od_in_20190826_agg3\n",
      "2024-06-15 02:27:37,955 - INFO - Executing SQL query for date 20190827\n",
      "2024-06-15 02:27:55,232 - INFO - Inserted data into table od_in_20190827_agg3\n",
      "2024-06-15 02:28:11,842 - INFO - Executing SQL query for date 20190828\n",
      "2024-06-15 02:28:29,447 - INFO - Inserted data into table od_in_20190828_agg3\n",
      "2024-06-15 02:28:46,581 - INFO - Executing SQL query for date 20190829\n",
      "2024-06-15 02:29:04,501 - INFO - Inserted data into table od_in_20190829_agg3\n",
      "2024-06-15 02:29:21,050 - INFO - Executing SQL query for date 20190830\n",
      "2024-06-15 02:29:38,555 - INFO - Inserted data into table od_in_20190830_agg3\n",
      "2024-06-15 02:29:55,131 - INFO - Executing SQL query for date 20190831\n",
      "2024-06-15 02:30:13,399 - INFO - Inserted data into table od_in_20190831_agg3\n",
      "2024-06-15 02:30:13,399 - INFO - Total processing time: 586.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# For fillin missing date gaps\n",
    "\n",
    "import pandas as pd\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'od_matrix'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=3), axis=1)\n",
    "        pe_tj_df['end_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=3), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist3 = sql_engine.read_sql(f\"SELECT geohash3 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg3\")\n",
    "            geohash_dict3 = celllist3.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash3 data for date {event_date}: {e}\")\n",
    "            geohash_dict3 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "        aggregated_df3['start_geohash_user'] = aggregated_df3['start_geohash3'].map(geohash_dict3)\n",
    "        aggregated_df3['end_geohash_user'] = aggregated_df3['end_geohash3'].map(geohash_dict3)\n",
    "\n",
    "        # Filter aggregated data and reorder columns\n",
    "        filtered_df3 = aggregated_df3.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        filtered_df3 = filtered_df3[['start_geohash3', 'start_geohash_user', 'end_geohash3', 'end_geohash_user', 'trip_count', \n",
    "                                     'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                     'm_points_no', 'mdn_points_no', 'sd_points_no']]\n",
    "        return filtered_df3\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Main processing loop for a list of dates\n",
    "def process_date_list(date_list, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "\n",
    "    for event_date in date_list:\n",
    "        filtered_df3 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "        # Insert data into the database\n",
    "        if not filtered_df3.empty:\n",
    "            # table_name = f\"{output_schema_name}.{country_code}_agg3\"\n",
    "            table_name = f\"od_{country_code.lower()}_{event_date}_agg3\"\n",
    "            insert_data_with_retry(filtered_df3, table_name, con)\n",
    "        else:\n",
    "            logging.info(f\"No data to insert for date {event_date}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Example usage with list of specific dates:\n",
    "date_list = [\"20190421\", \"20190422\", \"20190423\", \"20190630\", \"20190815\", \"20190816\", \"20191111\", \n",
    "             \"20191112\", \"20191113\", \"20191224\", \"20190825\", \"20190826\", \"20190827\", \"20190828\", \n",
    "             \"20190829\", \"20190830\", \"20190831\"]\n",
    "process_date_list(date_list, 'IN', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087639b0-581b-4dbf-a9a4-d4b6c7c05505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159c211-e0a9-4aa3-8f88-931269922469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20b8d0-be7b-44e6-95a5-ba70650ede5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
