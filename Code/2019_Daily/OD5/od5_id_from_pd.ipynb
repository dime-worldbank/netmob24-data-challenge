{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-geohash in /srv/conda/envs/notebook/lib/python3.9/site-packages (0.8.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ce44a5-4d87-414c-a497-0441c3ad2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 02:19:23,243 - INFO - Executing SQL query for date 20190528\n",
      "2024-06-07 02:21:40,539 - INFO - Data inserted into od_id_20190528_agg5\n",
      "2024-06-07 02:21:53,705 - INFO - Executing SQL query for date 20190529\n",
      "2024-06-07 02:24:17,945 - INFO - Data inserted into od_id_20190529_agg5\n",
      "2024-06-07 02:24:30,915 - INFO - Executing SQL query for date 20190530\n",
      "2024-06-07 02:27:00,669 - INFO - Data inserted into od_id_20190530_agg5\n",
      "2024-06-07 02:27:14,478 - INFO - Executing SQL query for date 20190531\n",
      "2024-06-07 02:29:50,836 - INFO - Data inserted into od_id_20190531_agg5\n",
      "2024-06-07 02:29:50,836 - INFO - Total processing time: 651.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test with gateway error handlling and continue working - working\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5), axis=1)\n",
    "        pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist5 = sql_engine.read_sql(f\"SELECT geohash5 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg5\")\n",
    "            geohash_dict5 = celllist5.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash5 data for date {event_date}: {e}\")\n",
    "            geohash_dict5 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m']\n",
    "\n",
    "        # Define the columns before mapping\n",
    "        aggregated_df5['start_geohash_user'] = aggregated_df5['start_geohash5'].map(geohash_dict5)\n",
    "        aggregated_df5['end_geohash_user'] = aggregated_df5['end_geohash5'].map(geohash_dict5)\n",
    "\n",
    "        # Filter aggregated data\n",
    "        filtered_df5 = aggregated_df5.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        \n",
    "        return filtered_df5\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data in chunks\n",
    "def insert_data_in_chunks(df, table_name, engine, chunk_size=5000):\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')\n",
    "\n",
    "# Main processing loop\n",
    "def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "        \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        try:\n",
    "            filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "            # Create the SQL engine\n",
    "            output_schema_name = \"od_matrix\"\n",
    "            final_table_5 = f\"od_{country_code.lower()}_{event_date}_agg5\"\n",
    "            con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "            # Create the SQL table with the correct name for 5-level geohash\n",
    "            create_table_query_5 = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {final_table_5} (\n",
    "                start_geohash5 varchar,\n",
    "                start_geohash_user bigint,\n",
    "                end_geohash5 varchar,\n",
    "                end_geohash_user bigint,\n",
    "                trip_count bigint,\n",
    "                m_duration_min double,\n",
    "                mdn_duration_min double,\n",
    "                sd_duration_min double,\n",
    "                m_length_m double,\n",
    "                mdn_length_m double,\n",
    "                sd_length_m double,\n",
    "                partition_key bigint\n",
    "            )\n",
    "            WITH (\n",
    "              partitioned_by = ARRAY['partition_key'],\n",
    "              bucketed_by = ARRAY['end_geohash5'],\n",
    "              bucket_count = 5\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            with con.connect() as connection:\n",
    "                connection.execute(create_table_query_5)\n",
    "\n",
    "            # Add partition key and ensure correct data types for 5-level geohash\n",
    "            filtered_df5['partition_key'] = (filtered_df5.index // 5000) + 1\n",
    "            filtered_df5 = filtered_df5.astype({\n",
    "                'start_geohash_user': 'int',\n",
    "                'end_geohash_user': 'int',\n",
    "                'trip_count': 'int',\n",
    "                'partition_key': 'int'\n",
    "            })\n",
    "\n",
    "            # Insert data into the table with the correct name\n",
    "            if not filtered_df5.empty:\n",
    "                insert_data_in_chunks(filtered_df5, final_table_5, con, 5000)\n",
    "                logging.info(f\"Data inserted into {final_table_5}\")\n",
    "            else:\n",
    "                logging.info(f\"No data to insert for {final_table_5} for 5-level geohash\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process data for date {event_date}: {e}\")\n",
    "\n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "process_date_range(datetime(2019, 11, 1), datetime(2019, 12, 31), 'ID', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fefde2-782d-4093-bdaf-53036e27a876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab65eb1-84d9-46fb-9e65-0472c002b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:05:59,959 - INFO - Executing SQL query for date 20191109\n",
      "2024-06-08 16:09:19,968 - INFO - Data inserted into od_id_20191109_agg5\n",
      "2024-06-08 16:09:33,519 - INFO - Executing SQL query for date 20191120\n",
      "2024-06-08 16:12:28,127 - INFO - Data inserted into od_id_20191120_agg5\n",
      "2024-06-08 16:12:40,973 - INFO - Executing SQL query for date 20191208\n",
      "2024-06-08 16:13:56,404 - INFO - Data inserted into od_id_20191208_agg5\n",
      "2024-06-08 16:14:21,484 - INFO - Executing SQL query for date 20191230\n",
      "2024-06-08 16:15:52,949 - INFO - Data inserted into od_id_20191230_agg5\n",
      "2024-06-08 16:15:52,950 - INFO - Total processing time: 607.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# For fillin missing data dates\n",
    "\n",
    "import pandas as pd\n",
    "import geohash\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5), axis=1)\n",
    "        pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist5 = sql_engine.read_sql(f\"SELECT geohash5 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg5\")\n",
    "            geohash_dict5 = celllist5.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash5 data for date {event_date}: {e}\")\n",
    "            geohash_dict5 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m']\n",
    "\n",
    "        # Define the columns before mapping\n",
    "        aggregated_df5['start_geohash_user'] = aggregated_df5['start_geohash5'].map(geohash_dict5)\n",
    "        aggregated_df5['end_geohash_user'] = aggregated_df5['end_geohash5'].map(geohash_dict5)\n",
    "\n",
    "        # Filter aggregated data\n",
    "        filtered_df5 = aggregated_df5.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        \n",
    "        return filtered_df5\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data in chunks\n",
    "def insert_data_in_chunks(df, table_name, engine, chunk_size=5000):\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')\n",
    "\n",
    "# Main processing loop for a list of dates\n",
    "def process_date_list(date_list, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "\n",
    "    for event_date in date_list:\n",
    "        try:\n",
    "            filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "            # Create the SQL engine\n",
    "            output_schema_name = \"od_matrix\"\n",
    "            final_table_5 = f\"od_{country_code.lower()}_{event_date}_agg5\"\n",
    "            con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "            # Create the SQL table with the correct name for 5-level geohash\n",
    "            create_table_query_5 = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {final_table_5} (\n",
    "                start_geohash5 varchar,\n",
    "                start_geohash_user bigint,\n",
    "                end_geohash5 varchar,\n",
    "                end_geohash_user bigint,\n",
    "                trip_count bigint,\n",
    "                m_duration_min double,\n",
    "                mdn_duration_min double,\n",
    "                sd_duration_min double,\n",
    "                m_length_m double,\n",
    "                mdn_length_m double,\n",
    "                sd_length_m double,\n",
    "                partition_key bigint\n",
    "            )\n",
    "            WITH (\n",
    "              partitioned_by = ARRAY['partition_key'],\n",
    "              bucketed_by = ARRAY['end_geohash5'],\n",
    "              bucket_count = 5\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            with con.connect() as connection:\n",
    "                connection.execute(create_table_query_5)\n",
    "\n",
    "            # Add partition key and ensure correct data types for 5-level geohash\n",
    "            filtered_df5['partition_key'] = (filtered_df5.index // 5000) + 1\n",
    "            filtered_df5 = filtered_df5.astype({\n",
    "                'start_geohash_user': 'int',\n",
    "                'end_geohash_user': 'int',\n",
    "                'trip_count': 'int',\n",
    "                'partition_key': 'int'\n",
    "            })\n",
    "\n",
    "            # Insert data into the table with the correct name\n",
    "            if not filtered_df5.empty:\n",
    "                insert_data_in_chunks(filtered_df5, final_table_5, con, 5000)\n",
    "                logging.info(f\"Data inserted into {final_table_5}\")\n",
    "            else:\n",
    "                logging.info(f\"No data to insert for {final_table_5} for 5-level geohash\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process data for date {event_date}: {e}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Example usage with list of specific dates:\n",
    "date_list = date_list = ['20191109', '20191120', '20191208', '20191230']\n",
    "\n",
    "process_date_list(date_list, 'ID', sql_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a39b4-4e5d-4fec-8f6b-17cedaa74efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8083f-bb6f-4ca6-8008-3861ebd079b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ab54-7ea9-4f44-8d58-48a9c5ed510f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9950b-f813-4496-95ee-8a850ab07181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7985f7e-5712-4bda-9320-de3f189b48f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 22:08:45,701 - INFO - Executing SQL query for date 20190515\n",
      "2024-06-06 22:11:16,100 - INFO - Data inserted into od_id_20190515_agg5\n",
      "2024-06-06 22:11:28,346 - INFO - Executing SQL query for date 20190516\n",
      "2024-06-06 22:13:52,708 - INFO - Data inserted into od_id_20190516_agg5\n",
      "2024-06-06 22:14:09,505 - INFO - Executing SQL query for date 20190517\n",
      "2024-06-06 22:16:36,709 - INFO - Data inserted into od_id_20190517_agg5\n",
      "2024-06-06 22:16:48,650 - INFO - Executing SQL query for date 20190518\n",
      "2024-06-06 22:19:15,876 - INFO - Data inserted into od_id_20190518_agg5\n",
      "2024-06-06 22:19:26,551 - INFO - Executing SQL query for date 20190519\n",
      "2024-06-06 22:21:42,681 - INFO - Data inserted into od_id_20190519_agg5\n",
      "2024-06-06 22:21:54,795 - INFO - Executing SQL query for date 20190520\n",
      "2024-06-06 22:24:24,868 - INFO - Data inserted into od_id_20190520_agg5\n",
      "2024-06-06 22:24:24,869 - INFO - Total processing time: 956.96 seconds\n"
     ]
    }
   ],
   "source": [
    "# # the normal and working one. \n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Function to process data for a single day\n",
    "# def process_day(event_date, country_code, sql_engine):\n",
    "#     try:\n",
    "#         # Read data from the SQL table\n",
    "#         pe_tj_df = sql_engine.read_sql(\n",
    "#             f\"\"\"\n",
    "#             SELECT \n",
    "#                 cuebiq_id,\n",
    "#                 start_lat,\n",
    "#                 start_lng,\n",
    "#                 end_lat,\n",
    "#                 end_lng,\n",
    "#                 duration_minutes,\n",
    "#                 length_meters\n",
    "#             FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "#             WHERE \n",
    "#                 event_date = {event_date}\n",
    "#                 AND end_country = '{country_code}' \n",
    "#                 AND start_country = '{country_code}' \n",
    "#             \"\"\"\n",
    "#         )\n",
    "#         logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "#         # Encode geohashes\n",
    "#         pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "#             lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5), axis=1)\n",
    "#         pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "#             lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5), axis=1)\n",
    "\n",
    "#         # Load cell lists from SQL\n",
    "#         try:\n",
    "#             celllist5 = sql_engine.read_sql(f\"SELECT geohash5 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg5\")\n",
    "#             geohash_dict5 = celllist5.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Failed to load geohash5 data for date {event_date}: {e}\")\n",
    "#             geohash_dict5 = {}\n",
    "\n",
    "#         # Add user numbers to the aggregated data\n",
    "#         aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "#             'cuebiq_id': 'count',\n",
    "#             'duration_minutes': ['mean', 'median', 'std'],\n",
    "#             'length_meters': ['mean', 'median', 'std']\n",
    "#         }).reset_index()\n",
    "#         aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m']\n",
    "\n",
    "#         # Define the columns before mapping\n",
    "#         aggregated_df5['start_geohash_user'] = aggregated_df5['start_geohash5'].map(geohash_dict5)\n",
    "#         aggregated_df5['end_geohash_user'] = aggregated_df5['end_geohash5'].map(geohash_dict5)\n",
    "\n",
    "#         # Filter aggregated data\n",
    "#         filtered_df5 = aggregated_df5.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        \n",
    "#         return filtered_df5\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # Function to insert data in chunks\n",
    "# def insert_data_in_chunks(df, table_name, engine, chunk_size=5000):\n",
    "#     for start in range(0, len(df), chunk_size):\n",
    "#         chunk = df.iloc[start:start + chunk_size]\n",
    "#         chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')\n",
    "\n",
    "# # Main processing loop\n",
    "# def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "#     start_time = time.time()  # Record start time before processing loop\n",
    "        \n",
    "#     current_date = start_date\n",
    "#     while current_date <= end_date:\n",
    "#         event_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "#         filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "#         # Create the SQL engine\n",
    "#         output_schema_name = \"od_matrix\"\n",
    "#         final_table_5 = f\"od_{country_code.lower()}_{event_date}_agg5\"\n",
    "#         con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "#         # Create the SQL table with the correct name for 5-level geohash\n",
    "#         create_table_query_5 = f\"\"\"\n",
    "#         CREATE TABLE IF NOT EXISTS {final_table_5} (\n",
    "#             start_geohash5 varchar,\n",
    "#             start_geohash_user bigint,\n",
    "#             end_geohash5 varchar,\n",
    "#             end_geohash_user bigint,\n",
    "#             trip_count bigint,\n",
    "#             m_duration_min double,\n",
    "#             mdn_duration_min double,\n",
    "#             sd_duration_min double,\n",
    "#             m_length_m double,\n",
    "#             mdn_length_m double,\n",
    "#             sd_length_m double,\n",
    "#             partition_key bigint\n",
    "#         )\n",
    "#         WITH (\n",
    "#           partitioned_by = ARRAY['partition_key'],\n",
    "#           bucketed_by = ARRAY['end_geohash5'],\n",
    "#           bucket_count = 5\n",
    "#         )\n",
    "#         \"\"\"\n",
    "\n",
    "#         with con.connect() as connection:\n",
    "#             connection.execute(create_table_query_5)\n",
    "\n",
    "#         # Add partition key and ensure correct data types for 5-level geohash\n",
    "#         filtered_df5['partition_key'] = (filtered_df5.index // 5000) + 1\n",
    "#         filtered_df5 = filtered_df5.astype({\n",
    "#             'start_geohash_user': 'int',\n",
    "#             'end_geohash_user': 'int',\n",
    "#             'trip_count': 'int',\n",
    "#             'partition_key': 'int'\n",
    "#         })\n",
    "\n",
    "#         # Insert data into the table with the correct name\n",
    "#         if not filtered_df5.empty:\n",
    "#             insert_data_in_chunks(filtered_df5, final_table_5, con, 5000)\n",
    "#             logging.info(f\"Data inserted into {final_table_5}\")\n",
    "#         else:\n",
    "#             logging.info(f\"No data to insert for {final_table_5} for 5-level geohash\")\n",
    "\n",
    "#         # Move to the next day\n",
    "#         current_date += timedelta(days=1)\n",
    "    \n",
    "#     end_time = time.time()  # Record end time after processing loop\n",
    "#     total_time = end_time - start_time\n",
    "#     logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# # Example usage:\n",
    "# process_date_range(datetime(2019, 5, 28), datetime(2019, 5, 31), 'ID', sql_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2f14e-fa1b-4810-a38d-e529d81569d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf17b0-d474-4e16-80a7-0538ca8d1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e4671-e3f7-4af4-92ec-49e7a9c8235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9771058-c43d-4cf3-97eb-3f91d2ca61ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df0a19-7477-4067-adef-72aa1d701faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb6f36-52ec-4aca-86d5-1fec273ea6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5ae12-830c-496c-be73-f527048603c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff191f4-b11e-4639-9c4e-502decf49696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d565d9-6119-467c-bef4-082acc505204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca5d0d-a0f6-43b0-af28-3d06ec9a23b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
