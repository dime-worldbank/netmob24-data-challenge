{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5563f12-6c19-4a60-9188-31a27e9ac75c",
   "metadata": {},
   "source": [
    "lack of no. of points info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-geohash\n",
      "  Using cached python_geohash-0.8.5-cp39-cp39-linux_x86_64.whl\n",
      "Installing collected packages: python-geohash\n",
      "Successfully installed python-geohash-0.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import geohash\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9771058-c43d-4cf3-97eb-3f91d2ca61ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 21:12:34,655 - INFO - Executing SQL query for date 20190714\n",
      "2024-06-07 21:12:45,217 - INFO - Inserted data into table od_id_20190714_agg3\n",
      "2024-06-07 21:12:45,218 - INFO - Total processing time: 36.24 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'od_matrix'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=3), axis=1)\n",
    "        pe_tj_df['end_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=3), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist3 = sql_engine.read_sql(f\"SELECT geohash3 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg3\")\n",
    "            geohash_dict3 = celllist3.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash3 data for date {event_date}: {e}\")\n",
    "            geohash_dict3 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "        aggregated_df3['start_geohash_user'] = aggregated_df3['start_geohash3'].map(geohash_dict3)\n",
    "        aggregated_df3['end_geohash_user'] = aggregated_df3['end_geohash3'].map(geohash_dict3)\n",
    "\n",
    "        # Filter aggregated data and reorder columns\n",
    "        filtered_df3 = aggregated_df3.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        filtered_df3 = filtered_df3[['start_geohash3', 'start_geohash_user', 'end_geohash3', 'end_geohash_user', 'trip_count', \n",
    "                                     'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                     'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                     'm_points_no', 'mdn_points_no', 'sd_points_no']]\n",
    "        return filtered_df3\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Main processing loop\n",
    "def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "        \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        filtered_df3 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "        # Insert data into the database\n",
    "        if not filtered_df3.empty:\n",
    "            table_name = f\"od_{country_code.lower()}_{event_date}_agg3\"\n",
    "            insert_data_with_retry(filtered_df3, table_name, con)\n",
    "        else:\n",
    "            logging.info(f\"No data to insert for date {event_date}\")\n",
    "\n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Example usage:\n",
    "process_date_range(datetime(2019, 7, 14), datetime(2019, 7, 14), 'ID', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba15bf9-7bd3-4950-8315-878df1634a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca435a9f-d6f9-4566-92a6-34fc21f9e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 21:25:42,437 - INFO - Executing SQL query for date 20190719\n",
      "2024-06-07 21:25:51,198 - INFO - Inserted data into table od_id_20190719_agg3\n",
      "2024-06-07 21:26:03,651 - INFO - Executing SQL query for date 20190731\n",
      "2024-06-07 21:26:12,308 - INFO - Inserted data into table od_id_20190731_agg3\n",
      "2024-06-07 21:26:24,771 - INFO - Executing SQL query for date 20190808\n",
      "2024-06-07 21:26:33,305 - INFO - Inserted data into table od_id_20190808_agg3\n",
      "2024-06-07 21:26:44,847 - INFO - Executing SQL query for date 20190810\n",
      "2024-06-07 21:26:52,437 - INFO - Inserted data into table od_id_20190810_agg3\n",
      "2024-06-07 21:27:05,505 - INFO - Executing SQL query for date 20190817\n",
      "2024-06-07 21:27:14,620 - INFO - Inserted data into table od_id_20190817_agg3\n",
      "2024-06-07 21:27:28,089 - INFO - Executing SQL query for date 20191020\n",
      "2024-06-07 21:27:37,418 - INFO - Inserted data into table od_id_20191020_agg3\n",
      "2024-06-07 21:27:50,388 - INFO - Executing SQL query for date 20191021\n",
      "2024-06-07 21:28:00,439 - INFO - Inserted data into table od_id_20191021_agg3\n",
      "2024-06-07 21:28:00,439 - INFO - Total processing time: 155.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# For fillin missing date gaps\n",
    "\n",
    "import pandas as pd\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'od_matrix'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=3), axis=1)\n",
    "        pe_tj_df['end_geohash3'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=3), axis=1)\n",
    "\n",
    "        # Load cell lists from SQL\n",
    "        try:\n",
    "            celllist3 = sql_engine.read_sql(f\"SELECT geohash3 AS geohash, no_of_unique_users FROM dedicated.pop_density.pd_{country_code}_{event_date}_agg3\")\n",
    "            geohash_dict3 = celllist3.set_index('geohash')['no_of_unique_users'].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load geohash3 data for date {event_date}: {e}\")\n",
    "            geohash_dict3 = {}\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m']\n",
    "        aggregated_df3['start_geohash_user'] = aggregated_df3['start_geohash3'].map(geohash_dict3)\n",
    "        aggregated_df3['end_geohash_user'] = aggregated_df3['end_geohash3'].map(geohash_dict3)\n",
    "\n",
    "        # Filter aggregated data and reorder columns\n",
    "        filtered_df3 = aggregated_df3.dropna(subset=['start_geohash_user', 'end_geohash_user'])\n",
    "        filtered_df3 = filtered_df3[['start_geohash3', 'start_geohash_user', 'end_geohash3', 'end_geohash_user', 'trip_count', 'm_duration_min', 'mdn_duration_min', 'sd_duration_min', 'm_length_m', 'mdn_length_m', 'sd_length_m']]\n",
    "        return filtered_df3\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Main processing loop for a list of dates\n",
    "def process_date_list(date_list, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "\n",
    "    for event_date in date_list:\n",
    "        filtered_df3 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "        # Insert data into the database\n",
    "        if not filtered_df3.empty:\n",
    "            # table_name = f\"{output_schema_name}.{country_code}_agg3\"\n",
    "            table_name = f\"od_{country_code.lower()}_{event_date}_agg3\"\n",
    "            insert_data_with_retry(filtered_df3, table_name, con)\n",
    "        else:\n",
    "            logging.info(f\"No data to insert for date {event_date}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Example usage with list of specific dates:\n",
    "date_list = ['20190719', '20190731', '20190808', '20190810', '20190817', '20191020', '20191021']\n",
    "process_date_list(date_list, 'ID', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087639b0-581b-4dbf-a9a4-d4b6c7c05505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159c211-e0a9-4aa3-8f88-931269922469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20b8d0-be7b-44e6-95a5-ba70650ede5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
