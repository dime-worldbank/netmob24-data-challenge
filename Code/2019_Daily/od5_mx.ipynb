{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb53c70-0bcd-42db-b993-22c1e27621c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb085d33-6d88-45c7-8157-b6117b60533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-geohash\n",
      "  Using cached python_geohash-0.8.5-cp39-cp39-linux_x86_64.whl\n",
      "Installing collected packages: python-geohash\n",
      "Successfully installed python-geohash-0.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geohash\n",
    "from datetime import datetime, timedelta\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552c77dc-ddde-4397-998e-37841474fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT DISTINCT\n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "        # Encode geohashes\n",
    "        pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5), axis=1)\n",
    "        pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "            lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5), axis=1)\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "            'cuebiq_id': 'count',\n",
    "            'duration_minutes': ['mean', 'median', 'std'],\n",
    "            'length_meters': ['mean', 'median', 'std'],\n",
    "            'number_of_points': ['mean', 'median', 'std']\n",
    "        }).reset_index()\n",
    "        aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                  'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                  'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                  'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "        # Filter aggregated data\n",
    "        filtered_df5 = aggregated_df5.loc[aggregated_df5['trip_count'] > 9]\n",
    "        return filtered_df5\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# # Function to insert data in chunks\n",
    "# def insert_data_in_chunks(df, table_name, engine, chunk_size):\n",
    "#     for start in range(0, len(df), chunk_size):\n",
    "#         chunk = df.iloc[start:start + chunk_size]\n",
    "#         chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')\n",
    "\n",
    "def insert_data_in_chunks(df, table_name, engine, chunk_size):\n",
    "    df.to_sql(table_name, engine, index=False, if_exists='append', method='multi', chunksize=chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f08b612-f19e-4f45-b189-5d258aa4876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop\n",
    "def process_date_range(start_date, end_date, country_code, sql_engine):\n",
    "    start_time = time.time()  # Record start time before processing loop\n",
    "    errored_dates = []  # List to store dates that encounter errors\n",
    "        \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        try:\n",
    "            filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "\n",
    "            # Create the SQL engine\n",
    "            output_schema_name = \"od_matrix_10\"\n",
    "            final_table_5 = f\"od_{country_code.lower()}_{event_date}_agg5_10\"\n",
    "            con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "            # Create the SQL table with the correct name for 5-level geohash\n",
    "            create_table_query_5 = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {final_table_5} (\n",
    "                start_geohash5 varchar,\n",
    "                end_geohash5 varchar,\n",
    "                trip_count bigint,\n",
    "                m_duration_min double,\n",
    "                mdn_duration_min double,\n",
    "                sd_duration_min double,\n",
    "                m_length_m double,\n",
    "                mdn_length_m double,\n",
    "                sd_length_m double,\n",
    "                m_points_no double,\n",
    "                mdn_points_no double,\n",
    "                sd_points_no double\n",
    "            )\n",
    "            WITH (\n",
    "              bucketed_by = ARRAY['end_geohash5'],\n",
    "              bucket_count = 30\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            with con.connect() as connection:\n",
    "                connection.execute(create_table_query_5)\n",
    "                \n",
    "            filtered_df5 = filtered_df5.astype({\n",
    "                'trip_count': 'int'\n",
    "            })\n",
    "            \n",
    "            # Insert data into the table with the correct name\n",
    "            if not filtered_df5.empty:\n",
    "                insert_data_in_chunks(filtered_df5, final_table_5, con, 1000)\n",
    "                logging.info(f\"Data inserted into {final_table_5}\")\n",
    "            else:\n",
    "                logging.info(f\"No data to insert for {final_table_5} for 5-level geohash\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process data for date {event_date}: {e}\")\n",
    "            errored_dates.append(event_date)  # Record the errored date\n",
    "\n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    end_time = time.time()  # Record end time after processing loop\n",
    "    total_time = end_time - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Log the errored dates\n",
    "    if errored_dates:\n",
    "        logging.info(f\"Errored dates: {', '.join(errored_dates)}\")\n",
    "    else:\n",
    "        logging.info(\"No errors encountered during processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd762a9-1202-4601-a6f2-9d908a44e107",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 01:50:45,036 - INFO - Executing SQL query for date 20190201\n",
      "2024-06-18 01:51:58,690 - INFO - Data inserted into od_mx_20190201_agg5_10\n",
      "2024-06-18 01:52:18,508 - INFO - Executing SQL query for date 20190202\n",
      "2024-06-18 01:53:29,075 - INFO - Data inserted into od_mx_20190202_agg5_10\n",
      "2024-06-18 01:53:50,609 - INFO - Executing SQL query for date 20190203\n",
      "2024-06-18 01:54:54,513 - INFO - Data inserted into od_mx_20190203_agg5_10\n",
      "2024-06-18 01:55:12,764 - INFO - Executing SQL query for date 20190204\n",
      "2024-06-18 01:56:12,505 - INFO - Data inserted into od_mx_20190204_agg5_10\n",
      "2024-06-18 01:56:34,532 - INFO - Executing SQL query for date 20190205\n",
      "2024-06-18 01:57:49,980 - INFO - Data inserted into od_mx_20190205_agg5_10\n",
      "2024-06-18 01:58:12,215 - INFO - Executing SQL query for date 20190206\n",
      "2024-06-18 01:59:27,151 - INFO - Data inserted into od_mx_20190206_agg5_10\n",
      "2024-06-18 01:59:51,128 - INFO - Executing SQL query for date 20190207\n",
      "2024-06-18 02:01:05,499 - INFO - Data inserted into od_mx_20190207_agg5_10\n",
      "2024-06-18 02:01:27,782 - INFO - Executing SQL query for date 20190208\n",
      "2024-06-18 02:02:39,498 - INFO - Data inserted into od_mx_20190208_agg5_10\n",
      "2024-06-18 02:02:59,282 - INFO - Executing SQL query for date 20190209\n",
      "2024-06-18 02:04:06,169 - INFO - Data inserted into od_mx_20190209_agg5_10\n",
      "2024-06-18 02:04:24,833 - INFO - Executing SQL query for date 20190210\n",
      "2024-06-18 02:05:24,842 - INFO - Data inserted into od_mx_20190210_agg5_10\n",
      "2024-06-18 02:05:45,832 - INFO - Executing SQL query for date 20190211\n",
      "2024-06-18 02:06:55,139 - INFO - Data inserted into od_mx_20190211_agg5_10\n",
      "2024-06-18 02:07:18,519 - INFO - Executing SQL query for date 20190212\n",
      "2024-06-18 02:08:19,227 - INFO - Data inserted into od_mx_20190212_agg5_10\n",
      "2024-06-18 02:08:41,137 - INFO - Executing SQL query for date 20190213\n",
      "2024-06-18 02:09:55,973 - INFO - Data inserted into od_mx_20190213_agg5_10\n",
      "2024-06-18 02:10:20,858 - INFO - Executing SQL query for date 20190214\n",
      "2024-06-18 02:11:37,882 - INFO - Data inserted into od_mx_20190214_agg5_10\n",
      "2024-06-18 02:12:01,242 - INFO - Executing SQL query for date 20190215\n",
      "2024-06-18 02:13:18,426 - INFO - Data inserted into od_mx_20190215_agg5_10\n",
      "2024-06-18 02:13:40,635 - INFO - Executing SQL query for date 20190216\n",
      "2024-06-18 02:14:54,758 - INFO - Data inserted into od_mx_20190216_agg5_10\n",
      "2024-06-18 02:15:14,950 - INFO - Executing SQL query for date 20190217\n",
      "2024-06-18 02:16:22,986 - INFO - Data inserted into od_mx_20190217_agg5_10\n",
      "2024-06-18 02:16:45,304 - INFO - Executing SQL query for date 20190218\n",
      "2024-06-18 02:18:01,509 - INFO - Data inserted into od_mx_20190218_agg5_10\n",
      "2024-06-18 02:18:24,582 - INFO - Executing SQL query for date 20190219\n",
      "2024-06-18 02:19:40,811 - INFO - Data inserted into od_mx_20190219_agg5_10\n",
      "2024-06-18 02:20:04,920 - INFO - Executing SQL query for date 20190220\n",
      "2024-06-18 02:21:24,324 - INFO - Data inserted into od_mx_20190220_agg5_10\n",
      "2024-06-18 02:21:49,192 - INFO - Executing SQL query for date 20190221\n",
      "2024-06-18 02:23:05,922 - INFO - Data inserted into od_mx_20190221_agg5_10\n",
      "2024-06-18 02:23:29,596 - INFO - Executing SQL query for date 20190222\n",
      "2024-06-18 02:24:48,141 - INFO - Data inserted into od_mx_20190222_agg5_10\n",
      "2024-06-18 02:25:13,042 - INFO - Executing SQL query for date 20190223\n",
      "2024-06-18 02:26:30,981 - INFO - Data inserted into od_mx_20190223_agg5_10\n",
      "2024-06-18 02:26:49,915 - INFO - Executing SQL query for date 20190224\n",
      "2024-06-18 02:27:53,860 - INFO - Data inserted into od_mx_20190224_agg5_10\n",
      "2024-06-18 02:28:15,279 - INFO - Executing SQL query for date 20190225\n",
      "2024-06-18 02:29:04,131 - INFO - failed after 3 attempts\n",
      "2024-06-18 02:29:05,584 - INFO - failed after 3 attempts\n",
      "2024-06-18 02:29:06,616 - INFO - failed after 3 attempts\n",
      "2024-06-18 02:29:06,617 - ERROR - Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 490, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 821, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 838, in fetch\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/sqlalchemy/dialect.py\", line 365, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 496, in execute\n",
      "    self._deallocate_prepared_statement(statement_name)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 469, in _deallocate_prepared_statement\n",
      "    query.execute()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 807, in execute\n",
      "    status = self._request.process(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 617, in process\n",
      "    self.raise_response_error(http_response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1995, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 618, in close\n",
      "    self.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py\", line 615, in cancel\n",
      "    self._query.cancel()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 862, in cancel\n",
      "    self._request.raise_response_error(response)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py\", line 600, in raise_response_error\n",
      "    raise exceptions.Http502Error(\"error 502: bad gateway\")\n",
      "trino.exceptions.Http502Error: error 502: bad gateway\n",
      "2024-06-18 02:29:06,618 - ERROR - Failed to process data for date 20190225: error 502: bad gateway\n",
      "2024-06-18 02:30:05,684 - INFO - Executing SQL query for date 20190226\n",
      "2024-06-18 02:31:29,772 - INFO - Data inserted into od_mx_20190226_agg5_10\n",
      "2024-06-18 02:31:54,240 - INFO - Executing SQL query for date 20190227\n",
      "2024-06-18 02:33:12,397 - INFO - Data inserted into od_mx_20190227_agg5_10\n",
      "2024-06-18 02:33:35,854 - INFO - Executing SQL query for date 20190228\n",
      "2024-06-18 02:34:53,099 - INFO - Data inserted into od_mx_20190228_agg5_10\n",
      "2024-06-18 02:34:53,099 - INFO - Total processing time: 2673.60 seconds\n",
      "2024-06-18 02:34:53,100 - INFO - Errored dates: 20190225\n"
     ]
    }
   ],
   "source": [
    "process_date_range(datetime(2019, 3, 1), datetime(2019, 12, 31), 'MX', sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c59fc9-2b81-4658-af45-69100b9c43ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c003a02-68a9-46b8-8e01-23717c123282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842e09da-d579-460a-9a88-f38f3036fb9d",
   "metadata": {},
   "source": [
    "# Dealing with missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d15b782-02da-46f5-bc50-3d407e164d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to process data for a single day\n",
    "def process_day(event_date, country_code, sql_engine):\n",
    "    try:\n",
    "        # Read data from the SQL table\n",
    "        pe_tj_df = sql_engine.read_sql(\n",
    "            f\"\"\"\n",
    "            SELECT DISTINCT\n",
    "                cuebiq_id,\n",
    "                start_lat,\n",
    "                start_lng,\n",
    "                end_lat,\n",
    "                end_lng,\n",
    "                duration_minutes,\n",
    "                length_meters,\n",
    "                number_of_points\n",
    "            FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "            WHERE \n",
    "                event_date = {event_date}\n",
    "                AND end_country = '{country_code}' \n",
    "                AND start_country = '{country_code}' \n",
    "            \"\"\"\n",
    "        )\n",
    "        logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "        \n",
    "        # Encode geohashes\n",
    "        try:\n",
    "            pe_tj_df['start_geohash5'] = pe_tj_df.apply(\n",
    "                lambda x: geohash.encode(x['start_lat'], x['start_lng'], precision=5) if pd.notnull(x['start_lat']) and pd.notnull(x['start_lng']) else None, axis=1)\n",
    "            pe_tj_df['end_geohash5'] = pe_tj_df.apply(\n",
    "                lambda x: geohash.encode(x['end_lat'], x['end_lng'], precision=5) if pd.notnull(x['end_lat']) and pd.notnull(x['end_lng']) else None, axis=1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error encoding geohashes for date {event_date}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Add user numbers to the aggregated data\n",
    "        try:\n",
    "            if not pe_tj_df.empty:\n",
    "                aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "                    'cuebiq_id': 'count',\n",
    "                    'duration_minutes': ['mean', 'median', 'std'],\n",
    "                    'length_meters': ['mean', 'median', 'std'],\n",
    "                    'number_of_points': ['mean', 'median', 'std']\n",
    "                }).reset_index()\n",
    "                aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                          'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                          'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                                          'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "            else:\n",
    "                logging.info(f\"No data to aggregate for date {event_date}\")\n",
    "                return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error aggregating data for date {event_date}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Filter aggregated data\n",
    "        try:\n",
    "            filtered_df5 = aggregated_df5.loc[aggregated_df5['trip_count'] > 9].copy()\n",
    "            filtered_df5['local_date'] = event_date\n",
    "            filtered_df5 = filtered_df5.astype({'trip_count': 'int'})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error filtering or assigning data for date {event_date}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return filtered_df5\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f918d8-b61c-4140-bd76-3d31e003141d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 22:18:16,921 - INFO - Executing SQL query for date 20190107\n",
      "2024-06-25 22:18:56,303 - INFO - Executing SQL query for date 20190131\n",
      "2024-06-25 22:19:35,295 - INFO - Executing SQL query for date 20190225\n",
      "2024-06-25 22:20:16,920 - INFO - Executing SQL query for date 20190301\n",
      "2024-06-25 22:20:54,602 - INFO - Executing SQL query for date 20190303\n",
      "2024-06-25 22:21:33,736 - INFO - Executing SQL query for date 20190312\n",
      "2024-06-25 22:22:19,809 - INFO - Executing SQL query for date 20190313\n",
      "2024-06-25 22:23:02,038 - INFO - Executing SQL query for date 20190315\n",
      "2024-06-25 22:23:39,676 - INFO - Executing SQL query for date 20190317\n",
      "2024-06-25 22:24:25,250 - INFO - Executing SQL query for date 20190331\n",
      "2024-06-25 22:24:59,839 - INFO - Executing SQL query for date 20190401\n",
      "2024-06-25 22:25:40,625 - INFO - Executing SQL query for date 20190410\n",
      "2024-06-25 22:26:21,936 - INFO - Executing SQL query for date 20190423\n",
      "2024-06-25 22:27:03,894 - INFO - Executing SQL query for date 20190430\n",
      "2024-06-25 22:27:46,174 - INFO - Executing SQL query for date 20190504\n",
      "2024-06-25 22:28:22,492 - INFO - Executing SQL query for date 20190518\n",
      "2024-06-25 22:29:06,162 - INFO - Executing SQL query for date 20190601\n",
      "2024-06-25 22:29:52,035 - INFO - Executing SQL query for date 20190611\n",
      "2024-06-25 22:30:30,065 - INFO - Executing SQL query for date 20190616\n",
      "2024-06-25 22:31:11,576 - INFO - Executing SQL query for date 20190624\n",
      "2024-06-25 22:31:49,604 - INFO - Executing SQL query for date 20190625\n",
      "2024-06-25 22:32:27,978 - INFO - Executing SQL query for date 20190630\n",
      "2024-06-25 22:33:10,547 - INFO - Executing SQL query for date 20190701\n",
      "2024-06-25 22:34:07,353 - INFO - Executing SQL query for date 20190720\n",
      "2024-06-25 22:34:57,101 - INFO - Executing SQL query for date 20190806\n",
      "2024-06-25 22:35:43,354 - INFO - Executing SQL query for date 20190808\n",
      "2024-06-25 22:36:22,127 - INFO - Executing SQL query for date 20190824\n",
      "2024-06-25 22:37:05,853 - INFO - Executing SQL query for date 20190902\n",
      "2024-06-25 22:37:44,414 - INFO - Executing SQL query for date 20190912\n",
      "2024-06-25 22:38:22,324 - INFO - Executing SQL query for date 20191010\n",
      "2024-06-25 22:38:51,789 - INFO - Executing SQL query for date 20191013\n",
      "2024-06-25 22:39:31,135 - INFO - Executing SQL query for date 20191015\n",
      "2024-06-25 22:40:01,152 - INFO - Executing SQL query for date 20191103\n",
      "2024-06-25 22:40:34,534 - INFO - Executing SQL query for date 20191106\n",
      "2024-06-25 22:41:03,281 - INFO - Executing SQL query for date 20191110\n",
      "2024-06-25 22:41:35,174 - INFO - Executing SQL query for date 20191117\n",
      "2024-06-25 22:42:01,884 - INFO - Executing SQL query for date 20191125\n",
      "2024-06-25 22:42:40,689 - INFO - Executing SQL query for date 20191208\n",
      "2024-06-25 22:43:08,085 - INFO - Executing SQL query for date 20191221\n",
      "2024-06-25 22:43:37,570 - INFO - Executing SQL query for date 20191227\n",
      "2024-06-25 22:44:00,408 - INFO - Executing SQL query for date 20191231\n"
     ]
    }
   ],
   "source": [
    "# List of specific dates to process\n",
    "date_list = [\n",
    "    '20190107', '20190131', '20190225', '20190301', '20190303', '20190312', \n",
    "    '20190313', '20190315', '20190317', '20190331', '20190401', '20190410', \n",
    "    '20190423', '20190430', '20190504', '20190518', '20190601', '20190611', \n",
    "    '20190616', '20190624', '20190625', '20190630', '20190701', '20190720', \n",
    "    '20190806', '20190808', '20190824', '20190902', '20190912', '20191010', \n",
    "    '20191013', '20191015', '20191103', '20191106', '20191110', '20191117', \n",
    "    '20191125', '20191208', '20191221', '20191227', '20191231'\n",
    "]\n",
    "\n",
    "country_code = 'MX'\n",
    "all_days_df = pd.DataFrame()\n",
    "\n",
    "# Process each day and concatenate the results\n",
    "for event_date in date_list:\n",
    "    filtered_df5 = process_day(event_date, country_code, sql_engine)\n",
    "    if not filtered_df5.empty:\n",
    "        all_days_df = pd.concat([all_days_df, filtered_df5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66710068-d5dc-4a1b-bc5f-708d8b81a6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_geohash5</th>\n",
       "      <th>end_geohash5</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>m_duration_min</th>\n",
       "      <th>mdn_duration_min</th>\n",
       "      <th>sd_duration_min</th>\n",
       "      <th>m_length_m</th>\n",
       "      <th>mdn_length_m</th>\n",
       "      <th>sd_length_m</th>\n",
       "      <th>m_points_no</th>\n",
       "      <th>mdn_points_no</th>\n",
       "      <th>sd_points_no</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9emmx</td>\n",
       "      <td>9emmx</td>\n",
       "      <td>13</td>\n",
       "      <td>57.775641</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>91.727256</td>\n",
       "      <td>548.178805</td>\n",
       "      <td>427.810485</td>\n",
       "      <td>626.120861</td>\n",
       "      <td>3.230769</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.423250</td>\n",
       "      <td>20190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9emt1</td>\n",
       "      <td>9emt1</td>\n",
       "      <td>11</td>\n",
       "      <td>53.127273</td>\n",
       "      <td>12.383333</td>\n",
       "      <td>69.829574</td>\n",
       "      <td>2519.050390</td>\n",
       "      <td>327.971030</td>\n",
       "      <td>6568.315633</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.769969</td>\n",
       "      <td>20190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9emt3</td>\n",
       "      <td>9emt3</td>\n",
       "      <td>105</td>\n",
       "      <td>64.662222</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>91.496411</td>\n",
       "      <td>2268.257680</td>\n",
       "      <td>956.767012</td>\n",
       "      <td>4366.371742</td>\n",
       "      <td>6.180952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.967510</td>\n",
       "      <td>20190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>9emtk</td>\n",
       "      <td>9emtk</td>\n",
       "      <td>58</td>\n",
       "      <td>40.962356</td>\n",
       "      <td>20.583333</td>\n",
       "      <td>60.703146</td>\n",
       "      <td>1211.532874</td>\n",
       "      <td>586.470368</td>\n",
       "      <td>1773.177908</td>\n",
       "      <td>4.431034</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.878304</td>\n",
       "      <td>20190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>9emu4</td>\n",
       "      <td>9emu4</td>\n",
       "      <td>14</td>\n",
       "      <td>43.578571</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>42.458430</td>\n",
       "      <td>1458.265822</td>\n",
       "      <td>437.202134</td>\n",
       "      <td>2764.783262</td>\n",
       "      <td>5.785714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.724732</td>\n",
       "      <td>20190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33371</th>\n",
       "      <td>d5f2b</td>\n",
       "      <td>d5f2b</td>\n",
       "      <td>166</td>\n",
       "      <td>43.662149</td>\n",
       "      <td>18.058333</td>\n",
       "      <td>62.739475</td>\n",
       "      <td>478.750023</td>\n",
       "      <td>277.756082</td>\n",
       "      <td>1142.851284</td>\n",
       "      <td>5.662651</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.174947</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33376</th>\n",
       "      <td>d5f2c</td>\n",
       "      <td>d5f2f</td>\n",
       "      <td>10</td>\n",
       "      <td>39.635000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>47.332935</td>\n",
       "      <td>2997.426362</td>\n",
       "      <td>480.980488</td>\n",
       "      <td>7921.177226</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.636392</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33389</th>\n",
       "      <td>d5f2f</td>\n",
       "      <td>d5f2c</td>\n",
       "      <td>11</td>\n",
       "      <td>31.774242</td>\n",
       "      <td>12.083333</td>\n",
       "      <td>60.396571</td>\n",
       "      <td>1518.747992</td>\n",
       "      <td>528.967291</td>\n",
       "      <td>3384.258606</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.423371</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33391</th>\n",
       "      <td>d5f2f</td>\n",
       "      <td>d5f2f</td>\n",
       "      <td>13</td>\n",
       "      <td>38.432051</td>\n",
       "      <td>13.383333</td>\n",
       "      <td>55.416803</td>\n",
       "      <td>759.649485</td>\n",
       "      <td>195.201728</td>\n",
       "      <td>1278.907148</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.959123</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33394</th>\n",
       "      <td>d5f30</td>\n",
       "      <td>d5f30</td>\n",
       "      <td>62</td>\n",
       "      <td>32.530108</td>\n",
       "      <td>13.816667</td>\n",
       "      <td>45.217360</td>\n",
       "      <td>320.561244</td>\n",
       "      <td>203.692617</td>\n",
       "      <td>438.701004</td>\n",
       "      <td>6.096774</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.728528</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236743 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      start_geohash5 end_geohash5  trip_count  m_duration_min  \\\n",
       "6              9emmx        9emmx          13       57.775641   \n",
       "29             9emt1        9emt1          11       53.127273   \n",
       "39             9emt3        9emt3         105       64.662222   \n",
       "53             9emtk        9emtk          58       40.962356   \n",
       "62             9emu4        9emu4          14       43.578571   \n",
       "...              ...          ...         ...             ...   \n",
       "33371          d5f2b        d5f2b         166       43.662149   \n",
       "33376          d5f2c        d5f2f          10       39.635000   \n",
       "33389          d5f2f        d5f2c          11       31.774242   \n",
       "33391          d5f2f        d5f2f          13       38.432051   \n",
       "33394          d5f30        d5f30          62       32.530108   \n",
       "\n",
       "       mdn_duration_min  sd_duration_min   m_length_m  mdn_length_m  \\\n",
       "6             11.300000        91.727256   548.178805    427.810485   \n",
       "29            12.383333        69.829574  2519.050390    327.971030   \n",
       "39            28.100000        91.496411  2268.257680    956.767012   \n",
       "53            20.583333        60.703146  1211.532874    586.470368   \n",
       "62            28.000000        42.458430  1458.265822    437.202134   \n",
       "...                 ...              ...          ...           ...   \n",
       "33371         18.058333        62.739475   478.750023    277.756082   \n",
       "33376         18.700000        47.332935  2997.426362    480.980488   \n",
       "33389         12.083333        60.396571  1518.747992    528.967291   \n",
       "33391         13.383333        55.416803   759.649485    195.201728   \n",
       "33394         13.816667        45.217360   320.561244    203.692617   \n",
       "\n",
       "       sd_length_m  m_points_no  mdn_points_no  sd_points_no local_date  \n",
       "6       626.120861     3.230769            3.0      1.423250   20190107  \n",
       "29     6568.315633     4.454545            3.0      2.769969   20190107  \n",
       "39     4366.371742     6.180952            4.0      5.967510   20190107  \n",
       "53     1773.177908     4.431034            3.0      2.878304   20190107  \n",
       "62     2764.783262     5.785714            5.0      3.724732   20190107  \n",
       "...            ...          ...            ...           ...        ...  \n",
       "33371  1142.851284     5.662651            4.0     11.174947   20191231  \n",
       "33376  7921.177226     4.300000            4.0      1.636392   20191231  \n",
       "33389  3384.258606     4.454545            3.0      2.423371   20191231  \n",
       "33391  1278.907148     4.384615            4.0      2.959123   20191231  \n",
       "33394   438.701004     6.096774            4.0      8.728528   20191231  \n",
       "\n",
       "[236743 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570a39b4-4e5d-4fec-8f6b-17cedaa74efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days_df.to_csv('/home/jovyan/Data/TJ/' + 'MX_MissingDates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8083f-bb6f-4ca6-8008-3861ebd079b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ab54-7ea9-4f44-8d58-48a9c5ed510f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9950b-f813-4496-95ee-8a850ab07181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2f14e-fa1b-4810-a38d-e529d81569d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf17b0-d474-4e16-80a7-0538ca8d1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e4671-e3f7-4af4-92ec-49e7a9c8235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9771058-c43d-4cf3-97eb-3f91d2ca61ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d96e-8311-4268-9172-a6fa7f5ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe55a9-be17-4357-862e-c09c797397e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df0a19-7477-4067-adef-72aa1d701faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb6f36-52ec-4aca-86d5-1fec273ea6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5ae12-830c-496c-be73-f527048603c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff191f4-b11e-4639-9c4e-502decf49696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d565d9-6119-467c-bef4-082acc505204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca5d0d-a0f6-43b0-af28-3d06ec9a23b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a779f-0b84-40f8-a379-acf4490e1a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
