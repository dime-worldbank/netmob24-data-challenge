{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from geohash2) (0.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import geohash2\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrinoEngine:\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query: str):\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 100000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b65f443-7d5a-4f9e-94b7-420318f85d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e32822-d9c8-4f95-83f1-54a460dcb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'IN'\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0d4de4-b34d-4a3b-8da2-8be107e4996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d216eb-cb17-45ac-88c2-66b7745deafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and aggregate data\n",
    "def process_and_aggregate_data(date, country_code, con):\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    lookback_date = date - timedelta(days=1)\n",
    "    lookahead_date = date + timedelta(days=35)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_current_date = date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Executing SQL query for date {formatted_current_date}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        pe_dl_table_gen = pd.read_sql_query(query, con, chunksize=100000)  # Adjust chunksize as needed\n",
    "        \n",
    "        for chunk in pe_dl_table_gen:\n",
    "            # Calculate geohashes\n",
    "            chunk['geohash5'] = chunk.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            chunk['geohash3'] = chunk.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = chunk.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = chunk.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25aee43-44f7-4380-ab5a-3e541c4d6c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:22:03,997 - INFO - Executing SQL query for date 20190101: \n",
      "    SELECT \n",
      "        cuebiq_id, \n",
      "        event_zoned_datetime, \n",
      "        processing_date,\n",
      "        lat,\n",
      "        lng\n",
      "    FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "    WHERE \n",
      "        processing_date BETWEEN 20181231 AND 20190205\n",
      "        AND country_code = 'IN' \n",
      "        AND event_zoned_datetime IS NOT NULL\n",
      "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
      "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('20190101', '%Y%m%d')\n",
      "    \n",
      "2024-06-06 01:22:04,579 - INFO - failed after 3 attempts\n",
      "2024-06-06 01:22:04,580 - ERROR - Error while processing data for date 20190101: error 502: bad gateway\n",
      "2024-06-06 01:22:04,581 - INFO - Executing SQL query for date 20190102: \n",
      "    SELECT \n",
      "        cuebiq_id, \n",
      "        event_zoned_datetime, \n",
      "        processing_date,\n",
      "        lat,\n",
      "        lng\n",
      "    FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "    WHERE \n",
      "        processing_date BETWEEN 20190101 AND 20190206\n",
      "        AND country_code = 'IN' \n",
      "        AND event_zoned_datetime IS NOT NULL\n",
      "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
      "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('20190102', '%Y%m%d')\n",
      "    \n",
      "2024-06-06 01:22:05,057 - INFO - failed after 3 attempts\n",
      "2024-06-06 01:22:05,058 - ERROR - Error while processing data for date 20190102: error 502: bad gateway\n",
      "2024-06-06 01:22:05,059 - INFO - Executing SQL query for date 20190103: \n",
      "    SELECT \n",
      "        cuebiq_id, \n",
      "        event_zoned_datetime, \n",
      "        processing_date,\n",
      "        lat,\n",
      "        lng\n",
      "    FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "    WHERE \n",
      "        processing_date BETWEEN 20190102 AND 20190207\n",
      "        AND country_code = 'IN' \n",
      "        AND event_zoned_datetime IS NOT NULL\n",
      "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
      "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('20190103', '%Y%m%d')\n",
      "    \n",
      "2024-06-06 01:22:05,638 - INFO - failed after 3 attempts\n",
      "2024-06-06 01:22:05,639 - ERROR - Error while processing data for date 20190103: error 502: bad gateway\n",
      "2024-06-06 01:22:05,640 - INFO - Data extraction, aggregation, and saving completed.\n",
      "2024-06-06 01:22:05,641 - INFO - Total time taken: 419.3414087295532 seconds\n"
     ]
    }
   ],
   "source": [
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    process_and_aggregate_data(current_date, country_code, con)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfa363-e2b0-403d-a809-e3d50b1f2716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce49dc4-6269-443f-9130-6d9a371d1db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7fab2-fb44-41e6-b131-dc04292478b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50916e84-52a0-4694-b662-0f4b554c60dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75f6cc7c-3ecd-48b1-b2dd-c9c2b1638704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with one day\n",
    "specific_date = 20190105\n",
    "specific_date_dt = datetime.strptime(str(specific_date), '%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f8d5292-d8b1-49bf-9645-48c034fc8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "445aea15-f9b0-4b77-a380-ff65467d9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and aggregate data for a single day\n",
    "def process_and_aggregate_data(date, country_code, con):\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    lookback_date = date - timedelta(days=1)\n",
    "    lookahead_date = date + timedelta(days=35)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_current_date = date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Executing SQL query for date {formatted_current_date}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        pe_dl_table_gen = pd.read_sql_query(query, con, chunksize=1000)  # Adjust chunksize as needed\n",
    "        \n",
    "        for chunk in pe_dl_table_gen:\n",
    "            # Calculate geohashes\n",
    "            chunk['geohash5'] = chunk.apply(lambda row: geohash.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            chunk['geohash3'] = chunk.apply(lambda row: geohash.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = chunk.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = chunk.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c51179e-9df9-422c-86cb-a2679a61acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:24:50,658 - INFO - Executing SQL query for date 20191015: \n",
      "    SELECT \n",
      "        cuebiq_id, \n",
      "        event_zoned_datetime, \n",
      "        processing_date,\n",
      "        lat,\n",
      "        lng\n",
      "    FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "    WHERE \n",
      "        processing_date BETWEEN 20191014 AND 20191119\n",
      "        AND country_code = 'IN' \n",
      "        AND event_zoned_datetime IS NOT NULL\n",
      "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
      "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('20191015', '%Y%m%d')\n",
      "    \n",
      "2024-06-06 01:24:51,031 - INFO - failed after 3 attempts\n",
      "2024-06-06 01:24:51,032 - ERROR - Error while processing data for date 20191015: error 502: bad gateway\n"
     ]
    }
   ],
   "source": [
    "# Process data for the specific date\n",
    "process_and_aggregate_data(specific_date_dt, country_code, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2e54b-598a-4ca7-870e-1af076ded36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3201ae-4cd2-4daa-b964-ac54c0806f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96903832-669e-4eb7-a1ce-3cdffe40b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# Define the input parameters for a single day\n",
    "country_code = 'IN'\n",
    "specific_date = 20190105\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer date to datetime object\n",
    "specific_date_dt = datetime.strptime(str(specific_date), '%Y%m%d')\n",
    "\n",
    "# Calculate the lookback and lookahead dates\n",
    "lookback_date = specific_date_dt - timedelta(days=1)\n",
    "lookahead_date = specific_date_dt + timedelta(days=35)\n",
    "\n",
    "# Format dates for the SQL query\n",
    "formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "formatted_current_date = specific_date_dt.strftime('%Y%m%d')\n",
    "formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ff976b8-ec57-4bd4-a3ba-ba0ea584093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:31:12,842 - INFO - Executing SQL query for date 20190105: \n",
      "SELECT \n",
      "    cuebiq_id, \n",
      "    event_zoned_datetime, \n",
      "    processing_date,\n",
      "    lat,\n",
      "    lng\n",
      "FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "WHERE \n",
      "    processing_date BETWEEN 20190104 AND 20190209\n",
      "    AND country_code = 'IN' \n",
      "    AND event_zoned_datetime IS NOT NULL\n",
      "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
      "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('20190105', '%Y%m%d')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct the SQL query\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "\"\"\"\n",
    "\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}: {query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d5582c-f8f5-45e6-965e-501cbe6c8a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:35:10,568 - INFO - Retrieved 12571991 records for date 20190105\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pe_dl_table_gen = pd.read_sql_query(query, con, chunksize=1000)  # Adjust chunksize as needed\n",
    "    chunks = [chunk for chunk in pe_dl_table_gen]  # Collect all chunks\n",
    "    if chunks:\n",
    "        pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "        logging.info(f\"Retrieved {len(pe_dl_table_df)} records for date {formatted_current_date}\")\n",
    "    else:\n",
    "        pe_dl_table_df = pd.DataFrame()\n",
    "        logging.info(f\"No data retrieved for date {formatted_current_date}\")\n",
    "except exc.SQLAlchemyError as e:\n",
    "    logging.error(f\"Error executing query for date {formatted_current_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69713793-6bf9-41d0-b0d3-9a59d4764a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:35:10,574 - INFO -     cuebiq_id       event_zoned_datetime  processing_date        lat  \\\n",
      "0  1708989676  2019-01-05T11:07:02+05:30         20190105  28.414726   \n",
      "1  1708989676  2019-01-05T07:48:52+05:30         20190105  28.414726   \n",
      "2  1708989676  2019-01-05T07:55:01+05:30         20190105  28.414726   \n",
      "3  1708989676  2019-01-05T07:55:37+05:30         20190105  28.414726   \n",
      "4  1708989676  2019-01-05T08:21:12+05:30         20190105  28.414726   \n",
      "\n",
      "         lng  \n",
      "0  77.312957  \n",
      "1  77.312957  \n",
      "2  77.312957  \n",
      "3  77.312957  \n",
      "4  77.312957  \n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows of the retrieved DataFrame\n",
    "if not pe_dl_table_df.empty:\n",
    "    logging.info(pe_dl_table_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950b568-e05b-45d0-9d86-ae4fd0ce6cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72be17-1620-47d2-a134-9398d8943351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185ae43-d905-4ce6-818a-458447d5bdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661eee0-4dee-4caf-9cb6-d6a48c0cdb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345545d-cd78-4eb5-aa61-100e4f667b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a652919-d7d1-4268-a8fc-55c2e1433ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
