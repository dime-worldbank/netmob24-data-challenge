{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff9e4df-146c-4120-a8f9-220b7d551c96",
   "metadata": {},
   "source": [
    "Download dl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5aa75d-6b26-48f8-b783-44b0e42e4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8071db6-5131-4ce1-96d4-2f3bb05c2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4658058-9886-417f-b56c-8b09190e1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "\n",
    "# dl_table = f\"{schema_name['cda']}.device_location\"  \n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "tj_table = f\"{schema_name['cda']}.trajectory\"     \n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\"\n",
    "\n",
    "# stop_table = f\"{schema['cda']}.stop\" \n",
    "pe_stop_table = f\"{schema_name['cda']}.stop_uplevelled\"\n",
    "\n",
    "visit_table = f\"{schema_name['cda']}.visit \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccbd66-ded9-406b-bd6b-27773cd5ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3259a4e-bddb-486e-b55f-050519bb9085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ecb71-8f95-4e8d-a032-1f3e66d05f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dba29-d5fc-4753-9789-811f6c08fd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4300f4dd-fb7d-4c71-847d-02cf287f07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrinoEngine:\n",
    "    def __init__(self):\n",
    "        self.conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = self.conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query: str):\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0390c906-4555-4941-910b-6c6f86ba523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: MX\n",
      "Processing date: 20190129 for country: MX\n",
      "Processing date: 20190130 for country: MX\n",
      "Processing date: 20190131 for country: MX\n",
      "Finished processing country: MX\n",
      "Total time taken: 1595.3800954818726 seconds\n"
     ]
    }
   ],
   "source": [
    "countries = ['MX']\n",
    "start_date = 20190129\n",
    "end_date = 20190131\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for country_code in countries:\n",
    "    print(f\"Processing country: {country_code}\")\n",
    "    current_date = start_date_dt\n",
    "    while current_date <= end_date_dt:\n",
    "        formatted_date = current_date.strftime('%Y%m%d')\n",
    "        next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                timezoneoffset_secs,\n",
    "                lat,\n",
    "                lng, \n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
    "                interval '1' second * timezoneoffset_secs) AS event_datetime_utc\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date = {formatted_date} \n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "        \"\"\"\n",
    "\n",
    "        for chunk in sql_engine.read_sql_chunked(query):\n",
    "            # Extract event_zoned_date from event_zoned_datetime\n",
    "            chunk['event_zoned_date'] = chunk['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "\n",
    "            # Convert processing_date and event_zoned_date to datetime objects\n",
    "            chunk['event_zoned_date'] = pd.to_datetime(chunk['event_zoned_date'])\n",
    "            processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "\n",
    "            # Filter data based on event_zoned_date\n",
    "            chunk = chunk[(chunk['event_zoned_date'] >= start_date_dt.strftime('%Y-%m-%d')) & \n",
    "                          (chunk['event_zoned_date'] <= processing_date_dt.strftime('%Y-%m-%d'))]\n",
    "\n",
    "            # Group by event_zoned_date\n",
    "            grouped = chunk.groupby('event_zoned_date')\n",
    "\n",
    "            for event_zoned_date, group_df in grouped:\n",
    "                # Remove duplicates\n",
    "                group_df = group_df.drop_duplicates()\n",
    "\n",
    "                # Convert event_zoned_date to string and remove hyphens\n",
    "                event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "                file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_dl.csv'\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    # If the file exists, append the new data\n",
    "                    group_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    # If the file does not exist, create it\n",
    "                    try:\n",
    "                        group_df.to_csv(file_path, index=False)\n",
    "                    except OSError as e:\n",
    "                        if not os.path.exists(os.path.dirname(file_path)):\n",
    "                            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                            group_df.to_csv(file_path, index=False)\n",
    "                        else:\n",
    "                            raise e\n",
    "\n",
    "            # Delete the DataFrame to free up memory\n",
    "            del chunk\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "\n",
    "# Processing date: 20190114 for country: IN\n",
    "# Processing date: 20190430 for country: ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ed202-4740-4553-8727-aac788b252cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108dc33-0202-4eab-8542-4bf08b3a3a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436063ba-b127-43eb-8820-f114e73f57da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb0731-d516-49ac-9d2c-974c8a2396d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73bc0154-231c-42c9-bf6d-653b1b371bcd",
   "metadata": {},
   "source": [
    "# Some how 有点毛病"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9736ef-20ae-4b92-9939-81c15fa10a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "\n",
    "# countries = ['IN']\n",
    "# start_date = 20190101\n",
    "# end_date = 20190131\n",
    "\n",
    "# # Convert integer dates to datetime objects\n",
    "# start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "# end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# for country_code in countries:\n",
    "#     print(f\"Processing country: {country_code}\")\n",
    "#     current_date = start_date_dt\n",
    "#     while current_date <= end_date_dt:\n",
    "#         formatted_date = current_date.strftime('%Y%m%d')\n",
    "#         next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "#         print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "#         query = f\"\"\"\n",
    "#             SELECT \n",
    "#                 cuebiq_id, \n",
    "#                 event_zoned_datetime, \n",
    "#                 processing_date,\n",
    "#                 timezoneoffset_secs,\n",
    "#                 lat,\n",
    "#                 lng, \n",
    "#                 TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
    "#                 interval '1' second * timezoneoffset_secs) AS event_datetime_utc\n",
    "#             FROM {pe_dl_table}\n",
    "#             WHERE \n",
    "#                 processing_date = {formatted_date} \n",
    "#                 AND country_code = '{country_code}' \n",
    "#                 AND event_zoned_datetime IS NOT NULL\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "#         \"\"\"\n",
    "\n",
    "#         pe_dl_table_df = sql_engine.read_sql(query)\n",
    "\n",
    "#         # Extract event_zoned_date from event_zoned_datetime\n",
    "#         pe_dl_table_df['event_zoned_date'] = pe_dl_table_df['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "        \n",
    "#         # Convert processing_date and event_zoned_date to datetime objects\n",
    "#         pe_dl_table_df['event_zoned_date'] = pd.to_datetime(pe_dl_table_df['event_zoned_date'])\n",
    "#         processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "        \n",
    "#         # Filter data based on event_zoned_date\n",
    "#         pe_dl_table_df = pe_dl_table_df[(pe_dl_table_df['event_zoned_date'] >= start_date_dt.strftime('%Y-%m-%d')) & \n",
    "#                                         (pe_dl_table_df['event_zoned_date'] <= processing_date_dt.strftime('%Y-%m-%d'))]\n",
    "\n",
    "#         # Group by event_zoned_date\n",
    "#         grouped = pe_dl_table_df.groupby('event_zoned_date')\n",
    "\n",
    "#         for event_zoned_date, group_df in grouped:\n",
    "#             # Remove duplicates\n",
    "#             group_df = group_df.drop_duplicates()\n",
    "\n",
    "#             # Convert event_zoned_date to string and remove hyphens\n",
    "#             event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "#             file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_dl.csv'\n",
    "\n",
    "#             if os.path.exists(file_path):\n",
    "#                 # If the file exists, append the new data\n",
    "#                 group_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "#             else:\n",
    "#                 # If the file does not exist, create it\n",
    "#                 try:\n",
    "#                     group_df.to_csv(file_path, index=False)\n",
    "#                 except OSError as e:\n",
    "#                     if not os.path.exists(os.path.dirname(file_path)):\n",
    "#                         os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#                         group_df.to_csv(file_path, index=False)\n",
    "#                     else:\n",
    "#                         raise e\n",
    "\n",
    "#         # Delete the DataFrame to free up memory\n",
    "#         del pe_dl_table_df\n",
    "#         # Force garbage collection\n",
    "#         gc.collect()\n",
    "\n",
    "#         current_date += timedelta(days=1)\n",
    "\n",
    "#     print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# # End timing\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate and print the total time taken\n",
    "# total_time = end_time - start_time\n",
    "# print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# # Processing date: 20190430 for country: ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468bb634-eb8c-4608-aef9-61ce2d915df4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: ID\n",
      "Processing date: 20190329 for country: ID\n",
      "Error processing date 20190329 for country ID: (trino.exceptions.TrinoUserError) TrinoUserError(type=USER_ERROR, name=INVALID_FUNCTION_ARGUMENT, message=\"Invalid format: \"+190552-01-08T12:50\" is malformed at \"52-01-08T12:50\"\", query_id=20240522_131146_00013_zcyzu)\n",
      "[SQL: \n",
      "            SELECT \n",
      "                cuebiq_id, \n",
      "                event_zoned_datetime, \n",
      "                processing_date,\n",
      "                timezoneoffset_secs,\n",
      "                lat,\n",
      "                lng, \n",
      "                \n",
      "                date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
      "                interval '1' second * timezoneoffset_secs AS event_datetime_utc\n",
      "                \n",
      "            FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
      "            WHERE \n",
      "                processing_date = 20190329 \n",
      "                AND country_code = 'ID' \n",
      "                AND event_zoned_datetime IS NOT NULL\n",
      "        ]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n",
      "Processing date: 20190330 for country: ID\n",
      "Processing date: 20190331 for country: ID\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 45\u001b[0m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m        cuebiq_id, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m        AND event_zoned_datetime IS NOT NULL\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     pe_dl_table_df \u001b[38;5;241m=\u001b[39m \u001b[43msql_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for country \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [2], line 28\u001b[0m, in \u001b[0;36mTrinoEngine.read_sql\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_sql\u001b[39m(\u001b[38;5;28mself\u001b[39m, query:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame: \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Select and insert into operations.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/sql.py:590\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    582\u001b[0m         sql,\n\u001b[1;32m    583\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/sql.py:1574\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[1;32m   1565\u001b[0m         result,\n\u001b[1;32m   1566\u001b[0m         chunksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[1;32m   1576\u001b[0m         data,\n\u001b[1;32m   1577\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1582\u001b[0m     )\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/result.py:1072\u001b[0m, in \u001b[0;36mResult.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetchall\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;124;03m\"\"\"A synonym for the :meth:`_engine.Result.all` method.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/result.py:401\u001b[0m, in \u001b[0;36mResultInternal._allrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m post_creational_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_creational_filter\n\u001b[1;32m    399\u001b[0m make_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_row_getter\n\u001b[0;32m--> 401\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_row:\n\u001b[1;32m    403\u001b[0m     made_rows \u001b[38;5;241m=\u001b[39m [make_row(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/cursor.py:1819\u001b[0m, in \u001b[0;36mCursorResult._fetchall_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/cursor.py:995\u001b[0m, in \u001b[0;36mCursorFetchStrategy.fetchall\u001b[0;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rows\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 995\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbapi_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/cursor.py:955\u001b[0m, in \u001b[0;36mCursorFetchStrategy.handle_exception\u001b[0;34m(self, result, dbapi_cursor, err)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_exception\u001b[39m(\u001b[38;5;28mself\u001b[39m, result, dbapi_cursor, err):\n\u001b[0;32m--> 955\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbapi_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2128\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   2125\u001b[0m             sqlalchemy_exception, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2128\u001b[0m         \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/util/compat.py:211\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    208\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/sqlalchemy/engine/cursor.py:991\u001b[0m, in \u001b[0;36mCursorFetchStrategy.fetchall\u001b[0;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetchall\u001b[39m(\u001b[38;5;28mself\u001b[39m, result, dbapi_cursor):\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m         rows \u001b[38;5;241m=\u001b[39m \u001b[43mdbapi_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m         result\u001b[38;5;241m.\u001b[39m_soft_close()\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rows\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/dbapi.py:608\u001b[0m, in \u001b[0;36mCursor.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetchall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[Any]]:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:720\u001b[0m, in \u001b[0;36mTrinoResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# A query only transitions to a FINISHED state when the results are fully consumed:\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;66;03m# The reception of the data is acknowledged by calling the next_uri before exposing the data through dbapi.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query\u001b[38;5;241m.\u001b[39mfinished \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         next_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query\u001b[38;5;241m.\u001b[39mfinished \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows:\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rownumber \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:847\u001b[0m, in \u001b[0;36mTrinoQuery.fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_row_mapper:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_row_mapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:1228\u001b[0m, in \u001b[0;36mRowMapper.map\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rows\n\u001b[0;32m-> 1228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_row(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:1228\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rows\n\u001b[0;32m-> 1228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:1231\u001b[0m, in \u001b[0;36mRowMapper._map_row\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[0;32m-> 1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_value(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[index]) \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row)]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:1231\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[0;32m-> 1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row)]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:1235\u001b[0m, in \u001b[0;36mRowMapper._map_value\u001b[0;34m(self, value, value_mapper)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, value_mapper: ValueMapper[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[T]:\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_mapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1237\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m into the associated python type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/trino/client.py:931\u001b[0m, in \u001b[0;36mDoubleValueMapper.map\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfinity\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Infinity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m:\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "\n",
    "# countries = ['ID']\n",
    "# start_date = 20190329\n",
    "# end_date = 20191231\n",
    "\n",
    "# # Convert integer dates to datetime objects\n",
    "# start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "# end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# for country_code in countries:\n",
    "#     print(f\"Processing country: {country_code}\")\n",
    "#     current_date = start_date_dt\n",
    "#     while current_date <= end_date_dt:\n",
    "#         formatted_date = current_date.strftime('%Y%m%d')\n",
    "#         print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "#         query = f\"\"\"\n",
    "#             SELECT \n",
    "#                 cuebiq_id, \n",
    "#                 event_zoned_datetime, \n",
    "#                 processing_date,\n",
    "#                 timezoneoffset_secs,\n",
    "#                 lat,\n",
    "#                 lng, \n",
    "                \n",
    "#                 date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
    "#                 interval '1' second * timezoneoffset_secs AS event_datetime_utc\n",
    "                \n",
    "#             FROM cuebiq.paas_cda_pe_v3.device_location_uplevelled\n",
    "#             WHERE \n",
    "#                 processing_date = {formatted_date} \n",
    "#                 AND country_code = '{country_code}' \n",
    "#                 AND event_zoned_datetime IS NOT NULL\n",
    "#         \"\"\"\n",
    "\n",
    "#         try:\n",
    "#             pe_dl_table_df = sql_engine.read_sql(query)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing date {formatted_date} for country {country_code}: {e}\")\n",
    "#             current_date += timedelta(days=1)\n",
    "#             continue\n",
    "\n",
    "#         # Extract event_zoned_date from event_zoned_datetime\n",
    "#         pe_dl_table_df['event_zoned_date'] = pe_dl_table_df['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "\n",
    "#         # Remove rows with invalid event_zoned_datetime formats\n",
    "#         pe_dl_table_df = pe_dl_table_df.dropna(subset=['event_zoned_date'])\n",
    "\n",
    "#         # Convert processing_date and event_zoned_date to datetime objects\n",
    "#         pe_dl_table_df['event_zoned_date'] = pd.to_datetime(pe_dl_table_df['event_zoned_date'], errors='coerce')\n",
    "\n",
    "#         # Remove rows with invalid dates\n",
    "#         pe_dl_table_df = pe_dl_table_df.dropna(subset=['event_zoned_date'])\n",
    "\n",
    "#         processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "\n",
    "#         # Filter data based on event_zoned_date\n",
    "#         pe_dl_table_df = pe_dl_table_df[pe_dl_table_df['event_zoned_date'] <= processing_date_dt]\n",
    "\n",
    "#         # Group by event_zoned_date\n",
    "#         grouped = pe_dl_table_df.groupby(pe_dl_table_df['event_zoned_date'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "#         for event_zoned_date, group_df in grouped:\n",
    "#             file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date.replace(\"-\", \"\")}_{country_code}_pe_dl.csv'\n",
    "\n",
    "#             if os.path.exists(file_path):\n",
    "#                 # If the file exists, append the new data\n",
    "#                 group_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "#             else:\n",
    "#                 # If the file does not exist, create it\n",
    "#                 try:\n",
    "#                     group_df.to_csv(file_path, index=False)\n",
    "#                 except OSError as e:\n",
    "#                     if not os.path.exists(os.path.dirname(file_path)):\n",
    "#                         os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#                         group_df.to_csv(file_path, index=False)\n",
    "#                     else:\n",
    "#                         raise e\n",
    "\n",
    "#         # Delete the DataFrame to free up memory\n",
    "#         del pe_dl_table_df\n",
    "#         # Force garbage collection\n",
    "#         gc.collect()\n",
    "\n",
    "#         current_date += timedelta(days=1)\n",
    "\n",
    "#     print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# # End timing\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate and print the total time taken\n",
    "# total_time = end_time - start_time\n",
    "# print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "\n",
    "# # Processing date: 20190329 for country: ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c649a0a-8c9a-4935-bf89-e7d7365f520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e518c37-7b3a-4a1a-8db5-ee3d4664aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 466.7766478061676 seconds\n"
     ]
    }
   ],
   "source": [
    "# country_code = 'ID'\n",
    "# start_date = 20190101\n",
    "# end_date = 20190102\n",
    "\n",
    "# # Convert integer dates to datetime objects\n",
    "# start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "# end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# current_date = start_date_dt\n",
    "# while current_date <= end_date_dt:\n",
    "#     formatted_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "#     query = f\"\"\"\n",
    "#         SELECT \n",
    "#             -- *,\n",
    "#             cuebiq_id, \n",
    "#             event_zoned_datetime, \n",
    "#             processing_date,\n",
    "#             timezoneoffset_secs,\n",
    "#             lat,\n",
    "#             lng, \n",
    "#             speed_ms, \n",
    "            \n",
    "#             -- Extract the date and time part and adjust by the timezone offset \n",
    "#             date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
    "#             interval '1' second * timezoneoffset_secs AS event_datetime_utc\n",
    "            \n",
    "#         FROM {pe_dl_table}\n",
    "#         WHERE \n",
    "#             processing_date = {formatted_date} \n",
    "#             AND country_code = '{country_code}' \n",
    "#             AND event_zoned_datetime IS NOT NULL\n",
    "#     \"\"\"\n",
    "    \n",
    "#     pe_dl_table_df = sql_engine.read_sql(query)\n",
    "    \n",
    "#     file_path = f'/home/jovyan/Data/DL/{country_code}/{formatted_date}_{country_code}_pe_dl.csv'\n",
    "\n",
    "#     try:\n",
    "#         # Attempt to save the DataFrame to the CSV file\n",
    "#         pe_dl_table_df.to_csv(file_path, index=False)\n",
    "#     except OSError as e:\n",
    "#         # If there is an OSError (e.g., directory does not exist), create the directory\n",
    "#         if not os.path.exists(os.path.dirname(file_path)):\n",
    "#             os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#             # Retry saving the DataFrame to the CSV file\n",
    "#             pe_dl_table_df.to_csv(file_path, index=False)\n",
    "#         else:\n",
    "#             # If the error is not related to the directory not existing, re-raise the exception\n",
    "#             raise e\n",
    "        \n",
    "#     # pe_dl_table_df.to_csv(f'/home/jovyan/Data/DL/{country_code}/{formatted_date}_{country_code}_pe_dl.csv', index=False)\n",
    "    \n",
    "#     # Delete the DataFrame to free up memory\n",
    "#     del pe_dl_table_df\n",
    "#     # Force garbage collection\n",
    "#     gc.collect()\n",
    "    \n",
    "#     current_date += timedelta(days=1)\n",
    "\n",
    "# # End timing\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate and print the total time taken\n",
    "# total_time = end_time - start_time\n",
    "# print(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a534b-6da5-4c3a-98e8-d0a4104b5a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfe27c-2e94-4f9b-b468-5b0e9781a262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9162fde-690a-480c-8238-040bd905d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# country_code = 'MX'\n",
    "# start_date = 20190101\n",
    "# end_date = 20190131\n",
    "\n",
    "# # Convert integer dates to datetime objects\n",
    "# start_date = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "# end_date = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# # Iterate through each day\n",
    "# current_date = start_date\n",
    "# while current_date <= end_date:\n",
    "#     print(current_date.strftime('%Y%m%d'))\n",
    "#     current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8205f0-0500-4fcf-9070-edb42522616c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1ace2-e79e-4bc2-a0a7-a9614bbbf02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f8902-2bf8-4668-ab34-7264eeed1ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837c139-d597-413d-b580-432ae9119b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
