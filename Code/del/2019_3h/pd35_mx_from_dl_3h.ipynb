{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d289ed9b-b5c6-42f8-9b8a-8d2653a94f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7304cfd6-5f4e-4fd1-9fcb-e4f799127977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: @cuebiq/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ccb1e3-ccad-4833-b2fd-3646480078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geohash2\n",
      "  Using cached geohash2-1.1-py3-none-any.whl\n",
      "Collecting docutils>=0.3\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Installing collected packages: docutils, geohash2\n",
      "Successfully installed docutils-0.21.2 geohash2-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d54996b-6d6c-475c-ad0a-0d895f41ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe46e82-6e2f-463e-b61d-c09c017bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        \n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a044108c-c3e8-4325-8f00-6fcbe62ea69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0a4d0-8ea5-495a-881b-7633a3c85d9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export to schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e34172-ee99-417e-b800-e6a93bc1f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 05:25:33,580 - INFO - Creating master table: pd_co_2019_3h_agg3\n",
      "2024-07-07 05:25:34,243 - INFO - Creating master table: pd_co_2019_3h_agg5\n"
     ]
    }
   ],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "# output_schema_name = 'presence_data'\n",
    "output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_date = '2019-11-01'\n",
    "end_date = '2019-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "country_abbre = country_code.lower()  \n",
    "master_table_3 = f\"pd_{country_abbre}_2019_3h_agg3\"\n",
    "master_table_5 = f\"pd_{country_abbre}_2019_3h_agg5\"\n",
    "\n",
    "# Create the master tables if they do not exist\n",
    "create_table_query_3 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_3}(\n",
    "    geohash_3 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "create_table_query_5 = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {master_table_5}(\n",
    "    geohash_5 varchar,\n",
    "    no_of_points bigint,\n",
    "    no_of_unique_users bigint,\n",
    "    local_time varchar,\n",
    "    local_date varchar\n",
    ")\n",
    "\"\"\"\n",
    "    \n",
    "with con.connect() as connection:\n",
    "    logging.info(f\"Creating master table: {master_table_3}\")\n",
    "    connection.execute(create_table_query_3)\n",
    "    logging.info(f\"Creating master table: {master_table_5}\")\n",
    "    connection.execute(create_table_query_5)\n",
    "\n",
    "# List to record errors\n",
    "error_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536c09d-ea52-406b-9c88-ee97c376a49c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "\n",
    "            # Process for geohash_5\n",
    "            logging.info(f\"Aggregating data for geohash_5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = pe_dl_table_gen.groupby('geohash_5').agg(\n",
    "                no_of_points=('geohash_5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_5.to_sql(master_table_5, con, if_exists='append', index=False)\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into {master_table_3}\")\n",
    "\n",
    "            # Process for geohash_3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = pe_dl_table_gen.groupby('geohash_3').agg(\n",
    "                no_of_points=('geohash_3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            filtered_data_3.to_sql(master_table_3, con, if_exists='append', index=False, method='multi')\n",
    "            logging.info(f\"Inserted aggregated data for date {formatted_current_date} interval {start_hour} to {end_hour} into the {master_table_5}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee53328-3554-4860-8729-bc48a13a89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288c67d-370a-496f-911b-e743e9642d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c277f112-2f83-42fc-9b8c-601e7b6f5536",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export to jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6543700-d70a-4f86-b7a2-6f8f7c58fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a883a0ca-4e38-476c-843f-8d41e0b28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = '2019-03-22'\n",
    "end_date = '2019-03-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'MX'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "# export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_path = '/home/jovyan/Data/pd3/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n",
    "\n",
    "# List to record errors\n",
    "error_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381a613-35a2-4ea3-8ab0-3215806ca7d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 20:44:46,233 - INFO - Executing SQL query for date 20190322 and interval 0 to 3\n",
      "2024-07-10 20:48:26,858 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-10 20:48:27,148 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-10 20:48:27,177 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-10 20:48:27,419 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-10 20:48:27,434 - INFO - Appended data for date 20190322 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-10 20:48:27,435 - INFO - Executing SQL query for date 20190322 and interval 3 to 6\n"
     ]
    }
   ],
   "source": [
    "for current_date in date_range:\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    \n",
    "    for start_hour in range(0, 24, 3):\n",
    "        end_hour = start_hour + 3\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "        try:\n",
    "            # SQL Query to fetch data for the current 3-hour interval\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "                geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "                geohash_encode(lat, lng, 3) AS geohash_3\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "                AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "            \"\"\"\n",
    "\n",
    "            logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "            pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "            \n",
    "            # Convert event_datetime_local to datetime once\n",
    "            pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            pe_dl_table_gen['3_hour_interval'] = interval\n",
    "            pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "            \n",
    "            for geohash_col, export_file_path, write_header in [\n",
    "                ('geohash_5', export_file_path_5, write_header_5),\n",
    "                ('geohash_3', export_file_path_3, write_header_3)\n",
    "            ]:\n",
    "                # Aggregate data for geohash\n",
    "                logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "                aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                    no_of_points=(geohash_col, 'size'),\n",
    "                    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                    local_time=('3_hour_interval', 'first'),\n",
    "                    local_date=('local_date', 'first')\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Filter rows with no_of_unique_users > 10\n",
    "                filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "                \n",
    "                # Append the DataFrame to the CSV file\n",
    "                logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "                filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "                \n",
    "                # After the first write, set the header flag to False\n",
    "                if geohash_col == 'geohash5':\n",
    "                    write_header_5 = False\n",
    "                else:\n",
    "                    write_header_3 = False\n",
    "                \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "            logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfce0a-3ded-4d71-9216-89e735316522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746c7a7-93fb-4033-be9f-d8ce25b94506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176d391-fc94-4e95-91c2-2025828ccc38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d477592f-0902-45f7-86ea-04d505c791e7",
   "metadata": {},
   "source": [
    "## Filling dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdaa638-a871-41c4-a391-787516b6192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_hour_interval(start_hour, formatted_current_date):\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{formatted_current_date} {start_hour:02d}:00:00 - {end_hour:02d}:00:00\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Inputs\n",
    "inputs = [\n",
    "    # {'date': '20190102', 'interval': '18:00 - 24:00'},\n",
    "    # {'date': '20190108', 'interval': '12:00 - 15:00'},\n",
    "    # {'date': '20190117', 'interval': '12:00 - 15:00'},\n",
    "    # {'date': '20190120', 'interval': '0:00 - 3:00'},\n",
    "    # {'date': '20190120', 'interval': '6:00 - 9:00'},\n",
    "    # {'date': '20190120', 'interval': '12:00 - 15:00'},\n",
    "    # {'date': '20190121', 'interval': '9:00 - 24:00'},\n",
    "    # {'date': '20190130', 'interval': '9:00 - 15:00'},\n",
    "    # {'date': '20190302', 'interval': '12:00 - 24:00'},\n",
    "    # {'date': '20190304', 'interval': '18:00 - 21:00'},\n",
    "    # {'date': '20190314', 'interval': '18:00 - 24:00'},\n",
    "    {'date': '20190414', 'interval': '15:00 - 24:00'},\n",
    "    {'date': '20190416', 'interval': '0:00 - 6:00'},\n",
    "    {'date': '20190430', 'interval': '9:00 - 24:00'},\n",
    "    {'date': '20190513', 'interval': '0:00 - 3:00'},\n",
    "    {'date': '20190521', 'interval': '18:00 - 24:00'},\n",
    "    {'date': '20190524', 'interval': '18:00 - 21:00'},\n",
    "    {'date': '20190531', 'interval': '21:00 - 24:00'},\n",
    "    {'date': '20190613', 'interval': '18:00 - 24:00'},\n",
    "    {'date': '20190615', 'interval': '9:00 - 12:00'},\n",
    "    {'date': '20190623', 'interval': '18:00 - 24:00'}\n",
    "]\n",
    "country_code = 'MX'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "# export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_path = '/home/jovyan/Data/pd3/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "write_header_5 = False\n",
    "write_header_3 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2a03a-132c-424e-8eca-77f512a4fb4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 12:50:10,877 - INFO - Executing SQL query for date 20190414 and interval 15 to 18\n",
      "2024-07-11 12:54:58,196 - INFO - Aggregating data for geohash_5 for interval 15 to 18\n",
      "2024-07-11 12:54:59,442 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-11 12:54:59,489 - INFO - Aggregating data for geohash_3 for interval 15 to 18\n",
      "2024-07-11 12:55:00,631 - INFO - Exporting data for interval 15 to 18\n",
      "2024-07-11 12:55:00,646 - INFO - Appended data for date 20190414 interval 15 to 18 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 12:55:00,647 - INFO - Executing SQL query for date 20190416 and interval 0 to 3\n",
      "2024-07-11 12:57:25,544 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-11 12:57:25,885 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-11 12:57:25,910 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-11 12:57:26,205 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-11 12:57:26,219 - INFO - Appended data for date 20190416 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 12:57:26,219 - INFO - Executing SQL query for date 20190430 and interval 9 to 12\n",
      "2024-07-11 13:00:33,764 - INFO - Aggregating data for geohash_5 for interval 9 to 12\n",
      "2024-07-11 13:00:34,829 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-11 13:00:34,858 - INFO - Aggregating data for geohash_3 for interval 9 to 12\n",
      "2024-07-11 13:00:35,820 - INFO - Exporting data for interval 9 to 12\n",
      "2024-07-11 13:00:35,835 - INFO - Appended data for date 20190430 interval 9 to 12 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 13:00:35,836 - INFO - Executing SQL query for date 20190513 and interval 0 to 3\n",
      "2024-07-11 13:02:38,387 - INFO - Aggregating data for geohash_5 for interval 0 to 3\n",
      "2024-07-11 13:02:38,591 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-11 13:02:38,615 - INFO - Aggregating data for geohash_3 for interval 0 to 3\n",
      "2024-07-11 13:02:38,802 - INFO - Exporting data for interval 0 to 3\n",
      "2024-07-11 13:02:38,814 - INFO - Appended data for date 20190513 interval 0 to 3 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 13:02:38,814 - INFO - Executing SQL query for date 20190521 and interval 18 to 21\n",
      "2024-07-11 13:05:46,182 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-11 13:05:47,126 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-11 13:05:47,151 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-11 13:05:48,008 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-11 13:05:48,020 - INFO - Appended data for date 20190521 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 13:05:48,021 - INFO - Executing SQL query for date 20190524 and interval 18 to 21\n",
      "2024-07-11 13:09:16,166 - INFO - Aggregating data for geohash_5 for interval 18 to 21\n",
      "2024-07-11 13:09:17,272 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-11 13:09:17,304 - INFO - Aggregating data for geohash_3 for interval 18 to 21\n",
      "2024-07-11 13:09:18,306 - INFO - Exporting data for interval 18 to 21\n",
      "2024-07-11 13:09:18,320 - INFO - Appended data for date 20190524 interval 18 to 21 to pd_mx_2019_agg5_3h.csv and pd_mx_2019_agg3_3h.csv\n",
      "2024-07-11 13:09:18,320 - INFO - Executing SQL query for date 20190531 and interval 21 to 24\n"
     ]
    }
   ],
   "source": [
    "# List to record errors\n",
    "error_records = []\n",
    "\n",
    "for input in inputs:\n",
    "    input_date = input['date']\n",
    "    input_interval = input['interval']\n",
    "\n",
    "    # Parse the input\n",
    "    formatted_current_date = input_date\n",
    "    start_hour = int(input_interval.split(':')[0])\n",
    "    end_hour = start_hour + 3\n",
    "\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    current_date = datetime.strptime(input_date, '%Y%m%d')\n",
    "    lookback_date = current_date - timedelta(days=1)\n",
    "    lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "    # Format dates for the SQL query in 'yyyymmdd' format\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "\n",
    "    try:\n",
    "        # SQL Query to fetch data for the specified date and interval\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash_5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash_3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "            AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN {start_hour} AND {end_hour-1}\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date} and interval {start_hour} to {end_hour}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime once\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Create 3-hour interval column\n",
    "        interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "        pe_dl_table_gen['3_hour_interval'] = interval\n",
    "        pe_dl_table_gen['local_date'] = formatted_current_date\n",
    "        \n",
    "        for geohash_col, export_file_path, write_header in [\n",
    "            ('geohash_5', export_file_path_5, write_header_5),\n",
    "            ('geohash_3', export_file_path_3, write_header_3)\n",
    "        ]:\n",
    "            # Aggregate data for geohash\n",
    "            logging.info(f\"Aggregating data for {geohash_col} for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data = pe_dl_table_gen.groupby(geohash_col).agg(\n",
    "                no_of_points=(geohash_col, 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data = aggregated_data[aggregated_data['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file\n",
    "            logging.info(f\"Exporting data for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data.to_csv(export_file_path, mode='a', header=write_header, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False\n",
    "            if geohash_col == 'geohash_5':\n",
    "                write_header_5 = False\n",
    "            else:\n",
    "                write_header_3 = False\n",
    "            \n",
    "        logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_records.append((formatted_current_date, start_hour, end_hour, str(e)))\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date} interval {start_hour} to {end_hour}: {e}\")\n",
    "\n",
    "# Log all error records at the end\n",
    "logging.info(\"Error records:\")\n",
    "for record in error_records:\n",
    "    logging.info(f\"Date: {record[0]}, Interval: {record[1]}:00 - {record[2]}:00, Error: {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba042a-244e-4f9e-ac06-74e87aec0221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d2e15f9-3fc3-4141-a782-71054cd873bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da2b5f41-d736-4644-98cb-b1951ee368ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0rfr</td>\n",
       "      <td>106</td>\n",
       "      <td>21</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20u8</td>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20u9</td>\n",
       "      <td>307</td>\n",
       "      <td>48</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d21nc</td>\n",
       "      <td>1073</td>\n",
       "      <td>170</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d21p1</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232296</th>\n",
       "      <td>d650p</td>\n",
       "      <td>149</td>\n",
       "      <td>27</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232297</th>\n",
       "      <td>d6h1s</td>\n",
       "      <td>634</td>\n",
       "      <td>80</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232298</th>\n",
       "      <td>d6h1t</td>\n",
       "      <td>280</td>\n",
       "      <td>54</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232299</th>\n",
       "      <td>d6h8e</td>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232300</th>\n",
       "      <td>d6h8s</td>\n",
       "      <td>190</td>\n",
       "      <td>29</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231815 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geohash_5 no_of_points no_of_unique_users  \\\n",
       "0          d0rfr          106                 21   \n",
       "1          d20u8           69                 16   \n",
       "2          d20u9          307                 48   \n",
       "3          d21nc         1073                170   \n",
       "4          d21p1           23                 11   \n",
       "...          ...          ...                ...   \n",
       "232296     d650p          149                 27   \n",
       "232297     d6h1s          634                 80   \n",
       "232298     d6h1t          280                 54   \n",
       "232299     d6h8e           87                 21   \n",
       "232300     d6h8s          190                 29   \n",
       "\n",
       "                          local_time  local_date  \n",
       "0       20191101 00:00:00 - 03:00:00    20191101  \n",
       "1       20191101 00:00:00 - 03:00:00    20191101  \n",
       "2       20191101 00:00:00 - 03:00:00    20191101  \n",
       "3       20191101 00:00:00 - 03:00:00    20191101  \n",
       "4       20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                              ...         ...  \n",
       "232296  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232297  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232298  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232299  20191108 09:00:00 - 12:00:00    20191108  \n",
       "232300  20191108 09:00:00 - 12:00:00    20191108  \n",
       "\n",
       "[231815 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the file paths\n",
    "export_file_path_5 = '/home/jovyan/Data/pd3/pd_co_2019_agg5_3h.csv'\n",
    "export_file_path_3 = '/home/jovyan/Data/pd3/pd_co_2019_agg3_3h.csv'\n",
    "\n",
    "# Read the geohash_5 CSV file to get the column names\n",
    "data_5 = pd.read_csv(export_file_path_5)\n",
    "data_5['local_date'] = pd.to_numeric(data_5['local_date'], errors='coerce')\n",
    "data_5 = data_5.dropna(subset=['local_date'])\n",
    "data_5['local_date'] = data_5['local_date'].astype(int)\n",
    "\n",
    "data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28a91dee-d7fc-41a9-8a2b-8e3176eacf22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0r</td>\n",
       "      <td>125</td>\n",
       "      <td>24</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20</td>\n",
       "      <td>678</td>\n",
       "      <td>107</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d21</td>\n",
       "      <td>1498</td>\n",
       "      <td>242</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d22</td>\n",
       "      <td>253</td>\n",
       "      <td>38</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d23</td>\n",
       "      <td>1636</td>\n",
       "      <td>289</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>d3s</td>\n",
       "      <td>210</td>\n",
       "      <td>20</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>d3u</td>\n",
       "      <td>1091</td>\n",
       "      <td>100</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19584</th>\n",
       "      <td>d4j</td>\n",
       "      <td>1140</td>\n",
       "      <td>94</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19585</th>\n",
       "      <td>d65</td>\n",
       "      <td>247</td>\n",
       "      <td>41</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586</th>\n",
       "      <td>d6h</td>\n",
       "      <td>1644</td>\n",
       "      <td>170</td>\n",
       "      <td>20191108 09:00:00 - 12:00:00</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19587 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      geohash_5  no_of_points  no_of_unique_users  \\\n",
       "0           d0r           125                  24   \n",
       "1           d20           678                 107   \n",
       "2           d21          1498                 242   \n",
       "3           d22           253                  38   \n",
       "4           d23          1636                 289   \n",
       "...         ...           ...                 ...   \n",
       "19582       d3s           210                  20   \n",
       "19583       d3u          1091                 100   \n",
       "19584       d4j          1140                  94   \n",
       "19585       d65           247                  41   \n",
       "19586       d6h          1644                 170   \n",
       "\n",
       "                         local_time  local_date  \n",
       "0      20191101 00:00:00 - 03:00:00    20191101  \n",
       "1      20191101 00:00:00 - 03:00:00    20191101  \n",
       "2      20191101 00:00:00 - 03:00:00    20191101  \n",
       "3      20191101 00:00:00 - 03:00:00    20191101  \n",
       "4      20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                             ...         ...  \n",
       "19582  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19583  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19584  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19585  20191108 09:00:00 - 12:00:00    20191108  \n",
       "19586  20191108 09:00:00 - 12:00:00    20191108  \n",
       "\n",
       "[19587 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the geohash_3 CSV file without headers\n",
    "data_3 = pd.read_csv(export_file_path_3, header=None)\n",
    "\n",
    "data_3.columns = data_5.columns # Assign column names from geohash_5 to geohash_3 for CO\n",
    "\n",
    "data_3['local_date'] = pd.to_numeric(data_3['local_date'], errors='coerce')\n",
    "data_3 = data_3.dropna(subset=['local_date'])\n",
    "data_3['local_date'] = data_3['local_date'].astype(int)\n",
    "\n",
    "data_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea29a4-9899-4b1b-92da-97707a4b33f9",
   "metadata": {},
   "source": [
    "## Format Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "515e23c3-c693-4a06-b41f-242070d07922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0r</td>\n",
       "      <td>125</td>\n",
       "      <td>24</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d34</td>\n",
       "      <td>21285</td>\n",
       "      <td>3198</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d35</td>\n",
       "      <td>1191</td>\n",
       "      <td>197</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d36</td>\n",
       "      <td>591</td>\n",
       "      <td>89</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d37</td>\n",
       "      <td>2493</td>\n",
       "      <td>392</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>d3d</td>\n",
       "      <td>7267</td>\n",
       "      <td>695</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>d39</td>\n",
       "      <td>497</td>\n",
       "      <td>43</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19584</th>\n",
       "      <td>d37</td>\n",
       "      <td>3988</td>\n",
       "      <td>462</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19585</th>\n",
       "      <td>d3j</td>\n",
       "      <td>148</td>\n",
       "      <td>13</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586</th>\n",
       "      <td>d23</td>\n",
       "      <td>3353</td>\n",
       "      <td>375</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19587 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      geohash_5  no_of_points  no_of_unique_users  \\\n",
       "0           d0r           125                  24   \n",
       "1           d34         21285                3198   \n",
       "2           d35          1191                 197   \n",
       "3           d36           591                  89   \n",
       "4           d37          2493                 392   \n",
       "...         ...           ...                 ...   \n",
       "19582       d3d          7267                 695   \n",
       "19583       d39           497                  43   \n",
       "19584       d37          3988                 462   \n",
       "19585       d3j           148                  13   \n",
       "19586       d23          3353                 375   \n",
       "\n",
       "                         local_time  local_date          start_time  \n",
       "0      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "1      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "2      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "3      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "4      20191101 00:00:00 - 03:00:00    20191101 2019-11-01 00:00:00  \n",
       "...                             ...         ...                 ...  \n",
       "19582  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19583  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19584  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19585  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "19586  20191231 21:00:00 - 24:00:00    20191231 2019-12-31 21:00:00  \n",
       "\n",
       "[19587 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the start time from local_time and convert to datetime\n",
    "data_5['start_time'] = data_5['local_time'].str.split(' - ').str[0]\n",
    "data_3['start_time'] = data_3['local_time'].str.split(' - ').str[0]\n",
    "\n",
    "data_5['start_time'] = pd.to_datetime(data_5['start_time'], format='%Y%m%d %H:%M:%S')\n",
    "data_3['start_time'] = pd.to_datetime(data_3['start_time'], format='%Y%m%d %H:%M:%S')\n",
    "\n",
    "# Sort the data by start_time\n",
    "data_5_sorted = data_5.sort_values(by='start_time').reset_index(drop=True)\n",
    "data_3_sorted = data_3.sort_values(by='start_time').reset_index(drop=True)\n",
    "data_3_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9044d4d-1a7c-43e2-8ceb-ff3e9698f856",
   "metadata": {},
   "source": [
    "## Check output missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6cd81e0-ac68-43d4-b9f5-e3fcfb5346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the full range of 3-hour intervals\n",
    "start_date = '2019-11-01'\n",
    "end_date = '2019-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='3H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d92852e0-2782-4feb-b162-ca25aeb2e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals in geohash_5 data:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing intervals in geohash_5 data\n",
    "data_5_intervals = data_5_sorted['start_time']\n",
    "missing_intervals_5 = date_range.difference(data_5_intervals)\n",
    "print(\"Missing intervals in geohash_5 data:\")\n",
    "print(missing_intervals_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2989532e-7104-4e34-9acc-d33326375f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals in geohash_3 data:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing intervals in geohash_3 data\n",
    "data_3_intervals = data_3_sorted['start_time']\n",
    "missing_intervals_3 = date_range.difference(data_3_intervals)\n",
    "print(\"Missing intervals in geohash_3 data:\")\n",
    "print(missing_intervals_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96a156-6fb7-4e17-99a6-476924af9c1d",
   "metadata": {},
   "source": [
    "## Overwrite output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f90d2e8-7292-49a7-87b0-a81272ef7568",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>no_of_points</th>\n",
       "      <th>no_of_unique_users</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0rfr</td>\n",
       "      <td>106</td>\n",
       "      <td>21</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d3473</td>\n",
       "      <td>166</td>\n",
       "      <td>40</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d3478</td>\n",
       "      <td>2829</td>\n",
       "      <td>516</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3479</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d347b</td>\n",
       "      <td>2333</td>\n",
       "      <td>420</td>\n",
       "      <td>20191101 00:00:00 - 03:00:00</td>\n",
       "      <td>20191101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231810</th>\n",
       "      <td>d345y</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231811</th>\n",
       "      <td>d345z</td>\n",
       "      <td>294</td>\n",
       "      <td>62</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231812</th>\n",
       "      <td>d3467</td>\n",
       "      <td>304</td>\n",
       "      <td>43</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231813</th>\n",
       "      <td>d3401</td>\n",
       "      <td>165</td>\n",
       "      <td>18</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231814</th>\n",
       "      <td>d2gk0</td>\n",
       "      <td>114</td>\n",
       "      <td>15</td>\n",
       "      <td>20191231 21:00:00 - 24:00:00</td>\n",
       "      <td>20191231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231815 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geohash_5 no_of_points no_of_unique_users  \\\n",
       "0          d0rfr          106                 21   \n",
       "1          d3473          166                 40   \n",
       "2          d3478         2829                516   \n",
       "3          d3479           57                 18   \n",
       "4          d347b         2333                420   \n",
       "...          ...          ...                ...   \n",
       "231810     d345y           30                 11   \n",
       "231811     d345z          294                 62   \n",
       "231812     d3467          304                 43   \n",
       "231813     d3401          165                 18   \n",
       "231814     d2gk0          114                 15   \n",
       "\n",
       "                          local_time  local_date  \n",
       "0       20191101 00:00:00 - 03:00:00    20191101  \n",
       "1       20191101 00:00:00 - 03:00:00    20191101  \n",
       "2       20191101 00:00:00 - 03:00:00    20191101  \n",
       "3       20191101 00:00:00 - 03:00:00    20191101  \n",
       "4       20191101 00:00:00 - 03:00:00    20191101  \n",
       "...                              ...         ...  \n",
       "231810  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231811  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231812  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231813  20191231 21:00:00 - 24:00:00    20191231  \n",
       "231814  20191231 21:00:00 - 24:00:00    20191231  \n",
       "\n",
       "[231815 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3_sorted = data_3_sorted.drop(['start_time'], axis=1)\n",
    "data_5_sorted = data_5_sorted.drop(['start_time'], axis=1)\n",
    "data_5_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "196c5ec9-d14d-454f-b186-143db0bcde2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the cleaned and sorted data back to the original paths, overwriting the existing files\n",
    "data_5_sorted.to_csv(export_file_path_5, index=False)\n",
    "data_3_sorted.to_csv(export_file_path_3, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5b18-b41f-4ade-be91-23cadc1ff2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c6003-99eb-46b3-922c-ed0cfed2cf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344be9b-be6f-4c9a-9445-6063316e8e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd3dda5-cf68-4ef5-bd61-89cc137a60ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45622ce-9901-45b1-9805-0252db6c76ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Worked query per day. - not working for MX\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_name_5} and {export_file_name_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fe83-0e29-441c-ae94-3913a9ea56ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055991e-c5d8-44c1-b790-961344e56c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa0f7-2b33-4d15-8970-63048d233858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1175b728-44a5-419f-9d80-2b87bed95a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# !!!!!!!!!!!!!!! 这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09410c-5fd9-43e4-9e2c-96af3d2afb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "# output_schema_name = 'pop_density'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'CO'\n",
    "start_date = 20191101\n",
    "end_date = 20191102\n",
    "# longitude_ranges = [(-82, -74.53125), (-74.53125, -65)]  # CO specific longitude ranges\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "failed_inserts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b9d47-428f-427a-8db5-40e7ef9dbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2019-11-12'\n",
    "end_date = '2019-11-11'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "country_code = 'CO'\n",
    "\n",
    "# Define the export file paths\n",
    "export_file_name_5 = f\"pd_{country_code.lower()}_2019_agg5_3h.csv\"\n",
    "export_file_name_3 = f\"pd_{country_code.lower()}_2019_agg3_3h.csv\"\n",
    "\n",
    "# Define the export file paths\n",
    "export_path = '/home/jovyan/Data/pd3_test/'\n",
    "export_file_path_5 = f\"{export_path}{export_file_name_5}\"\n",
    "export_file_path_3 = f\"{export_path}{export_file_name_3}\"\n",
    "\n",
    "# Check if files already exist to determine header writing\n",
    "write_header_5 = not os.path.exists(export_file_path_5)\n",
    "write_header_3 = not os.path.exists(export_file_path_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020a18-0fb9-4bee-b368-37b9309b61ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Loop through each day in the date range\n",
    "# for current_date in date_range:\n",
    "#     try:\n",
    "#         formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "#         # Calculate the lookback and lookahead dates\n",
    "#         lookback_date = current_date - timedelta(days=1)\n",
    "#         lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "#         # Format dates for the SQL query in 'yyyymmdd' format\n",
    "#         formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "#         formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "#         formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "#         # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "#         query = f\"\"\"\n",
    "#         SELECT \n",
    "#             cuebiq_id, \n",
    "#             event_zoned_datetime, \n",
    "#             processing_date,\n",
    "#             lat,\n",
    "#             lng,\n",
    "#             TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "#             EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "#             geohash_encode(lat, lng, 5) AS geohash5,\n",
    "#             geohash_encode(lat, lng, 3) AS geohash3\n",
    "#         FROM {pe_dl_table}\n",
    "#         WHERE \n",
    "#             processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "#             AND country_code = '{country_code}' \n",
    "#             AND event_zoned_datetime IS NOT NULL\n",
    "#             AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#             AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "#         \"\"\"\n",
    "        \n",
    "#         logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "#         pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "#         # Convert event_datetime_local to datetime\n",
    "#         pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "#         # Loop through each 3-hour interval\n",
    "#         for start_hour in range(0, 24, 3):\n",
    "#             end_hour = start_hour + 3\n",
    "            \n",
    "#             # Filter data for the current 3-hour interval\n",
    "#             interval_data = pe_dl_table_gen[\n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "#                 (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "#             ].copy()\n",
    "            \n",
    "#             # Create 3-hour interval column\n",
    "#             interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "#             interval_data['3_hour_interval'] = interval\n",
    "            \n",
    "#             # Aggregate data for geohash5\n",
    "#             logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "#                 no_of_points=('geohash5', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash5\n",
    "#             logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash5\n",
    "#             write_header_5 = False\n",
    "            \n",
    "#             # Aggregate data for geohash3\n",
    "#             logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "#             aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "#                 no_of_points=('geohash3', 'size'),\n",
    "#                 no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "#                 local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "#             ).reset_index()\n",
    "            \n",
    "#             # Filter rows with no_of_unique_users > 10\n",
    "#             filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "#             # Append the DataFrame to the CSV file for geohash3\n",
    "#             logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "#             filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "#             # After the first write, set the header flag to False for geohash3\n",
    "#             write_header_3 = False\n",
    "            \n",
    "#             logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "#     # Move to the next day\n",
    "#     current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f97b3a-022f-4b9b-9983-213d2dff3fe1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date with geohashes calculated in the query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "            geohash_encode(lat, lng, 5) AS geohash5,\n",
    "            geohash_encode(lat, lng, 3) AS geohash3\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "        \n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "            \n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "            \n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "            interval_data['local_date'] = formatted_current_date\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_path_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', 'first'),\n",
    "                local_date=('local_date', 'first')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_path_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "            \n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "            \n",
    "            logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adc895-13b8-4190-8651-894e4bad5697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fb058-0ded-431a-85aa-a3bfd92c51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash5 and geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash5,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 5) AS geohash5,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash5\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL5 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5 = sql_engine.read_sql(query)\n",
    "\n",
    "        # SQL Query to fetch and aggregate data for geohash3\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            geohash3,\n",
    "            COUNT(*) as no_of_points,\n",
    "            COUNT(DISTINCT cuebiq_id) as no_of_unique_users,\n",
    "            '{formatted_current_date}' as event_date,\n",
    "            '{get_3_hour_interval(0, formatted_current_date)}' as interval_0_3,\n",
    "            '{get_3_hour_interval(3, formatted_current_date)}' as interval_3_6,\n",
    "            '{get_3_hour_interval(6, formatted_current_date)}' as interval_6_9,\n",
    "            '{get_3_hour_interval(9, formatted_current_date)}' as interval_9_12,\n",
    "            '{get_3_hour_interval(12, formatted_current_date)}' as interval_12_15,\n",
    "            '{get_3_hour_interval(15, formatted_current_date)}' as interval_15_18,\n",
    "            '{get_3_hour_interval(18, formatted_current_date)}' as interval_18_21,\n",
    "            '{get_3_hour_interval(21, formatted_current_date)}' as interval_21_24\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                geohash_encode(lat, lng, 3) AS geohash3,\n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "                EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "                AND country_code = '{country_code}' \n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "                AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        )\n",
    "        GROUP BY geohash3\n",
    "        HAVING COUNT(DISTINCT cuebiq_id) > 10\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL3 query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3 = sql_engine.read_sql(query)\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash5\n",
    "        logging.info(f\"Exporting data to {export_file_path_5} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash5\n",
    "        write_header_5 = False\n",
    "        \n",
    "        # Append the DataFrame to the CSV file for geohash3\n",
    "        logging.info(f\"Exporting data to {export_file_path_3} for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "        \n",
    "        # After the first write, set the header flag to False for geohash3\n",
    "        write_header_3 = False\n",
    "\n",
    "        logging.info(f\"Appended data for date {formatted_current_date} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c587e04-766d-4c83-9234-f6bd93e6fff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5c1fb-bce9-4098-bef6-1b2dc952b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a1127-0a10-4c27-a131-467026fbae94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c274-2c3d-4198-8d89-2c127eb4e280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c91b-b3c2-4e93-9b79-6411d7b47f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8948ce-1ebc-42c4-af54-844999063440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116555cb-e34a-4303-b77d-9a91e2dad215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bcea9-c6aa-46f1-a2c5-2d246d6716dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf4250-178e-474b-8b33-efb5705d612a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Loop through each day in the date range\n",
    "for current_date in date_range:\n",
    "    try:\n",
    "        formatted_current_date = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "\n",
    "        # Format dates for the SQL query in 'yyyymmdd' format\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "\n",
    "        # SQL Query to fetch data for the current date\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng,\n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "\n",
    "        # Convert event_datetime_local to datetime\n",
    "        pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "        # Calculate geohashes\n",
    "        logging.info(f\"Processing geohashes for date {formatted_current_date}\")\n",
    "        pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "        # Loop through each 3-hour interval\n",
    "        for start_hour in range(0, 24, 3):\n",
    "            end_hour = start_hour + 3\n",
    "\n",
    "            # Filter data for the current 3-hour interval\n",
    "            interval_data = pe_dl_table_gen[\n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "                (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "            ].copy()\n",
    "\n",
    "            # Create 3-hour interval column\n",
    "            interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "            interval_data['3_hour_interval'] = interval\n",
    "\n",
    "            # Aggregate data for geohash5\n",
    "            logging.info(f\"Aggregating data for geohash5 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash5\n",
    "            logging.info(f\"Exporting data to {export_file_name_5} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash5\n",
    "            write_header_5 = False\n",
    "\n",
    "            # Aggregate data for geohash3\n",
    "            logging.info(f\"Aggregating data for geohash3 for interval {start_hour} to {end_hour}\")\n",
    "            aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "                local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "            # Append the DataFrame to the CSV file for geohash3\n",
    "            logging.info(f\"Exporting data to {export_file_name_3} for interval {start_hour} to {end_hour}\")\n",
    "            filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "\n",
    "            # After the first write, set the header flag to False for geohash3\n",
    "            write_header_3 = False\n",
    "\n",
    "            # logging.info(f\"Appended data for date {formatted_current_date} interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e18cd-7712-4ce5-8af2-17132ddfb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c1a10-ca49-4978-80af-bf1b1b436979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f70d6c-14ee-4bb3-a144-8f9d22756993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849c172-25b4-4015-9c3f-4c54c6f9b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f35f29-e38b-4be2-bbc4-b5603656734a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a575-0da2-445d-a070-9d5e3a37309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f2b3f-068f-441d-b29b-785322d09091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101c3db-396b-4176-b0da-f46abb48a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721c5a7-4c76-4278-a9b9-6bc0ab54028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75951-dd0a-4bc0-80ef-2e3cb2a2f8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d343d50-9c6c-4c8b-a774-2cec03b0b627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283036a-b031-4b32-8ae0-3cc1cdad8652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34e23-66c7-485c-807c-3476a4344dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abfca8-eb70-4e28-8fb1-0665f28903e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16774b-eb43-4b9b-b101-955588f36125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8862327-eaba-46a0-8ecb-5e142581315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965534c4-906e-4714-8dae-a12a9a7d83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd686a0-4c83-4154-a4f7-37acd1960ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64342f59-eaf3-41d2-aabd-932a4994830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e038-fb28-4124-a130-d6d2b36350f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54a6f6-e75b-4644-801c-dd37105ee619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b67b-9e18-4c61-8756-3a8aaae6384c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee41c-424a-40dd-8ed8-62a8764d629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed2e44-ad40-4863-9a63-637f70c3775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5481759-bef6-47f8-84fd-bd7bd04d43dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8a3f-9bbe-4abe-9fe0-236738e72528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ef14e-1604-4c31-b723-cddf9aab9947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfdaea-78a1-4a00-9dc3-6fa793f6fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one seemms to be working \n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    try:\n",
    "        # Calculate the lookback and lookahead dates\n",
    "        lookback_date = current_date - timedelta(days=1)\n",
    "        lookahead_date = current_date + timedelta(days=35)\n",
    "        \n",
    "        # Format dates for the SQL query\n",
    "        formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "        formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "        formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Construct the SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "        \n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_5.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash5 into SQL table\n",
    "            if not filtered_data_5.empty:\n",
    "                table_name_agg5 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg5\"\n",
    "                insert_data_with_retry(filtered_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows with no_of_unique_users > 10\n",
    "            filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "            \n",
    "            # Add the local_date column\n",
    "            filtered_data_3.loc[:, 'local_date'] = formatted_current_date\n",
    "            \n",
    "            # Insert filtered aggregated data for geohash3 into SQL table\n",
    "            if not filtered_data_3.empty:\n",
    "                table_name_agg3 = f\"pd_{country_code.lower()}_{formatted_current_date}_agg3\"\n",
    "                insert_data_with_retry(filtered_data_3, table_name_agg3, con)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e495e-3f2b-4872-b595-61c1f8242278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926ef6e-772d-442f-9416-55c3c6c5c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d2b9d-7e31-4ad1-824b-5cd8a23c95e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f16fd-3cfc-4ea4-87ee-baca476c647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0f9e2-49ff-452e-8f32-d84cb913dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51349b29-b674-4f30-83bd-9f6386e5655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd167fd2-7c44-4a94-9b9c-b9b5dca82d33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Check by single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b7184-2831-4507-8682-3af4bdd08f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46018200-ab33-4c97-bf10-53fbca072c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "import os\n",
    "\n",
    "# Define the export file path\n",
    "export_file_path_3 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg3_3h.csv'\n",
    "export_file_path_5 = '/home/jovyan/Data/pd3_test/pd_co_2019_agg5_3h.csv'\n",
    "\n",
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Create a function to determine the 3-hour interval based on a given date\n",
    "def get_3_hour_interval(start_hour, current_date):\n",
    "    start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=start_hour)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Initialize flags to write the headers only once\n",
    "write_header_5 = True\n",
    "write_header_3 = True\n",
    "\n",
    "# Loop through each 3-hour interval\n",
    "formatted_current_date = pd.to_datetime(formatted_current_date)  # Ensure it's a datetime object\n",
    "for start_hour in range(0, 24, 3):\n",
    "    end_hour = start_hour + 3\n",
    "    \n",
    "    # Filter data for the current 3-hour interval\n",
    "    interval_data = pe_dl_table_gen[\n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour >= start_hour) & \n",
    "        (pe_dl_table_gen['event_datetime_local'].dt.hour < end_hour)\n",
    "    ].copy()\n",
    "    \n",
    "    # Create 3-hour interval column\n",
    "    interval = get_3_hour_interval(start_hour, formatted_current_date)\n",
    "    interval_data['3_hour_interval'] = interval\n",
    "    \n",
    "    # Aggregate data for geohash5\n",
    "    aggregated_data_5 = interval_data.groupby('geohash5').agg(\n",
    "        no_of_points=('geohash5', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash5\n",
    "    filtered_data_5.to_csv(export_file_path_5, mode='a', header=write_header_5, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash5\n",
    "    write_header_5 = False\n",
    "    \n",
    "    # Aggregate data for geohash3\n",
    "    aggregated_data_3 = interval_data.groupby('geohash3').agg(\n",
    "        no_of_points=('geohash3', 'size'),\n",
    "        no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "        intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter rows with no_of_unique_users > 10\n",
    "    filtered_data_3 = aggregated_data_3[aggregated_data_3['no_of_unique_users'] > 10].copy()\n",
    "    \n",
    "    # Append the DataFrame to the CSV file for geohash3\n",
    "    filtered_data_3.to_csv(export_file_path_3, mode='a', header=write_header_3, index=False)\n",
    "    \n",
    "    # After the first write, set the header flag to False for geohash3\n",
    "    write_header_3 = False\n",
    "    \n",
    "    print(f\"Appended data for interval {start_hour} to {end_hour} to {export_file_path_5} and {export_file_path_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af6712-172d-42e8-92ef-e193c08202ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a73f2-0941-4640-b810-139180fc4924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369faf8d-fc67-4b50-99e2-698af62fb47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696fe4f-4b3c-456f-9862-69ae758fc1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915eb14-07e3-496b-b8b2-733d01d80cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bb1c2a-a79d-4575-823d-d60ae8d68441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc27da9-ce7c-4c81-9baa-9948cc38d47e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    processing_date,\n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a8dd2-4a7e-4ab0-8b90-ad328598d301",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate geohashes\n",
    "pe_dl_table_gen['geohash5'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "pe_dl_table_gen['geohash3'] = pe_dl_table_gen.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44cc33-43de-4484-ba65-c9ee31f13ba2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    local_time=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fc0e4-8076-47f9-8f45-eebbc712092b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geohash2\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cuebiq_id, \n",
    "    event_zoned_datetime, \n",
    "    lat,\n",
    "    lng,\n",
    "    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "    EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "    AND country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    AND EXTRACT(HOUR FROM TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) IN (0, 1, 2)\n",
    "\"\"\"\n",
    "logging.info(f\"Executing SQL query for date {formatted_current_date}\")\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "pe_dl_table_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3d588-1f07-4eb7-9fde-b9b8e34f6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate geohashes\n",
    "def calculate_geohashes(df, lat_col, lng_col):\n",
    "    df['geohash5'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=5), axis=1)\n",
    "    df['geohash3'] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lng_col], precision=3), axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate geohashes\n",
    "pe_dl_table_gen = calculate_geohashes(pe_dl_table_gen, 'lat', 'lng')\n",
    "\n",
    "# Convert event_datetime_local to datetime\n",
    "pe_dl_table_gen['event_datetime_local'] = pd.to_datetime(pe_dl_table_gen['event_datetime_local'])\n",
    "\n",
    "# Create a function to determine the 3-hour interval\n",
    "def get_3_hour_interval(dt):\n",
    "    start_hour = (dt.hour // 3) * 3\n",
    "    start_time = dt.replace(hour=start_hour, minute=0, second=0, microsecond=0)\n",
    "    end_time = start_time + pd.Timedelta(hours=3)\n",
    "    return f\"{start_time.strftime('%Y-%m-%d %H:%M:%S')}/{end_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Apply the function to create the 3-hour interval column\n",
    "pe_dl_table_gen['3_hour_interval'] = pe_dl_table_gen['event_datetime_local'].apply(get_3_hour_interval)\n",
    "\n",
    "# Aggregate data for geohash5\n",
    "aggregated_data_5 = pe_dl_table_gen.groupby('geohash5').agg(\n",
    "    no_of_points=('geohash5', 'size'),\n",
    "    no_of_unique_users=('cuebiq_id', 'nunique'),\n",
    "    intervals=('3_hour_interval', lambda x: '; '.join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Filter rows with no_of_unique_users > 10\n",
    "filtered_data_5 = aggregated_data_5[aggregated_data_5['no_of_unique_users'] > 10].copy()\n",
    "\n",
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb012c-6879-460f-b74e-ce3c3afac10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a0b31-b807-4379-a859-4fedd6e15832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab1ede-6b29-4dc0-812d-70013cacb04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ce8cd-9b8e-409c-b680-508c00dc6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17829f-2722-48d6-975c-9a3bb6623f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc2e21-e91d-4109-ac04-0c5a2de5b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0353dd-55c7-42de-837f-119f48be6435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2c057-683c-451d-8970-6b6d8e5f8172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad075e-4927-4a9c-ab40-ed648ec5dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c21ab-230e-4951-998a-2f67576f87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279564a-3432-451d-9f63-bd40672b375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aabfc8-5afd-4644-bde7-5055d567d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964deb-74d2-482b-9cd2-812b2da03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808822b-920e-4541-9c13-b335f8b43a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630359c-25d2-4a9c-be72-973dd6c58e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71e37f-1292-4161-8bbf-b5fdc1114214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
