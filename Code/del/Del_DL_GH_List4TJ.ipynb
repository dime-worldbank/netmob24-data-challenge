{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f2e1fe-4c8b-454f-a5fe-070cf69bb392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-geohash\n",
      "  Using cached python_geohash-0.8.5-cp39-cp39-linux_x86_64.whl\n",
      "Installing collected packages: python-geohash\n",
      "Successfully installed python-geohash-0.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3349e-1f4e-479e-a78d-9055a5c7961d",
   "metadata": {},
   "source": [
    "# Single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0440c7a3-2312-4247-a631-b973fc5e2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa05b6d-6044-4615-b093-b15d77d0b5f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>timezoneoffset_secs</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_utc</th>\n",
       "      <th>event_zoned_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946507429</td>\n",
       "      <td>2019-01-28T05:54:03-06:00</td>\n",
       "      <td>20190128</td>\n",
       "      <td>-21600</td>\n",
       "      <td>20.602415</td>\n",
       "      <td>-105.232750</td>\n",
       "      <td>2019-01-27 23:54:03</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>998611644</td>\n",
       "      <td>2019-01-28T14:36:10-06:00</td>\n",
       "      <td>20190128</td>\n",
       "      <td>-21600</td>\n",
       "      <td>24.040835</td>\n",
       "      <td>-104.657184</td>\n",
       "      <td>2019-01-28 08:36:10</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>998611644</td>\n",
       "      <td>2019-01-28T08:20:16-06:00</td>\n",
       "      <td>20190128</td>\n",
       "      <td>-21600</td>\n",
       "      <td>24.038745</td>\n",
       "      <td>-104.652544</td>\n",
       "      <td>2019-01-28 02:20:16</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>998611644</td>\n",
       "      <td>2019-01-28T14:33:14-06:00</td>\n",
       "      <td>20190128</td>\n",
       "      <td>-21600</td>\n",
       "      <td>24.038745</td>\n",
       "      <td>-104.652544</td>\n",
       "      <td>2019-01-28 08:33:14</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>998611644</td>\n",
       "      <td>2019-01-28T08:16:44-06:00</td>\n",
       "      <td>20190128</td>\n",
       "      <td>-21600</td>\n",
       "      <td>24.042602</td>\n",
       "      <td>-104.654026</td>\n",
       "      <td>2019-01-28 02:16:44</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556911</th>\n",
       "      <td>1127465074</td>\n",
       "      <td>2019-01-28T20:40:44-06:00</td>\n",
       "      <td>20190131</td>\n",
       "      <td>-21600</td>\n",
       "      <td>19.613165</td>\n",
       "      <td>-99.296402</td>\n",
       "      <td>2019-01-28 14:40:44</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556912</th>\n",
       "      <td>1127465074</td>\n",
       "      <td>2019-01-28T18:38:44-06:00</td>\n",
       "      <td>20190131</td>\n",
       "      <td>-21600</td>\n",
       "      <td>19.613171</td>\n",
       "      <td>-99.296399</td>\n",
       "      <td>2019-01-28 12:38:44</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556913</th>\n",
       "      <td>1454973565</td>\n",
       "      <td>2019-01-28T10:25:44-06:00</td>\n",
       "      <td>20190131</td>\n",
       "      <td>-21600</td>\n",
       "      <td>19.360675</td>\n",
       "      <td>-99.284703</td>\n",
       "      <td>2019-01-28 04:25:44</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556914</th>\n",
       "      <td>1123468044</td>\n",
       "      <td>2019-01-28T20:42:05-06:00</td>\n",
       "      <td>20190131</td>\n",
       "      <td>-21600</td>\n",
       "      <td>19.429335</td>\n",
       "      <td>-99.261565</td>\n",
       "      <td>2019-01-28 14:42:05</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556915</th>\n",
       "      <td>1123857925</td>\n",
       "      <td>2019-01-28T23:46:33-06:00</td>\n",
       "      <td>20190131</td>\n",
       "      <td>-21600</td>\n",
       "      <td>25.852660</td>\n",
       "      <td>-97.501724</td>\n",
       "      <td>2019-01-28 17:46:33</td>\n",
       "      <td>2019-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15556916 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cuebiq_id       event_zoned_datetime  processing_date  \\\n",
       "0          946507429  2019-01-28T05:54:03-06:00         20190128   \n",
       "1          998611644  2019-01-28T14:36:10-06:00         20190128   \n",
       "2          998611644  2019-01-28T08:20:16-06:00         20190128   \n",
       "3          998611644  2019-01-28T14:33:14-06:00         20190128   \n",
       "4          998611644  2019-01-28T08:16:44-06:00         20190128   \n",
       "...              ...                        ...              ...   \n",
       "15556911  1127465074  2019-01-28T20:40:44-06:00         20190131   \n",
       "15556912  1127465074  2019-01-28T18:38:44-06:00         20190131   \n",
       "15556913  1454973565  2019-01-28T10:25:44-06:00         20190131   \n",
       "15556914  1123468044  2019-01-28T20:42:05-06:00         20190131   \n",
       "15556915  1123857925  2019-01-28T23:46:33-06:00         20190131   \n",
       "\n",
       "          timezoneoffset_secs        lat         lng   event_datetime_utc  \\\n",
       "0                      -21600  20.602415 -105.232750  2019-01-27 23:54:03   \n",
       "1                      -21600  24.040835 -104.657184  2019-01-28 08:36:10   \n",
       "2                      -21600  24.038745 -104.652544  2019-01-28 02:20:16   \n",
       "3                      -21600  24.038745 -104.652544  2019-01-28 08:33:14   \n",
       "4                      -21600  24.042602 -104.654026  2019-01-28 02:16:44   \n",
       "...                       ...        ...         ...                  ...   \n",
       "15556911               -21600  19.613165  -99.296402  2019-01-28 14:40:44   \n",
       "15556912               -21600  19.613171  -99.296399  2019-01-28 12:38:44   \n",
       "15556913               -21600  19.360675  -99.284703  2019-01-28 04:25:44   \n",
       "15556914               -21600  19.429335  -99.261565  2019-01-28 14:42:05   \n",
       "15556915               -21600  25.852660  -97.501724  2019-01-28 17:46:33   \n",
       "\n",
       "         event_zoned_date  \n",
       "0              2019-01-28  \n",
       "1              2019-01-28  \n",
       "2              2019-01-28  \n",
       "3              2019-01-28  \n",
       "4              2019-01-28  \n",
       "...                   ...  \n",
       "15556911       2019-01-28  \n",
       "15556912       2019-01-28  \n",
       "15556913       2019-01-28  \n",
       "15556914       2019-01-28  \n",
       "15556915       2019-01-28  \n",
       "\n",
       "[15556916 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jovyan/Data/DL/MX/20190128_MX_pe_dl.csv')\n",
    "# df.sort_values('event_datetime_local')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14637f1-c053-4109-85b6-8e11937fee26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef6b328-2043-4041-98c0-176ff9e71d9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>timezoneoffset_secs</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_utc</th>\n",
       "      <th>event_zoned_date</th>\n",
       "      <th>event_datetime_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82761</th>\n",
       "      <td>1697829818</td>\n",
       "      <td>2019-01-06T00:00:00+08:00</td>\n",
       "      <td>20190105</td>\n",
       "      <td>28800</td>\n",
       "      <td>-5.141653</td>\n",
       "      <td>119.407306</td>\n",
       "      <td>2019-01-06 08:00:00</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62520</th>\n",
       "      <td>1703049111</td>\n",
       "      <td>2019-01-06T00:00:00+07:00</td>\n",
       "      <td>20190105</td>\n",
       "      <td>25200</td>\n",
       "      <td>3.461143</td>\n",
       "      <td>99.151084</td>\n",
       "      <td>2019-01-06 07:00:00</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114031</th>\n",
       "      <td>1557384850</td>\n",
       "      <td>2019-01-06T00:00:00+07:00</td>\n",
       "      <td>20190106</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.138829</td>\n",
       "      <td>106.819416</td>\n",
       "      <td>2019-01-06 07:00:00</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207691</th>\n",
       "      <td>1548262498</td>\n",
       "      <td>2019-01-06T00:00:00+07:00</td>\n",
       "      <td>20190105</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.242392</td>\n",
       "      <td>106.516978</td>\n",
       "      <td>2019-01-06 07:00:00</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114039</th>\n",
       "      <td>1557384850</td>\n",
       "      <td>2019-01-06T00:00:00+07:00</td>\n",
       "      <td>20190106</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.138829</td>\n",
       "      <td>106.819416</td>\n",
       "      <td>2019-01-06 07:00:00</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057405</th>\n",
       "      <td>1699038788</td>\n",
       "      <td>2019-01-06T23:59:59+07:00</td>\n",
       "      <td>20190106</td>\n",
       "      <td>25200</td>\n",
       "      <td>-3.066471</td>\n",
       "      <td>103.007785</td>\n",
       "      <td>2019-01-07 06:59:59</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453918</th>\n",
       "      <td>1507990846</td>\n",
       "      <td>2019-01-06T23:59:59+07:00</td>\n",
       "      <td>20190107</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.907427</td>\n",
       "      <td>107.597650</td>\n",
       "      <td>2019-01-07 06:59:59</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625121</th>\n",
       "      <td>1507778042</td>\n",
       "      <td>2019-01-06T23:59:59+07:00</td>\n",
       "      <td>20190106</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.367524</td>\n",
       "      <td>106.791107</td>\n",
       "      <td>2019-01-07 06:59:59</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161820</th>\n",
       "      <td>1509178306</td>\n",
       "      <td>2019-01-06T23:59:59+07:00</td>\n",
       "      <td>20190107</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.189438</td>\n",
       "      <td>106.753880</td>\n",
       "      <td>2019-01-07 06:59:59</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845965</th>\n",
       "      <td>1572296572</td>\n",
       "      <td>2019-01-06T23:59:59+07:00</td>\n",
       "      <td>20190106</td>\n",
       "      <td>25200</td>\n",
       "      <td>-7.759687</td>\n",
       "      <td>113.206132</td>\n",
       "      <td>2019-01-07 06:59:59</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6788764 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cuebiq_id       event_zoned_datetime  processing_date  \\\n",
       "82761    1697829818  2019-01-06T00:00:00+08:00         20190105   \n",
       "62520    1703049111  2019-01-06T00:00:00+07:00         20190105   \n",
       "2114031  1557384850  2019-01-06T00:00:00+07:00         20190106   \n",
       "207691   1548262498  2019-01-06T00:00:00+07:00         20190105   \n",
       "2114039  1557384850  2019-01-06T00:00:00+07:00         20190106   \n",
       "...             ...                        ...              ...   \n",
       "1057405  1699038788  2019-01-06T23:59:59+07:00         20190106   \n",
       "4453918  1507990846  2019-01-06T23:59:59+07:00         20190107   \n",
       "625121   1507778042  2019-01-06T23:59:59+07:00         20190106   \n",
       "4161820  1509178306  2019-01-06T23:59:59+07:00         20190107   \n",
       "1845965  1572296572  2019-01-06T23:59:59+07:00         20190106   \n",
       "\n",
       "         timezoneoffset_secs       lat         lng   event_datetime_utc  \\\n",
       "82761                  28800 -5.141653  119.407306  2019-01-06 08:00:00   \n",
       "62520                  25200  3.461143   99.151084  2019-01-06 07:00:00   \n",
       "2114031                25200 -6.138829  106.819416  2019-01-06 07:00:00   \n",
       "207691                 25200 -6.242392  106.516978  2019-01-06 07:00:00   \n",
       "2114039                25200 -6.138829  106.819416  2019-01-06 07:00:00   \n",
       "...                      ...       ...         ...                  ...   \n",
       "1057405                25200 -3.066471  103.007785  2019-01-07 06:59:59   \n",
       "4453918                25200 -6.907427  107.597650  2019-01-07 06:59:59   \n",
       "625121                 25200 -6.367524  106.791107  2019-01-07 06:59:59   \n",
       "4161820                25200 -6.189438  106.753880  2019-01-07 06:59:59   \n",
       "1845965                25200 -7.759687  113.206132  2019-01-07 06:59:59   \n",
       "\n",
       "        event_zoned_date event_datetime_local  \n",
       "82761         2019-01-06  2019-01-06 00:00:00  \n",
       "62520         2019-01-06  2019-01-06 00:00:00  \n",
       "2114031       2019-01-06  2019-01-06 00:00:00  \n",
       "207691        2019-01-06  2019-01-06 00:00:00  \n",
       "2114039       2019-01-06  2019-01-06 00:00:00  \n",
       "...                  ...                  ...  \n",
       "1057405       2019-01-06  2019-01-06 23:59:59  \n",
       "4453918       2019-01-06  2019-01-06 23:59:59  \n",
       "625121        2019-01-06  2019-01-06 23:59:59  \n",
       "4161820       2019-01-06  2019-01-06 23:59:59  \n",
       "1845965       2019-01-06  2019-01-06 23:59:59  \n",
       "\n",
       "[6788764 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jovyan/Data/DL/ID/20190106_ID_pe_dl.csv')\n",
    "df.sort_values('event_datetime_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd19b5f0-57ea-4202-8dbd-4e2d38df82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cuebiq_id', 'event_zoned_datetime', 'processing_date',\n",
       "       'timezoneoffset_secs', 'lat', 'lng', 'event_datetime_utc',\n",
       "       'event_zoned_date', 'event_datetime_local'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7e9336-f018-4b22-8d04-de74b765666f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>timezoneoffset_secs</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_utc</th>\n",
       "      <th>event_zoned_date</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>geohash_3</th>\n",
       "      <th>geohash_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1511752029</td>\n",
       "      <td>2019-01-06T16:55:43+07:00</td>\n",
       "      <td>20190101</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.603028</td>\n",
       "      <td>106.770312</td>\n",
       "      <td>2019-01-06 23:55:43</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 16:55:43</td>\n",
       "      <td>qqg</td>\n",
       "      <td>qqgfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1511752029</td>\n",
       "      <td>2019-01-06T16:54:44+07:00</td>\n",
       "      <td>20190101</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.602716</td>\n",
       "      <td>106.770154</td>\n",
       "      <td>2019-01-06 23:54:44</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 16:54:44</td>\n",
       "      <td>qqg</td>\n",
       "      <td>qqgfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1511752029</td>\n",
       "      <td>2019-01-06T14:27:40+07:00</td>\n",
       "      <td>20190101</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.602295</td>\n",
       "      <td>106.773553</td>\n",
       "      <td>2019-01-06 21:27:40</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 14:27:40</td>\n",
       "      <td>qqg</td>\n",
       "      <td>qqgfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1511752029</td>\n",
       "      <td>2019-01-06T14:27:40+07:00</td>\n",
       "      <td>20190101</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.602295</td>\n",
       "      <td>106.773553</td>\n",
       "      <td>2019-01-06 21:27:40</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 14:27:40</td>\n",
       "      <td>qqg</td>\n",
       "      <td>qqgfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1698288712</td>\n",
       "      <td>2019-01-06T21:49:38+07:00</td>\n",
       "      <td>20190102</td>\n",
       "      <td>25200</td>\n",
       "      <td>-6.932583</td>\n",
       "      <td>106.983620</td>\n",
       "      <td>2019-01-07 04:49:38</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 21:49:38</td>\n",
       "      <td>qqu</td>\n",
       "      <td>qqu0d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788759</th>\n",
       "      <td>1676928312</td>\n",
       "      <td>2019-01-06T14:17:36+07:00</td>\n",
       "      <td>20190205</td>\n",
       "      <td>25200</td>\n",
       "      <td>-4.568085</td>\n",
       "      <td>105.149007</td>\n",
       "      <td>2019-01-06 21:17:36</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 14:17:36</td>\n",
       "      <td>qr4</td>\n",
       "      <td>qr4y0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788760</th>\n",
       "      <td>1676928312</td>\n",
       "      <td>2019-01-06T15:09:11+07:00</td>\n",
       "      <td>20190205</td>\n",
       "      <td>25200</td>\n",
       "      <td>-4.568085</td>\n",
       "      <td>105.149007</td>\n",
       "      <td>2019-01-06 22:09:11</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 15:09:11</td>\n",
       "      <td>qr4</td>\n",
       "      <td>qr4y0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788761</th>\n",
       "      <td>1676928312</td>\n",
       "      <td>2019-01-06T16:22:10+07:00</td>\n",
       "      <td>20190205</td>\n",
       "      <td>25200</td>\n",
       "      <td>-4.568085</td>\n",
       "      <td>105.149007</td>\n",
       "      <td>2019-01-06 23:22:10</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 16:22:10</td>\n",
       "      <td>qr4</td>\n",
       "      <td>qr4y0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788762</th>\n",
       "      <td>1676928312</td>\n",
       "      <td>2019-01-06T14:09:05+07:00</td>\n",
       "      <td>20190205</td>\n",
       "      <td>25200</td>\n",
       "      <td>-4.567611</td>\n",
       "      <td>105.148847</td>\n",
       "      <td>2019-01-06 21:09:05</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 14:09:05</td>\n",
       "      <td>qr4</td>\n",
       "      <td>qr4y0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788763</th>\n",
       "      <td>1676928312</td>\n",
       "      <td>2019-01-06T16:26:49+07:00</td>\n",
       "      <td>20190205</td>\n",
       "      <td>25200</td>\n",
       "      <td>-4.566124</td>\n",
       "      <td>105.148082</td>\n",
       "      <td>2019-01-06 23:26:49</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>2019-01-06 16:26:49</td>\n",
       "      <td>qr4</td>\n",
       "      <td>qr4y0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6788764 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cuebiq_id       event_zoned_datetime  processing_date  \\\n",
       "0        1511752029  2019-01-06T16:55:43+07:00         20190101   \n",
       "1        1511752029  2019-01-06T16:54:44+07:00         20190101   \n",
       "2        1511752029  2019-01-06T14:27:40+07:00         20190101   \n",
       "3        1511752029  2019-01-06T14:27:40+07:00         20190101   \n",
       "4        1698288712  2019-01-06T21:49:38+07:00         20190102   \n",
       "...             ...                        ...              ...   \n",
       "6788759  1676928312  2019-01-06T14:17:36+07:00         20190205   \n",
       "6788760  1676928312  2019-01-06T15:09:11+07:00         20190205   \n",
       "6788761  1676928312  2019-01-06T16:22:10+07:00         20190205   \n",
       "6788762  1676928312  2019-01-06T14:09:05+07:00         20190205   \n",
       "6788763  1676928312  2019-01-06T16:26:49+07:00         20190205   \n",
       "\n",
       "         timezoneoffset_secs       lat         lng   event_datetime_utc  \\\n",
       "0                      25200 -6.603028  106.770312  2019-01-06 23:55:43   \n",
       "1                      25200 -6.602716  106.770154  2019-01-06 23:54:44   \n",
       "2                      25200 -6.602295  106.773553  2019-01-06 21:27:40   \n",
       "3                      25200 -6.602295  106.773553  2019-01-06 21:27:40   \n",
       "4                      25200 -6.932583  106.983620  2019-01-07 04:49:38   \n",
       "...                      ...       ...         ...                  ...   \n",
       "6788759                25200 -4.568085  105.149007  2019-01-06 21:17:36   \n",
       "6788760                25200 -4.568085  105.149007  2019-01-06 22:09:11   \n",
       "6788761                25200 -4.568085  105.149007  2019-01-06 23:22:10   \n",
       "6788762                25200 -4.567611  105.148847  2019-01-06 21:09:05   \n",
       "6788763                25200 -4.566124  105.148082  2019-01-06 23:26:49   \n",
       "\n",
       "        event_zoned_date event_datetime_local geohash_3 geohash_5  \n",
       "0             2019-01-06  2019-01-06 16:55:43       qqg     qqgfm  \n",
       "1             2019-01-06  2019-01-06 16:54:44       qqg     qqgfm  \n",
       "2             2019-01-06  2019-01-06 14:27:40       qqg     qqgfm  \n",
       "3             2019-01-06  2019-01-06 14:27:40       qqg     qqgfm  \n",
       "4             2019-01-06  2019-01-06 21:49:38       qqu     qqu0d  \n",
       "...                  ...                  ...       ...       ...  \n",
       "6788759       2019-01-06  2019-01-06 14:17:36       qr4     qr4y0  \n",
       "6788760       2019-01-06  2019-01-06 15:09:11       qr4     qr4y0  \n",
       "6788761       2019-01-06  2019-01-06 16:22:10       qr4     qr4y0  \n",
       "6788762       2019-01-06  2019-01-06 14:09:05       qr4     qr4y0  \n",
       "6788763       2019-01-06  2019-01-06 16:26:49       qr4     qr4y0  \n",
       "\n",
       "[6788764 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geohash\n",
    "\n",
    "# Function to calculate geohash at specified precision\n",
    "def calculate_geohash(lat, lng, precision):\n",
    "    return geohash.encode(lat, lng, precision=precision)\n",
    "\n",
    "# Add geohash columns\n",
    "df['geohash_3'] = df.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 3), axis=1)\n",
    "df['geohash_5'] = df.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 5), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4c6a52-36a1-401d-8417-7ad2a89e8756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_3</th>\n",
       "      <th>point_number</th>\n",
       "      <th>user_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpx</td>\n",
       "      <td>1212</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qpz</td>\n",
       "      <td>44699</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qqe</td>\n",
       "      <td>1695</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qqg</td>\n",
       "      <td>2308194</td>\n",
       "      <td>24568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qqs</td>\n",
       "      <td>45030</td>\n",
       "      <td>1026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>wb3</td>\n",
       "      <td>1005</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>wb4</td>\n",
       "      <td>2998</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>wb5</td>\n",
       "      <td>262</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>wb7</td>\n",
       "      <td>462</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>wb9</td>\n",
       "      <td>611</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    geohash_3  point_number  user_number\n",
       "0         qpx          1212           41\n",
       "1         qpz         44699          843\n",
       "2         qqe          1695           48\n",
       "3         qqg       2308194        24568\n",
       "4         qqs         45030         1026\n",
       "..        ...           ...          ...\n",
       "131       wb3          1005           45\n",
       "132       wb4          2998           98\n",
       "133       wb5           262           14\n",
       "134       wb7           462           21\n",
       "135       wb9           611           18\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by `geohash_3` and aggregate data\n",
    "aggregated_df3 = df.groupby('geohash_3').agg(\n",
    "    point_number=('geohash_3', 'size'),\n",
    "    user_number=('cuebiq_id', 'nunique')\n",
    ").reset_index()\n",
    "filtered_df3 = aggregated_df3[aggregated_df3['user_number'] > 10].reset_index(drop=True)\n",
    "\n",
    "filtered_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1ad16d-4245-4dcd-95a4-a229b2c5e688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash_5</th>\n",
       "      <th>point_number</th>\n",
       "      <th>user_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpz6e</td>\n",
       "      <td>5872</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qpz6g</td>\n",
       "      <td>4001</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpz6h</td>\n",
       "      <td>247</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qpz6k</td>\n",
       "      <td>901</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qpz6s</td>\n",
       "      <td>4944</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>wb300</td>\n",
       "      <td>597</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>wb4s6</td>\n",
       "      <td>1032</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>wb4sd</td>\n",
       "      <td>898</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>wb71b</td>\n",
       "      <td>243</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>wb9hx</td>\n",
       "      <td>431</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2967 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     geohash_5  point_number  user_number\n",
       "0        qpz6e          5872          175\n",
       "1        qpz6g          4001          147\n",
       "2        qpz6h           247           14\n",
       "3        qpz6k           901           50\n",
       "4        qpz6s          4944          183\n",
       "...        ...           ...          ...\n",
       "2962     wb300           597           35\n",
       "2963     wb4s6          1032           55\n",
       "2964     wb4sd           898           49\n",
       "2965     wb71b           243           14\n",
       "2966     wb9hx           431           13\n",
       "\n",
       "[2967 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by `geohash_5` and aggregate data\n",
    "aggregated_df5 = df.groupby('geohash_5').agg(\n",
    "    point_number=('geohash_5', 'size'),\n",
    "    user_number=('cuebiq_id', 'nunique')\n",
    ").reset_index()\n",
    "filtered_df5 = aggregated_df5[aggregated_df5['user_number'] > 10].reset_index(drop=True)\n",
    "\n",
    "filtered_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "621d98d6-8386-4e49-80ac-b826d320866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3.to_csv(f'/home/jovyan/Data/Agg_DL/ID3/20190106_ID_pe_dl3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725af972-13ed-40d0-9a29-e76881060bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df5.to_csv(f'/home/jovyan/Data/Agg_DL/ID5/20190106_ID_pe_dl5.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a823e43-b484-458f-9211-623cd5f71dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916651d3-930a-4c8d-806e-294aab0c6222",
   "metadata": {},
   "source": [
    "# By country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055be9f5-0dfc-4ab5-aad4-af6a05d252f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 20190623_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190704_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190308_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190227_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190506_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190425_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190326_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190407_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190128_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190209_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190524_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190605_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190713_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190317_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190119_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190515_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190416_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190218_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190614_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190111_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190210_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190201_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190120_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190102_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190429_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190708_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190627_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190528_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190609_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190519_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190618_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190115_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190511_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190430_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190313_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190214_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190610_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190331_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190412_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190205_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190124_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190601_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190520_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190403_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190322_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190223_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190304_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190421_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190502_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190106_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190526_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190607_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190328_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190409_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190706_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190625_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190508_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190427_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190616_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190418_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190319_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190517_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190131_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190212_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190410_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190311_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190113_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190221_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190302_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190104_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190401_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190320_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190203_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190122_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190629_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190414_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190531_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190612_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190216_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190513_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190117_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190711_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190630_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190315_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190423_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190504_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190108_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190621_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190702_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190306_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190225_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190603_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190522_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190126_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190207_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190324_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190405_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190417_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190219_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190615_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190516_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190318_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190426_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190507_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190228_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190309_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190624_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190705_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190129_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190606_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190525_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190408_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190327_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190103_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190301_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190220_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190121_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190202_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190112_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190310_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190211_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190130_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190619_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190628_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190709_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190529_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190305_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190224_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190701_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190620_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190503_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190422_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190107_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190323_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190404_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190521_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190602_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190206_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190125_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190314_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190710_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190512_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190116_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190413_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190611_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190530_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190215_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190518_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190419_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190617_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190329_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190608_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190527_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190428_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190509_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190626_ID_pe_dl.csv to respective folders\n",
      "Processed and saved 20190707_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190321_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190402_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190123_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190204_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190105_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190501_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190420_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190303_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190222_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190411_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190330_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190213_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190114_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190510_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190312_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190101_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190110_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190523_ID_pe_dl.csv to respective folders\n",
      "Processed and saved 20190604_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190208_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190127_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190406_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190325_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190703_ID_pe_dl.csv to respective folders\n",
      "Processed and saved 20190622_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190226_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190307_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190109_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190424_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190505_ID_pe_dl.csv to respective folders\n",
      "Processed and saved 20190613_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190217_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190415_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190712_ID_pe_dl.csv to respective folders\n",
      "Skipped 20190316_ID_pe_dl.csv as both output files already exist\n",
      "Skipped 20190118_ID_pe_dl.csv as both output files already exist\n",
      "Processed and saved 20190514_ID_pe_dl.csv to respective folders\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geohash\n",
    "\n",
    "# Function to calculate geohash at specified precision\n",
    "def calculate_geohash(lat, lng, precision):\n",
    "    return geohash.encode(lat, lng, precision=precision)\n",
    "\n",
    "# Directory paths\n",
    "input_folder = '/home/jovyan/Data/Test_DL/ID/'\n",
    "output_folder_3 = '/home/jovyan/Data/Agg_DL/ID3/'\n",
    "output_folder_5 = '/home/jovyan/Data/Agg_DL/ID5/'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_folder_3, exist_ok=True)\n",
    "os.makedirs(output_folder_5, exist_ok=True)\n",
    "\n",
    "# Chunk size for reading and processing\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        # Define output file paths\n",
    "        output_file_3 = os.path.join(output_folder_3, csv_file.replace('.csv', '_agg3.csv'))\n",
    "        output_file_5 = os.path.join(output_folder_5, csv_file.replace('.csv', '_agg5.csv'))\n",
    "        \n",
    "        # Check if either of the output files does not exist\n",
    "        if not os.path.exists(output_file_3) or not os.path.exists(output_file_5):\n",
    "            # Initialize lists to collect chunks\n",
    "            chunks3 = []\n",
    "            chunks5 = []\n",
    "            \n",
    "            # Read the CSV file in chunks\n",
    "            for chunk in pd.read_csv(os.path.join(input_folder, csv_file), chunksize=chunk_size):\n",
    "                # Sort the chunk by 'event_datetime_local'\n",
    "                chunk.sort_values('event_datetime_local', inplace=True)\n",
    "                \n",
    "                # Add geohash columns\n",
    "                chunk['geohash_3'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 3), axis=1)\n",
    "                chunk['geohash_5'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 5), axis=1)\n",
    "                \n",
    "                # Append processed chunk to the lists\n",
    "                chunks3.append(chunk[['geohash_3', 'cuebiq_id']])\n",
    "                chunks5.append(chunk[['geohash_5', 'cuebiq_id']])\n",
    "            \n",
    "            # Concatenate all chunks\n",
    "            df3 = pd.concat(chunks3)\n",
    "            df5 = pd.concat(chunks5)\n",
    "            \n",
    "            # Group by `geohash_3` and aggregate data\n",
    "            aggregated_df3 = df3.groupby('geohash_3').agg(\n",
    "                point_number=('geohash_3', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df3 = aggregated_df3[aggregated_df3['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 3\n",
    "            filtered_df3.to_csv(output_file_3, index=False)\n",
    "            \n",
    "            # Group by `geohash_5` and aggregate data\n",
    "            aggregated_df5 = df5.groupby('geohash_5').agg(\n",
    "                point_number=('geohash_5', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df5 = aggregated_df5[aggregated_df5['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 5\n",
    "            filtered_df5.to_csv(output_file_5, index=False)\n",
    "            \n",
    "            print(f\"Processed and saved {csv_file} to respective folders\")\n",
    "        else:\n",
    "            print(f\"Skipped {csv_file} as both output files already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b722db8-ed48-4fcc-9b7e-180dc72394fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 20190126_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190108_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190117_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190122_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190104_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190131_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190113_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190102_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190120_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190111_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190128_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190119_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190106_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190124_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190115_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190110_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190101_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190118_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190127_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190109_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190114_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190123_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190105_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190130_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190112_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190103_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190121_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190129_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190116_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190107_IN_pe_dl.csv as both output files already exist\n",
      "Skipped 20190125_IN_pe_dl.csv as both output files already exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geohash\n",
    "\n",
    "# Function to calculate geohash at specified precision\n",
    "def calculate_geohash(lat, lng, precision):\n",
    "    return geohash.encode(lat, lng, precision=precision)\n",
    "\n",
    "# Directory paths\n",
    "input_folder = '/home/jovyan/Data/DL/IN/'\n",
    "output_folder_3 = '/home/jovyan/Data/Agg_DL/IN3/'\n",
    "output_folder_5 = '/home/jovyan/Data/Agg_DL/IN5/'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_folder_3, exist_ok=True)\n",
    "os.makedirs(output_folder_5, exist_ok=True)\n",
    "\n",
    "# Chunk size for reading and processing\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        # Define output file paths\n",
    "        output_file_3 = os.path.join(output_folder_3, csv_file.replace('.csv', '_agg3.csv'))\n",
    "        output_file_5 = os.path.join(output_folder_5, csv_file.replace('.csv', '_agg5.csv'))\n",
    "        \n",
    "        # Check if either of the output files does not exist\n",
    "        if not os.path.exists(output_file_3) or not os.path.exists(output_file_5):\n",
    "            # Initialize lists to collect chunks\n",
    "            chunks3 = []\n",
    "            chunks5 = []\n",
    "            \n",
    "            # Read the CSV file in chunks\n",
    "            for chunk in pd.read_csv(os.path.join(input_folder, csv_file), chunksize=chunk_size):\n",
    "                # Sort the chunk by 'event_datetime_local'\n",
    "                chunk.sort_values('event_datetime_local', inplace=True)\n",
    "                \n",
    "                # Add geohash columns\n",
    "                chunk['geohash_3'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 3), axis=1)\n",
    "                chunk['geohash_5'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 5), axis=1)\n",
    "                \n",
    "                # Append processed chunk to the lists\n",
    "                chunks3.append(chunk[['geohash_3', 'cuebiq_id']])\n",
    "                chunks5.append(chunk[['geohash_5', 'cuebiq_id']])\n",
    "            \n",
    "            # Concatenate all chunks\n",
    "            df3 = pd.concat(chunks3)\n",
    "            df5 = pd.concat(chunks5)\n",
    "            \n",
    "            # Group by `geohash_3` and aggregate data\n",
    "            aggregated_df3 = df3.groupby('geohash_3').agg(\n",
    "                point_number=('geohash_3', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df3 = aggregated_df3[aggregated_df3['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 3\n",
    "            filtered_df3.to_csv(output_file_3, index=False)\n",
    "            \n",
    "            # Group by `geohash_5` and aggregate data\n",
    "            aggregated_df5 = df5.groupby('geohash_5').agg(\n",
    "                point_number=('geohash_5', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df5 = aggregated_df5[aggregated_df5['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 5\n",
    "            filtered_df5.to_csv(output_file_5, index=False)\n",
    "            \n",
    "            print(f\"Processed and saved {csv_file} to respective folders\")\n",
    "        else:\n",
    "            print(f\"Skipped {csv_file} as both output files already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4de90-c5e3-4dfa-b85e-9806585d7de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d287b64-c1ef-47e5-bb1c-84425430db4b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190623_ID_pe_dl.csv\n",
      "Skipped 20190623_ID_pe_dl.csv as both output files already exist\n",
      "20190704_ID_pe_dl.csv\n",
      "Skipped 20190704_ID_pe_dl.csv as both output files already exist\n",
      "20190308_ID_pe_dl.csv\n",
      "Skipped 20190308_ID_pe_dl.csv as both output files already exist\n",
      "20190227_ID_pe_dl.csv\n",
      "Skipped 20190227_ID_pe_dl.csv as both output files already exist\n",
      "20190506_ID_pe_dl.csv\n",
      "Skipped 20190506_ID_pe_dl.csv as both output files already exist\n",
      "20190425_ID_pe_dl.csv\n",
      "Skipped 20190425_ID_pe_dl.csv as both output files already exist\n",
      "20190722_ID_pe_dl.csv\n",
      "Processed and saved 20190722_ID_pe_dl.csv to respective folders\n",
      "20190326_ID_pe_dl.csv\n",
      "Skipped 20190326_ID_pe_dl.csv as both output files already exist\n",
      "20190407_ID_pe_dl.csv\n",
      "Skipped 20190407_ID_pe_dl.csv as both output files already exist\n",
      "20190128_ID_pe_dl.csv\n",
      "Skipped 20190128_ID_pe_dl.csv as both output files already exist\n",
      "20190209_ID_pe_dl.csv\n",
      "Skipped 20190209_ID_pe_dl.csv as both output files already exist\n",
      "20190524_ID_pe_dl.csv\n",
      "Skipped 20190524_ID_pe_dl.csv as both output files already exist\n",
      "20190605_ID_pe_dl.csv\n",
      "Skipped 20190605_ID_pe_dl.csv as both output files already exist\n",
      "20190713_ID_pe_dl.csv\n",
      "Skipped 20190713_ID_pe_dl.csv as both output files already exist\n",
      "20190317_ID_pe_dl.csv\n",
      "Skipped 20190317_ID_pe_dl.csv as both output files already exist\n",
      "20190119_ID_pe_dl.csv\n",
      "Skipped 20190119_ID_pe_dl.csv as both output files already exist\n",
      "20190515_ID_pe_dl.csv\n",
      "Skipped 20190515_ID_pe_dl.csv as both output files already exist\n",
      "20190731_ID_pe_dl.csv\n",
      "Processed and saved 20190731_ID_pe_dl.csv to respective folders\n",
      "20190416_ID_pe_dl.csv\n",
      "Skipped 20190416_ID_pe_dl.csv as both output files already exist\n",
      "20190218_ID_pe_dl.csv\n",
      "Skipped 20190218_ID_pe_dl.csv as both output files already exist\n",
      "20190614_ID_pe_dl.csv\n",
      "Skipped 20190614_ID_pe_dl.csv as both output files already exist\n",
      "20190111_ID_pe_dl.csv\n",
      "Skipped 20190111_ID_pe_dl.csv as both output files already exist\n",
      "20190210_ID_pe_dl.csv\n",
      "Skipped 20190210_ID_pe_dl.csv as both output files already exist\n",
      "20190201_ID_pe_dl.csv\n",
      "Skipped 20190201_ID_pe_dl.csv as both output files already exist\n",
      "20190120_ID_pe_dl.csv\n",
      "Skipped 20190120_ID_pe_dl.csv as both output files already exist\n",
      "20190102_ID_pe_dl.csv\n",
      "Skipped 20190102_ID_pe_dl.csv as both output files already exist\n",
      "20190429_ID_pe_dl.csv\n",
      "Skipped 20190429_ID_pe_dl.csv as both output files already exist\n",
      "20190708_ID_pe_dl.csv\n",
      "Skipped 20190708_ID_pe_dl.csv as both output files already exist\n",
      "20190627_ID_pe_dl.csv\n",
      "Skipped 20190627_ID_pe_dl.csv as both output files already exist\n",
      "20190528_ID_pe_dl.csv\n",
      "Skipped 20190528_ID_pe_dl.csv as both output files already exist\n",
      "20190609_ID_pe_dl.csv\n",
      "Skipped 20190609_ID_pe_dl.csv as both output files already exist\n",
      "20190726_ID_pe_dl.csv\n",
      "Processed and saved 20190726_ID_pe_dl.csv to respective folders\n",
      "20190519_ID_pe_dl.csv\n",
      "Skipped 20190519_ID_pe_dl.csv as both output files already exist\n",
      "20190717_ID_pe_dl.csv\n",
      "Processed and saved 20190717_ID_pe_dl.csv to respective folders\n",
      "20190618_ID_pe_dl.csv\n",
      "Skipped 20190618_ID_pe_dl.csv as both output files already exist\n",
      "20190115_ID_pe_dl.csv\n",
      "Skipped 20190115_ID_pe_dl.csv as both output files already exist\n",
      "20190511_ID_pe_dl.csv\n",
      "Skipped 20190511_ID_pe_dl.csv as both output files already exist\n",
      "20190430_ID_pe_dl.csv\n",
      "Skipped 20190430_ID_pe_dl.csv as both output files already exist\n",
      "20190313_ID_pe_dl.csv\n",
      "Skipped 20190313_ID_pe_dl.csv as both output files already exist\n",
      "20190214_ID_pe_dl.csv\n",
      "Skipped 20190214_ID_pe_dl.csv as both output files already exist\n",
      "20190610_ID_pe_dl.csv\n",
      "Skipped 20190610_ID_pe_dl.csv as both output files already exist\n",
      "20190331_ID_pe_dl.csv\n",
      "Skipped 20190331_ID_pe_dl.csv as both output files already exist\n",
      "20190412_ID_pe_dl.csv\n",
      "Skipped 20190412_ID_pe_dl.csv as both output files already exist\n",
      "20190205_ID_pe_dl.csv\n",
      "Skipped 20190205_ID_pe_dl.csv as both output files already exist\n",
      "20190124_ID_pe_dl.csv\n",
      "Skipped 20190124_ID_pe_dl.csv as both output files already exist\n",
      "20190601_ID_pe_dl.csv\n",
      "Skipped 20190601_ID_pe_dl.csv as both output files already exist\n",
      "20190520_ID_pe_dl.csv\n",
      "Skipped 20190520_ID_pe_dl.csv as both output files already exist\n",
      "20190403_ID_pe_dl.csv\n",
      "Skipped 20190403_ID_pe_dl.csv as both output files already exist\n",
      "20190322_ID_pe_dl.csv\n",
      "Skipped 20190322_ID_pe_dl.csv as both output files already exist\n",
      "20190223_ID_pe_dl.csv\n",
      "Skipped 20190223_ID_pe_dl.csv as both output files already exist\n",
      "20190304_ID_pe_dl.csv\n",
      "Skipped 20190304_ID_pe_dl.csv as both output files already exist\n",
      "20190421_ID_pe_dl.csv\n",
      "Skipped 20190421_ID_pe_dl.csv as both output files already exist\n",
      "20190502_ID_pe_dl.csv\n",
      "Skipped 20190502_ID_pe_dl.csv as both output files already exist\n",
      "20190106_ID_pe_dl.csv\n",
      "Skipped 20190106_ID_pe_dl.csv as both output files already exist\n",
      "20190526_ID_pe_dl.csv\n",
      "Skipped 20190526_ID_pe_dl.csv as both output files already exist\n",
      "20190607_ID_pe_dl.csv\n",
      "Skipped 20190607_ID_pe_dl.csv as both output files already exist\n",
      "20190328_ID_pe_dl.csv\n",
      "Skipped 20190328_ID_pe_dl.csv as both output files already exist\n",
      "20190409_ID_pe_dl.csv\n",
      "Skipped 20190409_ID_pe_dl.csv as both output files already exist\n",
      "20190724_ID_pe_dl.csv\n",
      "Processed and saved 20190724_ID_pe_dl.csv to respective folders\n",
      "20190706_ID_pe_dl.csv\n",
      "Skipped 20190706_ID_pe_dl.csv as both output files already exist\n",
      "20190625_ID_pe_dl.csv\n",
      "Skipped 20190625_ID_pe_dl.csv as both output files already exist\n",
      "20190508_ID_pe_dl.csv\n",
      "Skipped 20190508_ID_pe_dl.csv as both output files already exist\n",
      "20190427_ID_pe_dl.csv\n",
      "Skipped 20190427_ID_pe_dl.csv as both output files already exist\n",
      "20190616_ID_pe_dl.csv\n",
      "Skipped 20190616_ID_pe_dl.csv as both output files already exist\n",
      "20190418_ID_pe_dl.csv\n",
      "Skipped 20190418_ID_pe_dl.csv as both output files already exist\n",
      "20190319_ID_pe_dl.csv\n",
      "Skipped 20190319_ID_pe_dl.csv as both output files already exist\n",
      "20190715_ID_pe_dl.csv\n",
      "Processed and saved 20190715_ID_pe_dl.csv to respective folders\n",
      "20190517_ID_pe_dl.csv\n",
      "Skipped 20190517_ID_pe_dl.csv as both output files already exist\n",
      "20190131_ID_pe_dl.csv\n",
      "Skipped 20190131_ID_pe_dl.csv as both output files already exist\n",
      "20190212_ID_pe_dl.csv\n",
      "Skipped 20190212_ID_pe_dl.csv as both output files already exist\n",
      "20190410_ID_pe_dl.csv\n",
      "Skipped 20190410_ID_pe_dl.csv as both output files already exist\n",
      "20190311_ID_pe_dl.csv\n",
      "Skipped 20190311_ID_pe_dl.csv as both output files already exist\n",
      "20190113_ID_pe_dl.csv\n",
      "Skipped 20190113_ID_pe_dl.csv as both output files already exist\n",
      "20190221_ID_pe_dl.csv\n",
      "Skipped 20190221_ID_pe_dl.csv as both output files already exist\n",
      "20190302_ID_pe_dl.csv\n",
      "Skipped 20190302_ID_pe_dl.csv as both output files already exist\n",
      "20190104_ID_pe_dl.csv\n",
      "Skipped 20190104_ID_pe_dl.csv as both output files already exist\n",
      "20190401_ID_pe_dl.csv\n",
      "Skipped 20190401_ID_pe_dl.csv as both output files already exist\n",
      "20190320_ID_pe_dl.csv\n",
      "Skipped 20190320_ID_pe_dl.csv as both output files already exist\n",
      "20190203_ID_pe_dl.csv\n",
      "Skipped 20190203_ID_pe_dl.csv as both output files already exist\n",
      "20190122_ID_pe_dl.csv\n",
      "Skipped 20190122_ID_pe_dl.csv as both output files already exist\n",
      "20190728_ID_pe_dl.csv\n",
      "Processed and saved 20190728_ID_pe_dl.csv to respective folders\n",
      "20190629_ID_pe_dl.csv\n",
      "Skipped 20190629_ID_pe_dl.csv as both output files already exist\n",
      "20190719_ID_pe_dl.csv\n",
      "Processed and saved 20190719_ID_pe_dl.csv to respective folders\n",
      "20190414_ID_pe_dl.csv\n",
      "Skipped 20190414_ID_pe_dl.csv as both output files already exist\n",
      "20190531_ID_pe_dl.csv\n",
      "Skipped 20190531_ID_pe_dl.csv as both output files already exist\n",
      "20190612_ID_pe_dl.csv\n",
      "Skipped 20190612_ID_pe_dl.csv as both output files already exist\n",
      "20190216_ID_pe_dl.csv\n",
      "Skipped 20190216_ID_pe_dl.csv as both output files already exist\n",
      "20190513_ID_pe_dl.csv\n",
      "Skipped 20190513_ID_pe_dl.csv as both output files already exist\n",
      "20190117_ID_pe_dl.csv\n",
      "Skipped 20190117_ID_pe_dl.csv as both output files already exist\n",
      "20190711_ID_pe_dl.csv\n",
      "Skipped 20190711_ID_pe_dl.csv as both output files already exist\n",
      "20190630_ID_pe_dl.csv\n",
      "Skipped 20190630_ID_pe_dl.csv as both output files already exist\n",
      "20190315_ID_pe_dl.csv\n",
      "Skipped 20190315_ID_pe_dl.csv as both output files already exist\n",
      "20190423_ID_pe_dl.csv\n",
      "Skipped 20190423_ID_pe_dl.csv as both output files already exist\n",
      "20190504_ID_pe_dl.csv\n",
      "Skipped 20190504_ID_pe_dl.csv as both output files already exist\n",
      "20190108_ID_pe_dl.csv\n",
      "Skipped 20190108_ID_pe_dl.csv as both output files already exist\n",
      "20190621_ID_pe_dl.csv\n",
      "Skipped 20190621_ID_pe_dl.csv as both output files already exist\n",
      "20190702_ID_pe_dl.csv\n",
      "Skipped 20190702_ID_pe_dl.csv as both output files already exist\n",
      "20190306_ID_pe_dl.csv\n",
      "Skipped 20190306_ID_pe_dl.csv as both output files already exist\n",
      "20190225_ID_pe_dl.csv\n",
      "Skipped 20190225_ID_pe_dl.csv as both output files already exist\n",
      "20190603_ID_pe_dl.csv\n",
      "Skipped 20190603_ID_pe_dl.csv as both output files already exist\n",
      "20190522_ID_pe_dl.csv\n",
      "Skipped 20190522_ID_pe_dl.csv as both output files already exist\n",
      "20190126_ID_pe_dl.csv\n",
      "Skipped 20190126_ID_pe_dl.csv as both output files already exist\n",
      "20190207_ID_pe_dl.csv\n",
      "Skipped 20190207_ID_pe_dl.csv as both output files already exist\n",
      "20190720_ID_pe_dl.csv\n",
      "Processed and saved 20190720_ID_pe_dl.csv to respective folders\n",
      "20190324_ID_pe_dl.csv\n",
      "Skipped 20190324_ID_pe_dl.csv as both output files already exist\n",
      "20190405_ID_pe_dl.csv\n",
      "Skipped 20190405_ID_pe_dl.csv as both output files already exist\n",
      "20190417_ID_pe_dl.csv\n",
      "Skipped 20190417_ID_pe_dl.csv as both output files already exist\n",
      "20190219_ID_pe_dl.csv\n",
      "Skipped 20190219_ID_pe_dl.csv as both output files already exist\n",
      "20190615_ID_pe_dl.csv\n",
      "Skipped 20190615_ID_pe_dl.csv as both output files already exist\n",
      "20190516_ID_pe_dl.csv\n",
      "Skipped 20190516_ID_pe_dl.csv as both output files already exist\n",
      "20190318_ID_pe_dl.csv\n",
      "Skipped 20190318_ID_pe_dl.csv as both output files already exist\n",
      "20190714_ID_pe_dl.csv\n",
      "Processed and saved 20190714_ID_pe_dl.csv to respective folders\n",
      "20190426_ID_pe_dl.csv\n",
      "Skipped 20190426_ID_pe_dl.csv as both output files already exist\n",
      "20190507_ID_pe_dl.csv\n",
      "Skipped 20190507_ID_pe_dl.csv as both output files already exist\n",
      "20190228_ID_pe_dl.csv\n",
      "Skipped 20190228_ID_pe_dl.csv as both output files already exist\n",
      "20190309_ID_pe_dl.csv\n",
      "Skipped 20190309_ID_pe_dl.csv as both output files already exist\n",
      "20190624_ID_pe_dl.csv\n",
      "Skipped 20190624_ID_pe_dl.csv as both output files already exist\n",
      "20190705_ID_pe_dl.csv\n",
      "Skipped 20190705_ID_pe_dl.csv as both output files already exist\n",
      "20190129_ID_pe_dl.csv\n",
      "Skipped 20190129_ID_pe_dl.csv as both output files already exist\n",
      "20190606_ID_pe_dl.csv\n",
      "Skipped 20190606_ID_pe_dl.csv as both output files already exist\n",
      "20190525_ID_pe_dl.csv\n",
      "Skipped 20190525_ID_pe_dl.csv as both output files already exist\n",
      "20190408_ID_pe_dl.csv\n",
      "Skipped 20190408_ID_pe_dl.csv as both output files already exist\n",
      "20190327_ID_pe_dl.csv\n",
      "Skipped 20190327_ID_pe_dl.csv as both output files already exist\n",
      "20190723_ID_pe_dl.csv\n",
      "Processed and saved 20190723_ID_pe_dl.csv to respective folders\n",
      "20190103_ID_pe_dl.csv\n",
      "Skipped 20190103_ID_pe_dl.csv as both output files already exist\n",
      "20190301_ID_pe_dl.csv\n",
      "Skipped 20190301_ID_pe_dl.csv as both output files already exist\n",
      "20190220_ID_pe_dl.csv\n",
      "Skipped 20190220_ID_pe_dl.csv as both output files already exist\n",
      "20190121_ID_pe_dl.csv\n",
      "Skipped 20190121_ID_pe_dl.csv as both output files already exist\n",
      "20190202_ID_pe_dl.csv\n",
      "Skipped 20190202_ID_pe_dl.csv as both output files already exist\n",
      "20190112_ID_pe_dl.csv\n",
      "Skipped 20190112_ID_pe_dl.csv as both output files already exist\n",
      "20190310_ID_pe_dl.csv\n",
      "Skipped 20190310_ID_pe_dl.csv as both output files already exist\n",
      "20190211_ID_pe_dl.csv\n",
      "Skipped 20190211_ID_pe_dl.csv as both output files already exist\n",
      "20190130_ID_pe_dl.csv\n",
      "Skipped 20190130_ID_pe_dl.csv as both output files already exist\n",
      "20190619_ID_pe_dl.csv\n",
      "Skipped 20190619_ID_pe_dl.csv as both output files already exist\n",
      "20190718_ID_pe_dl.csv\n",
      "Processed and saved 20190718_ID_pe_dl.csv to respective folders\n",
      "20190628_ID_pe_dl.csv\n",
      "Skipped 20190628_ID_pe_dl.csv as both output files already exist\n",
      "20190709_ID_pe_dl.csv\n",
      "Skipped 20190709_ID_pe_dl.csv as both output files already exist\n",
      "20190727_ID_pe_dl.csv\n",
      "Processed and saved 20190727_ID_pe_dl.csv to respective folders\n",
      "20190529_ID_pe_dl.csv\n",
      "Skipped 20190529_ID_pe_dl.csv as both output files already exist\n",
      "20190305_ID_pe_dl.csv\n",
      "Skipped 20190305_ID_pe_dl.csv as both output files already exist\n",
      "20190224_ID_pe_dl.csv\n",
      "Skipped 20190224_ID_pe_dl.csv as both output files already exist\n",
      "20190701_ID_pe_dl.csv\n",
      "Skipped 20190701_ID_pe_dl.csv as both output files already exist\n",
      "20190620_ID_pe_dl.csv\n",
      "Skipped 20190620_ID_pe_dl.csv as both output files already exist\n",
      "20190503_ID_pe_dl.csv\n",
      "Skipped 20190503_ID_pe_dl.csv as both output files already exist\n",
      "20190422_ID_pe_dl.csv\n",
      "Skipped 20190422_ID_pe_dl.csv as both output files already exist\n",
      "20190107_ID_pe_dl.csv\n",
      "Skipped 20190107_ID_pe_dl.csv as both output files already exist\n",
      "20190323_ID_pe_dl.csv\n",
      "Skipped 20190323_ID_pe_dl.csv as both output files already exist\n",
      "20190404_ID_pe_dl.csv\n",
      "Skipped 20190404_ID_pe_dl.csv as both output files already exist\n",
      "20190521_ID_pe_dl.csv\n",
      "Skipped 20190521_ID_pe_dl.csv as both output files already exist\n",
      "20190602_ID_pe_dl.csv\n",
      "Skipped 20190602_ID_pe_dl.csv as both output files already exist\n",
      "20190206_ID_pe_dl.csv\n",
      "Skipped 20190206_ID_pe_dl.csv as both output files already exist\n",
      "20190125_ID_pe_dl.csv\n",
      "Skipped 20190125_ID_pe_dl.csv as both output files already exist\n",
      "20190314_ID_pe_dl.csv\n",
      "Skipped 20190314_ID_pe_dl.csv as both output files already exist\n",
      "20190710_ID_pe_dl.csv\n",
      "Skipped 20190710_ID_pe_dl.csv as both output files already exist\n",
      "20190512_ID_pe_dl.csv\n",
      "Skipped 20190512_ID_pe_dl.csv as both output files already exist\n",
      "20190116_ID_pe_dl.csv\n",
      "Skipped 20190116_ID_pe_dl.csv as both output files already exist\n",
      "20190413_ID_pe_dl.csv\n",
      "Skipped 20190413_ID_pe_dl.csv as both output files already exist\n",
      "20190611_ID_pe_dl.csv\n",
      "Skipped 20190611_ID_pe_dl.csv as both output files already exist\n",
      "20190530_ID_pe_dl.csv\n",
      "Skipped 20190530_ID_pe_dl.csv as both output files already exist\n",
      "20190215_ID_pe_dl.csv\n",
      "Skipped 20190215_ID_pe_dl.csv as both output files already exist\n",
      "20190716_ID_pe_dl.csv\n",
      "Processed and saved 20190716_ID_pe_dl.csv to respective folders\n",
      "20190518_ID_pe_dl.csv\n",
      "Skipped 20190518_ID_pe_dl.csv as both output files already exist\n",
      "20190419_ID_pe_dl.csv\n",
      "Skipped 20190419_ID_pe_dl.csv as both output files already exist\n",
      "20190617_ID_pe_dl.csv\n",
      "Skipped 20190617_ID_pe_dl.csv as both output files already exist\n",
      "20190329_ID_pe_dl.csv\n",
      "Skipped 20190329_ID_pe_dl.csv as both output files already exist\n",
      "20190725_ID_pe_dl.csv\n",
      "Processed and saved 20190725_ID_pe_dl.csv to respective folders\n",
      "20190608_ID_pe_dl.csv\n",
      "Skipped 20190608_ID_pe_dl.csv as both output files already exist\n",
      "20190527_ID_pe_dl.csv\n",
      "Skipped 20190527_ID_pe_dl.csv as both output files already exist\n",
      "20190428_ID_pe_dl.csv\n",
      "Skipped 20190428_ID_pe_dl.csv as both output files already exist\n",
      "20190509_ID_pe_dl.csv\n",
      "Skipped 20190509_ID_pe_dl.csv as both output files already exist\n",
      "20190626_ID_pe_dl.csv\n",
      "Skipped 20190626_ID_pe_dl.csv as both output files already exist\n",
      "20190707_ID_pe_dl.csv\n",
      "Skipped 20190707_ID_pe_dl.csv as both output files already exist\n",
      "20190321_ID_pe_dl.csv\n",
      "Skipped 20190321_ID_pe_dl.csv as both output files already exist\n",
      "20190402_ID_pe_dl.csv\n",
      "Skipped 20190402_ID_pe_dl.csv as both output files already exist\n",
      "20190123_ID_pe_dl.csv\n",
      "Skipped 20190123_ID_pe_dl.csv as both output files already exist\n",
      "20190204_ID_pe_dl.csv\n",
      "Skipped 20190204_ID_pe_dl.csv as both output files already exist\n",
      "20190105_ID_pe_dl.csv\n",
      "Skipped 20190105_ID_pe_dl.csv as both output files already exist\n",
      "20190501_ID_pe_dl.csv\n",
      "Skipped 20190501_ID_pe_dl.csv as both output files already exist\n",
      "20190420_ID_pe_dl.csv\n",
      "Skipped 20190420_ID_pe_dl.csv as both output files already exist\n",
      "20190303_ID_pe_dl.csv\n",
      "Skipped 20190303_ID_pe_dl.csv as both output files already exist\n",
      "20190222_ID_pe_dl.csv\n",
      "Skipped 20190222_ID_pe_dl.csv as both output files already exist\n",
      "20190411_ID_pe_dl.csv\n",
      "Skipped 20190411_ID_pe_dl.csv as both output files already exist\n",
      "20190330_ID_pe_dl.csv\n",
      "Skipped 20190330_ID_pe_dl.csv as both output files already exist\n",
      "20190213_ID_pe_dl.csv\n",
      "Skipped 20190213_ID_pe_dl.csv as both output files already exist\n",
      "20190114_ID_pe_dl.csv\n",
      "Skipped 20190114_ID_pe_dl.csv as both output files already exist\n",
      "20190510_ID_pe_dl.csv\n",
      "Skipped 20190510_ID_pe_dl.csv as both output files already exist\n",
      "20190312_ID_pe_dl.csv\n",
      "Skipped 20190312_ID_pe_dl.csv as both output files already exist\n",
      "20190101_ID_pe_dl.csv\n",
      "Skipped 20190101_ID_pe_dl.csv as both output files already exist\n",
      "20190729_ID_pe_dl.csv\n",
      "Processed and saved 20190729_ID_pe_dl.csv to respective folders\n",
      "20190110_ID_pe_dl.csv\n",
      "Skipped 20190110_ID_pe_dl.csv as both output files already exist\n",
      "20190523_ID_pe_dl.csv\n",
      "Skipped 20190523_ID_pe_dl.csv as both output files already exist\n",
      "20190604_ID_pe_dl.csv\n",
      "Skipped 20190604_ID_pe_dl.csv as both output files already exist\n",
      "20190208_ID_pe_dl.csv\n",
      "Skipped 20190208_ID_pe_dl.csv as both output files already exist\n",
      "20190127_ID_pe_dl.csv\n",
      "Skipped 20190127_ID_pe_dl.csv as both output files already exist\n",
      "20190721_ID_pe_dl.csv\n",
      "Processed and saved 20190721_ID_pe_dl.csv to respective folders\n",
      "20190406_ID_pe_dl.csv\n",
      "Skipped 20190406_ID_pe_dl.csv as both output files already exist\n",
      "20190325_ID_pe_dl.csv\n",
      "Skipped 20190325_ID_pe_dl.csv as both output files already exist\n",
      "20190703_ID_pe_dl.csv\n",
      "Skipped 20190703_ID_pe_dl.csv as both output files already exist\n",
      "20190622_ID_pe_dl.csv\n",
      "Skipped 20190622_ID_pe_dl.csv as both output files already exist\n",
      "20190226_ID_pe_dl.csv\n",
      "Skipped 20190226_ID_pe_dl.csv as both output files already exist\n",
      "20190307_ID_pe_dl.csv\n",
      "Skipped 20190307_ID_pe_dl.csv as both output files already exist\n",
      "20190109_ID_pe_dl.csv\n",
      "Skipped 20190109_ID_pe_dl.csv as both output files already exist\n",
      "20190424_ID_pe_dl.csv\n",
      "Skipped 20190424_ID_pe_dl.csv as both output files already exist\n",
      "20190505_ID_pe_dl.csv\n",
      "Skipped 20190505_ID_pe_dl.csv as both output files already exist\n",
      "20190613_ID_pe_dl.csv\n",
      "Skipped 20190613_ID_pe_dl.csv as both output files already exist\n",
      "20190217_ID_pe_dl.csv\n",
      "Skipped 20190217_ID_pe_dl.csv as both output files already exist\n",
      "20190730_ID_pe_dl.csv\n",
      "Processed and saved 20190730_ID_pe_dl.csv to respective folders\n",
      "20190415_ID_pe_dl.csv\n",
      "Skipped 20190415_ID_pe_dl.csv as both output files already exist\n",
      "20190712_ID_pe_dl.csv\n",
      "Skipped 20190712_ID_pe_dl.csv as both output files already exist\n",
      "20190316_ID_pe_dl.csv\n",
      "Skipped 20190316_ID_pe_dl.csv as both output files already exist\n",
      "20190118_ID_pe_dl.csv\n",
      "Skipped 20190118_ID_pe_dl.csv as both output files already exist\n",
      "20190514_ID_pe_dl.csv\n",
      "Skipped 20190514_ID_pe_dl.csv as both output files already exist\n"
     ]
    }
   ],
   "source": [
    "# 用的是不是这个！！\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geohash\n",
    "\n",
    "# Function to calculate geohash at specified precision\n",
    "def calculate_geohash(lat, lng, precision):\n",
    "    return geohash.encode(lat, lng, precision=precision)\n",
    "\n",
    "# Directory paths\n",
    "input_folder = '/home/jovyan/Data/DL/ID/'\n",
    "output_folder_3 = '/home/jovyan/Data/Agg_DL/ID3/'\n",
    "output_folder_5 = '/home/jovyan/Data/Agg_DL/ID5/'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_folder_3, exist_ok=True)\n",
    "os.makedirs(output_folder_5, exist_ok=True)\n",
    "\n",
    "# Chunk size for reading and processing\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        print(csv_file)\n",
    "        \n",
    "        # Define output file paths\n",
    "        output_file_3 = os.path.join(output_folder_3, csv_file.replace('.csv', '_agg3.csv'))\n",
    "        output_file_5 = os.path.join(output_folder_5, csv_file.replace('.csv', '_agg5.csv'))\n",
    "        \n",
    "        # Check if either of the output files does not exist\n",
    "        if not os.path.exists(output_file_3) or not os.path.exists(output_file_5):\n",
    "            # Initialize lists to collect chunks\n",
    "            chunks3 = []\n",
    "            chunks5 = []\n",
    "            \n",
    "            # Read the CSV file in chunks\n",
    "            for chunk in pd.read_csv(os.path.join(input_folder, csv_file), chunksize=chunk_size):\n",
    "                # Sort the chunk by 'event_datetime_local'\n",
    "                # chunk.sort_values('event_datetime_local', inplace=True)\n",
    "                \n",
    "                # Add geohash columns\n",
    "                chunk['geohash_3'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 3), axis=1)\n",
    "                chunk['geohash_5'] = chunk.apply(lambda row: calculate_geohash(row['lat'], row['lng'], 5), axis=1)\n",
    "                \n",
    "                # Append processed chunk to the lists\n",
    "                chunks3.append(chunk[['geohash_3', 'cuebiq_id']])\n",
    "                chunks5.append(chunk[['geohash_5', 'cuebiq_id']])\n",
    "            \n",
    "            # Concatenate all chunks\n",
    "            df3 = pd.concat(chunks3)\n",
    "            df5 = pd.concat(chunks5)\n",
    "            \n",
    "            # Group by `geohash_3` and aggregate data\n",
    "            aggregated_df3 = df3.groupby('geohash_3').agg(\n",
    "                point_number=('geohash_3', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df3 = aggregated_df3[aggregated_df3['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 3\n",
    "            filtered_df3.to_csv(output_file_3, index=False)\n",
    "            \n",
    "            # Group by `geohash_5` and aggregate data\n",
    "            aggregated_df5 = df5.groupby('geohash_5').agg(\n",
    "                point_number=('geohash_5', 'size'),\n",
    "                user_number=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Filter rows where `user_number` is greater than 10\n",
    "            filtered_df5 = aggregated_df5[aggregated_df5['user_number'] > 10].reset_index(drop=True)\n",
    "            \n",
    "            # Save the filtered DataFrame to the output folder for geohash 5\n",
    "            filtered_df5.to_csv(output_file_5, index=False)\n",
    "            \n",
    "            print(f\"Processed and saved {csv_file} to respective folders\")\n",
    "        else:\n",
    "            print(f\"Skipped {csv_file} as both output files already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86725b-b557-4fce-8b2b-407110f1e3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c66c9-d225-422f-be9d-1d0babf3ab7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d34e6d-39d3-4ac6-bf59-f46b73ebe989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db146c-08e2-4cd6-abcf-5212edb3f34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82097030-b944-4d42-9bba-145e1946922b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
