{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff9e4df-146c-4120-a8f9-220b7d551c96",
   "metadata": {},
   "source": [
    "Download dl data and save it in the system as csv.   \n",
    "check MX, IN for the edges before 0431, 0731, cause some data might be missing, need to add them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5aa75d-6b26-48f8-b783-44b0e42e4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8071db6-5131-4ce1-96d4-2f3bb05c2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4658058-9886-417f-b56c-8b09190e1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "\n",
    "# dl_table = f\"{schema_name['cda']}.device_location\"  \n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "tj_table = f\"{schema_name['cda']}.trajectory\"     \n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\"\n",
    "\n",
    "# stop_table = f\"{schema['cda']}.stop\" \n",
    "pe_stop_table = f\"{schema_name['cda']}.stop_uplevelled\"\n",
    "\n",
    "visit_table = f\"{schema_name['cda']}.visit \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4300f4dd-fb7d-4c71-847d-02cf287f07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrinoEngine:\n",
    "    def __init__(self):\n",
    "        self.conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = self.conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query: str):\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccbd66-ded9-406b-bd6b-27773cd5ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ecb71-8f95-4e8d-a032-1f3e66d05f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fafcb0d-5282-4dd4-bdfb-7d93efead119",
   "metadata": {},
   "source": [
    "# dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf39d97-8afa-4672-99ec-cca082dc1ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e88ca0-3813-405d-8e57-b470824464a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae532cb-7531-4b7d-b153-7b272251db66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0470bb-0da5-4456-b73e-d48956c0f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b730ab-bab9-4f50-a739-3efee073256f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94808830-7db1-4a88-b596-e296b184c4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f78852-bd03-46ed-9b24-4246f936e5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773ea01-b919-4704-a594-24337a3d4942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9be44cd-78ba-4482-a0c4-bc98111d9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add event_datetime_local to pre downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb41817-0f57-450a-98a9-68a76109cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to parse the `event_zoned_datetime` and generate `event_datetime_local`\n",
    "def generate_event_datetime_local(df):\n",
    "    df['event_datetime_local'] = pd.to_datetime(df['event_zoned_datetime'].str[:19], format='%Y-%m-%dT%H:%M:%S')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8e46e6-8122-4fd6-94d9-2cadcb1223e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing CSV files\n",
    "folder_path = '/home/jovyan/Data/DL/MX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232fc85f-a534-4c22-b345-398f54665ac1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and overwritten 20190111_MX_pe_dl.csv\n",
      "Processed and overwritten 20190102_MX_pe_dl.csv\n",
      "Processed and overwritten 20190120_MX_pe_dl.csv\n",
      "Processed and overwritten 20190119_MX_pe_dl.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Read the CSV file in chunks\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39mchunk_size):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Generate `event_datetime_local` column for each chunk\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_event_datetime_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate all chunks into a single DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m, in \u001b[0;36mgenerate_event_datetime_local\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_event_datetime_local\u001b[39m(df):\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_datetime_local\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_zoned_datetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1068\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:403\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    401\u001b[0m orig_arg \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     arg, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimezones\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_get_tz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:2265\u001b[0m, in \u001b[0;36mmaybe_convert_dtype\u001b[0;34m(data, copy, tz)\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_timedelta64_dtype(data\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_bool_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# GH#29794 enforcing deprecation introduced in GH#23539\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted to datetime64[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_period_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# Note: without explicitly raising here, PeriodIndex\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;66;03m#  test_setops.test_join_does_not_recur fails\u001b[39;00m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing PeriodDtype data is invalid. Use `data.to_timestamp()` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m     )\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_extension_array_dtype(data\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_datetime64tz_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;66;03m# TODO: We have no tests for these\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/dtypes/common.py:460\u001b[0m, in \u001b[0;36mis_period_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr_or_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeriodDtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_or_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py:977\u001b[0m, in \u001b[0;36mPeriodDtype.is_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# for pickle compat. __getstate__ is defined in the\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# PandasExtensionDtype superclass and uses the public properties to\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# pickle -> need to set the settable private ones here (see GH26067)\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_freq \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 977\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_dtype\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    Return a boolean if we if the passed type is an actual dtype that we\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    can match (via string or type)\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;66;03m# PeriodDtype can be instantiated from freq string like \"U\",\u001b[39;00m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;66;03m# but doesn't regard freq str like \"U\" as dtype.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Chunk size for reading and processing\n",
    "chunk_size = 10000\n",
    "\n",
    "# Iterate through each CSV file and process it in chunks\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Initialize an empty list to store chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Generate `event_datetime_local` column for each chunk\n",
    "        chunk = generate_event_datetime_local(chunk)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Concatenate all chunks into a single DataFrame\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Save the processed DataFrame back to the same CSV file (overwrite)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Print a message to indicate processing is done\n",
    "    print(f\"Processed and overwritten {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e48f9-5e67-4150-8615-48a319d22529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525992f-6248-4e2d-b3cc-173e5ad543fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7712a7-5e16-4cf6-aeb5-1e12e4d4ee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2321d65-a9a9-4b92-b039-7af78c7ab7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffafa634-f45d-4bf9-8fcf-611c2bd387e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: CO\n",
      "Processing date: 20190101 for country: CO\n",
      "Processing date: 20190102 for country: CO\n",
      "Processing date: 20190103 for country: CO\n",
      "Finished processing country: CO\n",
      "Total time taken: 422.44857931137085 seconds\n"
     ]
    }
   ],
   "source": [
    "# Select based on event_datetime_local direcly but very slow\n",
    "\n",
    "countries = ['CO']\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for country_code in countries:\n",
    "    print(f\"Processing country: {country_code}\")\n",
    "    current_date = start_date_dt\n",
    "    while current_date <= end_date_dt:\n",
    "        formatted_date = current_date.strftime('%Y%m%d')\n",
    "        next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            WITH event_data AS (\n",
    "                SELECT \n",
    "                    cuebiq_id, \n",
    "                    event_zoned_datetime, \n",
    "                    processing_date,\n",
    "                    timezoneoffset_secs,\n",
    "                    lat,\n",
    "                    lng, \n",
    "                    TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local\n",
    "                FROM {pe_dl_table}\n",
    "                WHERE \n",
    "                    country_code = '{country_code}' \n",
    "                    AND event_zoned_datetime IS NOT NULL\n",
    "                    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            )\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                timezoneoffset_secs,\n",
    "                lat,\n",
    "                lng\n",
    "            FROM event_data\n",
    "            WHERE \n",
    "                event_datetime_local >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "                AND event_datetime_local < date_parse('{next_date}', '%Y-%m-%d')\n",
    "        \"\"\"\n",
    "\n",
    "        for chunk in sql_engine.read_sql_chunked(query):\n",
    "            # Extract event_zoned_date from event_zoned_datetime\n",
    "            chunk['event_zoned_date'] = chunk['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "\n",
    "            # Convert processing_date and event_zoned_date to datetime objects\n",
    "            chunk['event_zoned_date'] = pd.to_datetime(chunk['event_zoned_date'])\n",
    "            processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "\n",
    "            # Filter data based on event_zoned_date\n",
    "            chunk = chunk[(chunk['event_zoned_date'] >= start_date_dt.strftime('%Y-%m-%d')) & \n",
    "                          (chunk['event_zoned_date'] <= processing_date_dt.strftime('%Y-%m-%d'))]\n",
    "\n",
    "            # Group by event_zoned_date\n",
    "            grouped = chunk.groupby('event_zoned_date')\n",
    "\n",
    "            for event_zoned_date, group_df in grouped:\n",
    "                # Remove duplicates\n",
    "                group_df = group_df.drop_duplicates()\n",
    "\n",
    "                # Convert event_zoned_date to string and remove hyphens\n",
    "                event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "                file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_dl.csv'\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    # If the file exists, append the new data\n",
    "                    group_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    # If the file does not exist, create it\n",
    "                    try:\n",
    "                        group_df.to_csv(file_path, index=False)\n",
    "                    except OSError as e:\n",
    "                        if not os.path.exists(os.path.dirname(file_path)):\n",
    "                            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                            group_df.to_csv(file_path, index=False)\n",
    "                        else:\n",
    "                            raise e\n",
    "\n",
    "            # Delete the DataFrame to free up memory\n",
    "            del chunk\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "\n",
    "# Processing date: 20190114 for country: IN\n",
    "# Processing date: 20190430 for country: ID\n",
    "# Processing date: 20190130 for country: MX\n",
    "\n",
    "# Works but Slowwwwwww!  7min for 3 day co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ee7569-e20c-4c5e-ba96-d54e616da4d0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: ID\n",
      "Processing date: 20190801 for country: ID\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Save the combined DataFrame back to the file\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# If the file does not exist, create it\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/codecs.py:186\u001b[0m, in \u001b[0;36mIncrementalEncoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalEncoder\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    An IncrementalEncoder encodes an input in multiple steps. The input can\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    be passed piece by piece to the encode() method. The IncrementalEncoder\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    remembers the state of the encoding process between calls to encode().\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        Creates an IncrementalEncoder instance.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m        for a list of possible values.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m errors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate event_datetime_local column and export\n",
    "# Using This one!!!!!!!!!!!!!!!!!!! (I think)\n",
    "\n",
    "countries = ['ID']\n",
    "start_date = 20190801\n",
    "end_date = 20190831\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for country_code in countries:\n",
    "    print(f\"Processing country: {country_code}\")\n",
    "    current_date = start_date_dt\n",
    "    while current_date <= end_date_dt:\n",
    "        formatted_date = current_date.strftime('%Y%m%d')\n",
    "        next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng, \n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local\n",
    "\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date = {formatted_date} \n",
    "                AND country_code = '{country_code}' \n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('20190101', '%Y%m%d')\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        \"\"\"\n",
    "\n",
    "        for chunk in sql_engine.read_sql_chunked(query):\n",
    "            # Extract event_zoned_date from event_zoned_datetime\n",
    "            chunk['event_zoned_date'] = chunk['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "\n",
    "            # Convert processing_date and event_zoned_date to datetime objects\n",
    "            chunk['event_zoned_date'] = pd.to_datetime(chunk['event_zoned_date'])\n",
    "            processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "\n",
    "            # Filter data based on event_zoned_date\n",
    "            chunk = chunk[(chunk['event_zoned_date'] >= start_date_dt.strftime('%Y-%m-%d')) & \n",
    "                          (chunk['event_zoned_date'] <= processing_date_dt.strftime('%Y-%m-%d'))]\n",
    "\n",
    "            # Group by event_zoned_date\n",
    "            grouped = chunk.groupby('event_zoned_date')\n",
    "\n",
    "            for event_zoned_date, group_df in grouped:\n",
    "                # Remove duplicates within the current group_df\n",
    "                group_df = group_df.drop_duplicates()\n",
    "\n",
    "                # Convert event_zoned_date to string and remove hyphens\n",
    "                event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "                file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_dl.csv'\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    # If the file exists, read the existing data\n",
    "                    existing_df = pd.read_csv(file_path)\n",
    "\n",
    "                    # Concatenate the existing data with the new group_df\n",
    "                    combined_df = pd.concat([existing_df, group_df])\n",
    "\n",
    "                    # Remove duplicates from the combined DataFrame\n",
    "                    combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "                    # Save the combined DataFrame back to the file\n",
    "                    combined_df.to_csv(file_path, index=False)\n",
    "                else:\n",
    "                    # If the file does not exist, create it\n",
    "                    try:\n",
    "                        group_df.to_csv(file_path, index=False)\n",
    "                    except OSError as e:\n",
    "                        if not os.path.exists(os.path.dirname(file_path)):\n",
    "                            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                            group_df.to_csv(file_path, index=False)\n",
    "                        else:\n",
    "                            raise e\n",
    "\n",
    "            # Delete the DataFrame to free up memory\n",
    "            del chunk\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "\n",
    "# Processing date: 20190131 for country: IN\n",
    "# Processing date: 20190731 for country: ID   1 hour for 10 day\n",
    "# Processing date: 20190228 for country: MX   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ab530-2ba6-455b-b832-bd9012fbebc8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 20190801 for country: ID\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "# Function to process data for a given date range and country\n",
    "def process_country_data(country_code, start_date_dt, end_date_dt):\n",
    "    current_date = start_date_dt\n",
    "    while current_date <= end_date_dt:\n",
    "        formatted_date = current_date.strftime('%Y%m%d')\n",
    "        next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                cuebiq_id, \n",
    "                event_zoned_datetime, \n",
    "                processing_date,\n",
    "                lat,\n",
    "                lng, \n",
    "                TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local\n",
    "            FROM {pe_dl_table}\n",
    "            WHERE \n",
    "                processing_date = {formatted_date} \n",
    "                AND country_code = '{country_code}' \n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('20190101', '%Y%m%d')\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "                AND event_zoned_datetime IS NOT NULL\n",
    "                AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        \"\"\"\n",
    "\n",
    "        batch_data = []\n",
    "\n",
    "        for chunk in sql_engine.read_sql_chunked(query):\n",
    "            chunk['event_zoned_date'] = pd.to_datetime(chunk['event_zoned_datetime'].str[:10])\n",
    "\n",
    "            processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "            chunk = chunk[(chunk['event_zoned_date'] >= start_date_dt) & \n",
    "                          (chunk['event_zoned_date'] <= processing_date_dt)]\n",
    "\n",
    "            grouped = chunk.groupby('event_zoned_date')\n",
    "            for event_zoned_date, group_df in grouped:\n",
    "                group_df = group_df.drop_duplicates()\n",
    "                event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "                batch_data.append((event_zoned_date_str, group_df))\n",
    "\n",
    "            del chunk\n",
    "\n",
    "        # Save batch data\n",
    "        for event_zoned_date_str, group_df in batch_data:\n",
    "            file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_dl.csv'\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    existing_df = pd.read_csv(file_path)\n",
    "                    combined_df = pd.concat([existing_df, group_df]).drop_duplicates()\n",
    "                except EmptyDataError:\n",
    "                    combined_df = group_df\n",
    "                combined_df.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                group_df.to_csv(file_path, index=False)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "    print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# Main script\n",
    "countries = ['ID']\n",
    "start_date = 20190801\n",
    "end_date = 20190831\n",
    "\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for country_code in countries:\n",
    "    process_country_data(country_code, start_date_dt, end_date_dt)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8d3db-ed7f-4d8b-a819-3f1d8e5db571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414aef3-ff70-4b64-8521-86965b392cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14bc935b-9892-49bf-92c0-a88549993459",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuebiq_id</th>\n",
       "      <th>event_zoned_datetime</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>event_datetime_local</th>\n",
       "      <th>event_zoned_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1318392</th>\n",
       "      <td>1972801775</td>\n",
       "      <td>2019-06-13T00:00:00+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-5.379151</td>\n",
       "      <td>105.216373</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090119</th>\n",
       "      <td>1874892836</td>\n",
       "      <td>2019-06-13T00:00:00+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>1.185788</td>\n",
       "      <td>104.011441</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495442</th>\n",
       "      <td>1893971178</td>\n",
       "      <td>2019-06-13T00:00:00+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-8.513576</td>\n",
       "      <td>114.111792</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958164</th>\n",
       "      <td>2089337707</td>\n",
       "      <td>2019-06-13T00:00:00+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-6.150520</td>\n",
       "      <td>106.869127</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627766</th>\n",
       "      <td>1980479954</td>\n",
       "      <td>2019-06-13T00:00:00+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-7.421599</td>\n",
       "      <td>109.478756</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356614</th>\n",
       "      <td>2087388010</td>\n",
       "      <td>2019-06-13T23:59:59+09:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-1.179000</td>\n",
       "      <td>136.188175</td>\n",
       "      <td>2019-06-13 23:59:59</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393475</th>\n",
       "      <td>1988646522</td>\n",
       "      <td>2019-06-13T23:59:59+08:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-8.668958</td>\n",
       "      <td>115.182401</td>\n",
       "      <td>2019-06-13 23:59:59</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76350</th>\n",
       "      <td>2096776848</td>\n",
       "      <td>2019-06-13T23:59:59+08:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-8.167199</td>\n",
       "      <td>114.733030</td>\n",
       "      <td>2019-06-13 23:59:59</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125366</th>\n",
       "      <td>2065007831</td>\n",
       "      <td>2019-06-13T23:59:59+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>-6.241934</td>\n",
       "      <td>106.963437</td>\n",
       "      <td>2019-06-13 23:59:59</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826495</th>\n",
       "      <td>1884246196</td>\n",
       "      <td>2019-06-13T23:59:59+07:00</td>\n",
       "      <td>20190613</td>\n",
       "      <td>1.081643</td>\n",
       "      <td>100.251776</td>\n",
       "      <td>2019-06-13 23:59:59</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4566125 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cuebiq_id       event_zoned_datetime  processing_date       lat  \\\n",
       "1318392  1972801775  2019-06-13T00:00:00+07:00         20190613 -5.379151   \n",
       "4090119  1874892836  2019-06-13T00:00:00+07:00         20190613  1.185788   \n",
       "3495442  1893971178  2019-06-13T00:00:00+07:00         20190613 -8.513576   \n",
       "1958164  2089337707  2019-06-13T00:00:00+07:00         20190613 -6.150520   \n",
       "627766   1980479954  2019-06-13T00:00:00+07:00         20190613 -7.421599   \n",
       "...             ...                        ...              ...       ...   \n",
       "2356614  2087388010  2019-06-13T23:59:59+09:00         20190613 -1.179000   \n",
       "393475   1988646522  2019-06-13T23:59:59+08:00         20190613 -8.668958   \n",
       "76350    2096776848  2019-06-13T23:59:59+08:00         20190613 -8.167199   \n",
       "4125366  2065007831  2019-06-13T23:59:59+07:00         20190613 -6.241934   \n",
       "3826495  1884246196  2019-06-13T23:59:59+07:00         20190613  1.081643   \n",
       "\n",
       "                lng event_datetime_local event_zoned_date  \n",
       "1318392  105.216373  2019-06-13 00:00:00       2019-06-13  \n",
       "4090119  104.011441  2019-06-13 00:00:00       2019-06-13  \n",
       "3495442  114.111792  2019-06-13 00:00:00       2019-06-13  \n",
       "1958164  106.869127  2019-06-13 00:00:00       2019-06-13  \n",
       "627766   109.478756  2019-06-13 00:00:00       2019-06-13  \n",
       "...             ...                  ...              ...  \n",
       "2356614  136.188175  2019-06-13 23:59:59       2019-06-13  \n",
       "393475   115.182401  2019-06-13 23:59:59       2019-06-13  \n",
       "76350    114.733030  2019-06-13 23:59:59       2019-06-13  \n",
       "4125366  106.963437  2019-06-13 23:59:59       2019-06-13  \n",
       "3826495  100.251776  2019-06-13 23:59:59       2019-06-13  \n",
       "\n",
       "[4566125 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jovyan/Data/DL/ID/20190613_ID_pe_dl.csv')\n",
    "df.sort_values('event_datetime_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0390c906-4555-4941-910b-6c6f86ba523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With UTC correction\n",
    "\n",
    "# countries = ['MX']\n",
    "# start_date = 20190205\n",
    "# end_date = 20190205\n",
    "\n",
    "# # Convert integer dates to datetime objects\n",
    "# start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "# end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# for country_code in countries:\n",
    "#     print(f\"Processing country: {country_code}\")\n",
    "#     current_date = start_date_dt\n",
    "#     while current_date <= end_date_dt:\n",
    "#         formatted_date = current_date.strftime('%Y%m%d')\n",
    "#         next_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "#         print(f\"Processing date: {formatted_date} for country: {country_code}\")\n",
    "\n",
    "#         query = f\"\"\"\n",
    "#             SELECT \n",
    "#                 cuebiq_id, \n",
    "#                 event_zoned_datetime, \n",
    "#                 processing_date,\n",
    "#                 timezoneoffset_secs,\n",
    "#                 lat,\n",
    "#                 lng, \n",
    "#                 TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s') +\n",
    "#                 interval '1' second * timezoneoffset_secs) AS event_datetime_utc\n",
    "\n",
    "#             FROM {pe_dl_table}\n",
    "#             WHERE \n",
    "#                 processing_date = {formatted_date} \n",
    "#                 AND country_code = '{country_code}' \n",
    "#                 AND event_zoned_datetime IS NOT NULL\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "#                 AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "#         \"\"\"\n",
    "\n",
    "#         for chunk in sql_engine.read_sql_chunked(query):\n",
    "#             # Extract event_zoned_date from event_zoned_datetime\n",
    "#             chunk['event_zoned_date'] = chunk['event_zoned_datetime'].apply(lambda x: x[:10])\n",
    "\n",
    "#             # Convert processing_date and event_zoned_date to datetime objects\n",
    "#             chunk['event_zoned_date'] = pd.to_datetime(chunk['event_zoned_date'])\n",
    "#             processing_date_dt = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "\n",
    "#             # Filter data based on event_zoned_date\n",
    "#             chunk = chunk[(chunk['event_zoned_date'] >= start_date_dt.strftime('%Y-%m-%d')) & \n",
    "#                           (chunk['event_zoned_date'] <= processing_date_dt.strftime('%Y-%m-%d'))]\n",
    "\n",
    "#             # Group by event_zoned_date\n",
    "#             grouped = chunk.groupby('event_zoned_date')\n",
    "\n",
    "#             for event_zoned_date, group_df in grouped:\n",
    "#                 # Remove duplicates\n",
    "#                 group_df = group_df.drop_duplicates()\n",
    "\n",
    "#                 # Convert event_zoned_date to string and remove hyphens\n",
    "#                 event_zoned_date_str = event_zoned_date.strftime('%Y%m%d')\n",
    "#                 file_path = f'/home/jovyan/Data/DL/{country_code}/{event_zoned_date_str}_{country_code}_pe_tj.csv'\n",
    "\n",
    "#                 if os.path.exists(file_path):\n",
    "#                     # If the file exists, append the new data\n",
    "#                     group_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "#                 else:\n",
    "#                     # If the file does not exist, create it\n",
    "#                     try:\n",
    "#                         group_df.to_csv(file_path, index=False)\n",
    "#                     except OSError as e:\n",
    "#                         if not os.path.exists(os.path.dirname(file_path)):\n",
    "#                             os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#                             group_df.to_csv(file_path, index=False)\n",
    "#                         else:\n",
    "#                             raise e\n",
    "\n",
    "#             # Delete the DataFrame to free up memory\n",
    "#             del chunk\n",
    "\n",
    "#         current_date += timedelta(days=1)\n",
    "\n",
    "#     print(f\"Finished processing country: {country_code}\")\n",
    "\n",
    "# # End timing\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate and print the total time taken\n",
    "# total_time = end_time - start_time\n",
    "# print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# # CO\n",
    "# # Processing date: 20190114 for country: IN\n",
    "# # Processing date: 20190430 for country: ID\n",
    "# # Processing date: 20190130 for country: MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ed202-4740-4553-8727-aac788b252cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb0731-d516-49ac-9d2c-974c8a2396d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c649a0a-8c9a-4935-bf89-e7d7365f520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0a482-907d-4344-9e78-e33cc39fd929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1931913-a41d-4852-815a-4c70d885cb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2f627-20b3-4fd9-9162-dc8539087624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b1d58-10da-40f1-8385-5dd0d8f977ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03bba8-7470-49c3-bcbd-115cd8505c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b858f-94e4-4491-8082-f4bea968bc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38b025-b6fc-468c-a9e3-9d5f658ba0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cc2ef-27a9-4d44-add2-caba2862f776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d0dc5-6c53-40ee-8bd8-d70edc62e090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4736e1-416e-49bf-8c5a-8a0bab49f203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c381c41-2cc1-44aa-8f39-3650a0bec7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b0cd7-033b-4b4f-ad27-bf2d05870309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53227c98-5f9f-4779-8fc8-d962e38f182f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837c139-d597-413d-b580-432ae9119b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
