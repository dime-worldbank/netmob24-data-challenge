{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882f66d0-2847-4e67-9526-143c32038dc7",
   "metadata": {},
   "source": [
    "move file (pd) to dedicated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c8a25-4f22-47bd-b852-4e61dfbf6a98",
   "metadata": {},
   "source": [
    "# In SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f423498-467c-42cf-ac05-9d419a21bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6657bd-fea4-4c96-808c-74ab81caae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c6b5-75dd-46e9-9916-765264ce232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import create_engine\n",
    "output_schema_name = 'presence_data'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbade843-bbd5-49a0-8ba4-8727b4085455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from trino.dbapi import connect\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee6a83-45aa-4978-90c6-3254b860374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show schemas from dedicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133bef3-7656-46e2-a787-778243d582b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables from dedicated.pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba65c7-7aa5-4824-ad9e-4971e5524e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables from dedicated.od_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfe7a0-af41-432a-bf90-4f140f5385f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables from dedicated.presence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff86971-9427-4c37-860f-8e5fb9b10c5b",
   "metadata": {},
   "source": [
    "## Create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09f49c-32cc-477b-8fd6-8c95abd1be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_schema_name = \"OD_matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09dc60-5aba-4cfd-ae0d-fd11fe5c9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql create schema if not exists dedicated.$output_schema_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b1df5-d7fc-41ad-a8bd-9910d01b441d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c2034e-6cc2-4b38-b5b5-58927d6fa939",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c66815-39e8-40bf-8e54-cac840767f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_schema_name = 'presence_data'\n",
    "# table_name = f\"dedicated.{output_schema_name}.new_table_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194b24d-a284-45a0-92d9-d1ed19883c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql create table if not exists $table_name (col_0 varchar, col_1 bigint, col_2 varchar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbab66a-a62b-4922-b575-96d4bd83667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql show tables from dedicated.presence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf338f-a581-482e-ba2b-ff61adc05a65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37c34c-7c05-4a74-9509-1ec11aa63174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS hive.od_matrix.od_id_20190301_agg5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33b8e7-9136-4238-9ed2-1d78b8ca2cd5",
   "metadata": {},
   "source": [
    "## Rename table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431c997-fae3-4bf2-b83a-32ec4ada2484",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename the table to the final name based on date and country code\n",
    "\n",
    "output_schema_name = 'od_matrix'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "rename_table_query = f\"\"\"\n",
    "ALTER TABLE \"od_id_20190301_test_agg5\" RENAME TO \"od_id_20190301_agg5\"\n",
    "\"\"\"\n",
    "\n",
    "# Execute the rename table query\n",
    "with con.connect() as connection:\n",
    "    connection.execute(rename_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb751e5-3813-45e1-b29a-48bc4c0a8bcb",
   "metadata": {},
   "source": [
    "## Check table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559dd598-4010-468a-88e9-fcd005c74e78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check\n",
    "q2 = f\"select * from dedicated.presence_data.od_id_20190301_agg5\"\n",
    "\n",
    "%sql $q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae093425-b7ee-4b6e-bed6-fcfb206a4fc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check\n",
    "q2 = f\"select * from dedicated.od_matrix.od_id_20190303_agg5\"\n",
    "%sql $q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0aa229-5e64-4513-b718-97b478962f8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check\n",
    "q1 = f\"select * from dedicated.pop_density.pd_in_20190401_agg3_2\"\n",
    "%sql $q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd3b39-675a-4bde-9453-9f54eef30660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "# Replace with your actual schema name\n",
    "output_schema_name = \"presence_data\"\n",
    "bucketized_table = f\"dedicated.{output_schema_name}.bucketized_table\"\n",
    "\n",
    "# Partition size\n",
    "partition_size = 5000\n",
    "\n",
    "# Data preparation\n",
    "df['partition_key'] = (df.index // partition_size) + 1\n",
    "df = df.astype({\n",
    "    'start_geohash_user': 'int',\n",
    "    'end_geohash_user': 'int',\n",
    "    'trip_count': 'int',\n",
    "    'partition_key': 'int'\n",
    "})\n",
    "\n",
    "# Create the SQL engine\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "# Define the SQL query to create the bucketized and partitioned table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE {bucketized_table} (\n",
    "    start_geohash5 varchar,\n",
    "    start_geohash_user bigint,\n",
    "    end_geohash5 varchar,\n",
    "    end_geohash_user bigint,\n",
    "    trip_count bigint,\n",
    "    m_duration_min double,\n",
    "    mdn_duration_min double,\n",
    "    sd_duration_min double,\n",
    "    m_length_m double,\n",
    "    mdn_length_m double,\n",
    "    sd_length_m double,\n",
    "    partition_key bigint\n",
    ")\n",
    "WITH (\n",
    "  partitioned_by = ARRAY['partition_key'],\n",
    "  bucketed_by = ARRAY['end_geohash5'],\n",
    "  bucket_count = 5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the create table query\n",
    "with con.connect() as connection:\n",
    "    connection.execute(create_table_query)\n",
    "\n",
    "# Function to insert data in chunks\n",
    "def insert_data_in_chunks(df, table_name, engine, chunk_size=5000):\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        chunk.to_sql(table_name, engine, index=False, if_exists='append', method='multi')\n",
    "\n",
    "# Insert data into the bucketized table\n",
    "insert_data_in_chunks(df, \"bucketized_table\", con, chunk_size)\n",
    "\n",
    "country_code = 'id'\n",
    "date_str = \"20190101\"\n",
    "final_bucketized_table = f\"od_{country_code.lower()}_{event_date}_agg3\"\n",
    "\n",
    "# Rename the table to the final name based on the date range and country code\n",
    "rename_table_query = f\"\"\"\n",
    "ALTER TABLE \"bucketized_table\" RENAME TO \"{final_bucketized_table}\"\n",
    "\"\"\"\n",
    "\n",
    "# Execute the rename table query\n",
    "with con.connect() as connection:\n",
    "    connection.execute(rename_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6bef6-4172-418b-9373-0d84194ceedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ce4be-e732-4283-9dee-65eb8a718c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef1659-daf8-4ab0-b190-ff16fbf7d597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c6acc-7ea5-45b0-9e94-a960d6a66d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e01a670-90b3-41ae-b025-3ed460232add",
   "metadata": {},
   "source": [
    "## From existing file (in jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf684ad-ba48-4fd5-8b17-356e38036e84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate date strings from 20190101 to 20190430\n",
    "start_date = datetime.strptime(\"20190714\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20190731\", \"%Y%m%d\")\n",
    "date_generated = [start_date + timedelta(days=x) for x in range(0, (end_date-start_date).days+1)]\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            print(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Start timing the process\n",
    "start_time = time.time()\n",
    "                \n",
    "# Iterate over each date, read the corresponding CSV file, and insert into the database for both agg3 and agg5\n",
    "for date in date_generated:\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Process agg5 files\n",
    "    file_path_agg5 = f\"/home/jovyan/Data/Agg_DL/ID5/{date_str}_ID_pe_dl_agg5.csv\"\n",
    "    table_name_agg5 = f\"pd_id_{date_str}_agg5\"\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file for agg5\n",
    "        df_agg5 = pd.read_csv(file_path_agg5)\n",
    "        \n",
    "        # Convert all column names to lowercase\n",
    "        df_agg5.columns = [col.lower() for col in df_agg5.columns]\n",
    "        \n",
    "        # Insert DataFrame into the table with retry mechanism\n",
    "        insert_data_with_retry(df_agg5, table_name_agg5, con)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path_agg5}: {e}\")\n",
    "\n",
    "    # Process agg3 files\n",
    "    file_path_agg3 = f\"/home/jovyan/Data/Agg_DL/ID3/{date_str}_ID_pe_dl_agg3.csv\"\n",
    "    table_name_agg3 = f\"pd_id_{date_str}_agg3\"\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file for agg3\n",
    "        df_agg3 = pd.read_csv(file_path_agg3)\n",
    "        \n",
    "        # Convert all column names to lowercase\n",
    "        df_agg3.columns = [col.lower() for col in df_agg3.columns]\n",
    "        \n",
    "        # Insert DataFrame into the table with retry mechanism\n",
    "        insert_data_with_retry(df_agg3, table_name_agg3, con)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path_agg3}: {e}\")\n",
    "        \n",
    "# End timing the process\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# 638s for 20190511 - 20190713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc00c6c-7d08-4738-b7f1-873b12c21f67",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Single one for testing\n",
    "\n",
    "# # Generate date strings from 20190101 to 20190430\n",
    "# start_date = datetime.strptime(\"20190101\", \"%Y%m%d\")\n",
    "# end_date = datetime.strptime(\"20190104\", \"%Y%m%d\")\n",
    "# date_generated = [start_date + timedelta(days=x) for x in range(0, (end_date-start_date).days+1)]\n",
    "\n",
    "# # Iterate over each date, read the corresponding CSV file, and insert into the database\n",
    "# for date in date_generated:\n",
    "#     date_str = date.strftime(\"%Y%m%d\")\n",
    "#     file_path = f\"/home/jovyan/Data/Agg_DL/CO5/{date_str}_CO_pe_dl_agg5.csv\"\n",
    "#     table_name = f\"pd_co_{date_str}_agg5\"\n",
    "    \n",
    "#     try:\n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Convert all column names to lowercase\n",
    "#         df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "#         # Insert DataFrame into the table\n",
    "#         df.to_sql(\n",
    "#             table_name, \n",
    "#             con, \n",
    "#             index=False, \n",
    "#             if_exists=\"append\", \n",
    "#             method=\"multi\"\n",
    "#         )\n",
    "#         print(f\"Inserted data into table {table_name} from {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# CO 5 不行 不知道为啥？？？ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18214a-d461-4a1e-8995-c13780e69bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c24c5-fdfc-4ec1-a093-c7301ca4f544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700df145-af40-4d82-9d45-81d1d0707f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42551b94-0462-4418-8096-e11d3b224fa6",
   "metadata": {},
   "source": [
    "### test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada8e1c-6033-4118-8ccb-8e2f76accc6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/home/jovyan/Data/Agg_DL/ID3/20190101_ID_pe_dl_agg3.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09d77a-7282-4549-830b-e64bf0513104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df.to_sql(\n",
    "#     \"my_processed_dataset3\", \n",
    "#     con, \n",
    "#     index=False, \n",
    "#     if_exists=\"append\", \n",
    "#     method=\"multi\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d8dbc-101b-4e72-bd23-5ce795434c29",
   "metadata": {},
   "source": [
    "1. if_exists='fail': The method will raise a ValueError if the table already exists. This is the default behavior.\n",
    "2. if_exists='replace': If the table exists, it will be dropped and replaced with the new data.\n",
    "3. if_exists='append': If the table exists, the new data will be inserted into the existing table. If the table does not exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db5e6c-9db8-4ecc-b420-6323c5cac55c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598a215-93d6-4b1e-9bc3-2b41ea2fae42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecbde5-5883-4669-9833-079016a107fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfebc4-5452-4cc8-a49c-8d209ec9cc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462ed36-ecac-4b66-97ad-f8bf0d40d267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c33af0-ae63-4ed1-b443-9c1d348c82d0",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334064e-c113-4c55-9291-06de7488c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jovyan/Data/DL/MX/20190128_MX_pe_dl.csv')\n",
    "# df.sort_values('event_datetime_local')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c571f-be4e-49c3-95e4-438c6511e0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471a97b-b00c-458a-892c-4d86ddff9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcd739-a2ce-4f64-b035-ffdbfe36aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a66ed-5181-48a0-951c-ee6d5837fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "\n",
    "# dl_table = f\"{schema_name['cda']}.device_location\"  \n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "tj_table = f\"{schema_name['cda']}.trajectory\"     \n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\"\n",
    "\n",
    "# stop_table = f\"{schema['cda']}.stop\" \n",
    "pe_stop_table = f\"{schema_name['cda']}.stop_uplevelled\"\n",
    "\n",
    "visit_table = f\"{schema_name['cda']}.visit \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68119f-efa9-4df1-8aa0-c5e2f8026e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrinoEngine:\n",
    "    def __init__(self):\n",
    "        self.conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = self.conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query: str):\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70859d-4642-405b-9cff-1847f2e77957",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = 'ID'\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "formatted_date = start_date_dt.strftime('%Y%m%d')\n",
    "next_date = (start_date_dt + timedelta(days=1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfe90d-bb7e-4729-a5ef-c7a8beab1535",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe_tj_table = sql_engine.read_sql_chunked(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng, \n",
    "        TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date = {formatted_date} \n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# pe_tj_table\n",
    "first_chunk = next(pe_tj_table)\n",
    "first_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1ba02-1027-4c5e-a721-f860cf82040e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe_tj_table = sql_engine.read_sql_chunked(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng, \n",
    "        TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_date_local\n",
    "\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date = {formatted_date} \n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{next_date}', '%Y-%m-%d')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert the generator to a DataFrame\n",
    "chunks = [chunk for chunk in pe_tj_table]\n",
    "pe_tj_table_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "pe_tj_table_df\n",
    "\n",
    "# Process date 用前面的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf148d2-0115-4230-b2b7-5fb3e575342b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# probabliy not correct!!!!\n",
    "# 因为要求时间<开始时间\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'ID'\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Calculate the date 45 days before the start_date\n",
    "lookback_date_dt = start_date_dt - timedelta(days=45)\n",
    "formatted_lookback_date = lookback_date_dt.strftime('%Y%m%d')\n",
    "formatted_end_date = end_date_dt.strftime('%Y%m%d')\n",
    "\n",
    "# Construct the SQL query\n",
    "pe_tj_table = sql_engine.read_sql_chunked(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng, \n",
    "        TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_date_local\n",
    "\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date >= {formatted_lookback_date}\n",
    "        AND processing_date <= {formatted_end_date}\n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) <= date_parse('{end_date_dt.strftime('%Y-%m-%d')}', '%Y-%m-%d')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert the generator to a DataFrame\n",
    "chunks = [chunk for chunk in pe_tj_table]\n",
    "pe_tj_table_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "pe_tj_table_df\n",
    "\n",
    "# probabliy not correct!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f52cf-594c-41f5-9e96-f96fa35e0d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f489c9-7d49-4845-ba99-1e02c26a1acd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one seems like correct \n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'MX'\n",
    "start_date = 20190102\n",
    "end_date = 20190103\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    lookback_date = current_date - timedelta(days=1)\n",
    "    lookahead_date = current_date + timedelta(days=35)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct and execute the SQL query\n",
    "    pe_dl_table = sql_engine.read_sql_chunked(\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            lat,\n",
    "            lng, \n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local,\n",
    "            DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_date_local\n",
    "\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            processing_date >= {formatted_lookback_date} \n",
    "            AND processing_date <= {formatted_lookahead_date}\n",
    "            AND country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "            AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Convert the generator to a DataFrame\n",
    "    chunks = [chunk for chunk in pe_tj_table]\n",
    "    if chunks:\n",
    "        pe_tj_table_df = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        output_filename = f'/home/jovyan/Data/Test/0604/pe_tj_table_{formatted_current_date}.csv'\n",
    "        pe_tj_table_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved data for {formatted_current_date} to {output_filename}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(\"Data extraction and saving completed.\")\n",
    "\n",
    "\n",
    "end_time = time.time() # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6270e-31ad-4989-bde8-bf1216b449d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform all event date and get one day. \n",
    "import time\n",
    "\n",
    "country_code = 'MX'\n",
    "start_date = 20190101\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "next_date = (start_date_dt + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "pe_tj_table = sql_engine.read_sql_chunked(\n",
    "    f\"\"\"\n",
    "    WITH event_data AS (\n",
    "        SELECT \n",
    "            cuebiq_id, \n",
    "            event_zoned_datetime, \n",
    "            processing_date,\n",
    "            timezoneoffset_secs,\n",
    "            lat,\n",
    "            lng, \n",
    "            TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) AS event_datetime_local\n",
    "        FROM {pe_dl_table}\n",
    "        WHERE \n",
    "            country_code = '{country_code}' \n",
    "            AND event_zoned_datetime IS NOT NULL\n",
    "            AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        timezoneoffset_secs,\n",
    "        lat,\n",
    "        lng\n",
    "    FROM event_data\n",
    "    WHERE \n",
    "        event_datetime_local >= date_parse('{start_date_dt.strftime('%Y%m%d')}', '%Y%m%d')\n",
    "        AND event_datetime_local < date_parse('{next_date}', '%Y-%m-%d')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert the generator to a DataFrame\n",
    "chunks = [chunk for chunk in pe_tj_table]\n",
    "pe_tj_table_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "\n",
    "pe_tj_table_df\n",
    "\n",
    "\n",
    "# transform all event date and get one day. \n",
    "# 877 for one day in ID \n",
    "# 1980 for one day in MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc26a47-31e6-49a2-8dd1-10fb8da07bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c126bac-7cfe-4c2c-a410-b4ca69d2a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d469c-22ac-43cf-a1f7-505b24f4d360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with logging and works. export data to the system. \n",
    "# 1933.101960659027 for 3 day in IN\n",
    "# process date -1 to +35\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import geohash2\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'IN'\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    lookback_date = current_date - timedelta(days=1)\n",
    "    lookahead_date = current_date + timedelta(days=35)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Executing SQL query for date {formatted_current_date}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Save the aggregated data to a CSV file for geohash5\n",
    "            output_filename_5 = f'/home/jovyan/Data/Agg_DL/IN5/aggregated_pe_tj_table_geohash5_{formatted_current_date}.csv'\n",
    "            aggregated_data_5.to_csv(output_filename_5, index=False)\n",
    "            logging.info(f\"Saved aggregated data for geohash5 for {formatted_current_date} to {output_filename_5}\")\n",
    "\n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Save the aggregated data to a CSV file for geohash3\n",
    "            output_filename_3 = f'/home/jovyan/Data/Agg_DL/IN3/aggregated_pe_tj_table_geohash3_{formatted_current_date}.csv'\n",
    "            aggregated_data_3.to_csv(output_filename_3, index=False)\n",
    "            logging.info(f\"Saved aggregated data for geohash3 for {formatted_current_date} to {output_filename_3}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215eb06a-cb9f-45f3-b8b6-1e2b70b25821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530cf83-fcef-44b3-9819-42bf3bf3d832",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write in to the table\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import geohash2\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Database connection setup\n",
    "output_schema_name = 'presence_data'\n",
    "con = create_engine(f\"trino://localhost:9090/dedicated/{output_schema_name}\")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'IN'\n",
    "start_date = 20190101\n",
    "end_date = 20190103\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\"\n",
    "\n",
    "# Convert integer dates to datetime objects\n",
    "start_date_dt = datetime.strptime(str(start_date), '%Y%m%d')\n",
    "end_date_dt = datetime.strptime(str(end_date), '%Y%m%d')\n",
    "\n",
    "# Function to insert data with retry mechanism\n",
    "def insert_data_with_retry(df, table_name, con, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                con, \n",
    "                index=False, \n",
    "                if_exists=\"append\", \n",
    "                method=\"multi\"\n",
    "            )\n",
    "            logging.info(f\"Inserted data into table {table_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"Failed to insert data into table {table_name} after {retries} attempts\")\n",
    "\n",
    "# Loop through each day from start_date to end_date\n",
    "current_date = start_date_dt\n",
    "while current_date <= end_date_dt:\n",
    "    # Calculate the lookback and lookahead dates\n",
    "    lookback_date = current_date - timedelta(days=1)\n",
    "    lookahead_date = current_date + timedelta(days=35)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    formatted_lookback_date = lookback_date.strftime('%Y%m%d')\n",
    "    formatted_current_date = current_date.strftime('%Y%m%d')\n",
    "    formatted_lookahead_date = lookahead_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id, \n",
    "        event_zoned_datetime, \n",
    "        processing_date,\n",
    "        lat,\n",
    "        lng\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        processing_date BETWEEN {formatted_lookback_date} AND {formatted_lookahead_date}\n",
    "        AND country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) = date_parse('{formatted_current_date}', '%Y%m%d')\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Executing SQL query for date {formatted_current_date}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        pe_dl_table_gen = sql_engine.read_sql_chunked(query)\n",
    "        \n",
    "        # Convert the generator to a DataFrame\n",
    "        chunks = [chunk for chunk in pe_dl_table_gen]\n",
    "        if chunks:\n",
    "            pe_dl_table_df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Calculate geohashes\n",
    "            pe_dl_table_df['geohash5'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=5), axis=1)\n",
    "            pe_dl_table_df['geohash3'] = pe_dl_table_df.apply(lambda row: geohash2.encode(row['lat'], row['lng'], precision=3), axis=1)\n",
    "            \n",
    "            # Aggregate data for geohash5\n",
    "            aggregated_data_5 = pe_dl_table_df.groupby('geohash5').agg(\n",
    "                no_of_points=('geohash5', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Insert aggregated data for geohash5 into SQL table\n",
    "            table_name_agg5 = f\"pd_{country_code}_{formatted_current_date}_agg5\"\n",
    "            insert_data_with_retry(aggregated_data_5, table_name_agg5, con)\n",
    "            \n",
    "            # Aggregate data for geohash3\n",
    "            aggregated_data_3 = pe_dl_table_df.groupby('geohash3').agg(\n",
    "                no_of_points=('geohash3', 'size'),\n",
    "                no_of_unique_users=('cuebiq_id', 'nunique')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Insert aggregated data for geohash3 into SQL table\n",
    "            table_name_agg3 = f\"pd_{country_code}_{formatted_current_date}_agg3\"\n",
    "            insert_data_with_retry(aggregated_data_3, table_name_agg3, con)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while processing data for date {formatted_current_date}: {e}\")\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logging.info(\"Data extraction, aggregation, and saving completed.\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "logging.info(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9f72b-c102-41c1-ad5e-b0d42f7cbfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c38721-e315-453f-b3e7-cf483dc7c5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
