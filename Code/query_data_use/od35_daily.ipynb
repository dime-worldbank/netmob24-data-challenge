{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac80492-7321-4716-9eb6-480a39e423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.autocommit=False\n",
    "%config SqlMagic.autolimit=0\n",
    "%config SqlMagic.autopandas=True\n",
    "%config SqlMagic.displaylimit=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32854b4e-a8fc-4f2c-af6d-0a2703447897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql trino://localhost:9090/cuebiq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ead77-7643-4b96-833e-1727f453930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geohash\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd797a-04e3-4191-9761-96e660fd396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583b13b-220f-4219-be2f-439cf2c1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_tj_table = f\"{schema_name['cda']}.trajectory_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f56262-6bce-45d7-8ce8-5c1b8e423558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88726018-c858-43e6-9c8d-ed6e25929b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_daily_data(start_date, end_date, country_code, export_path):\n",
    "    \"\"\"\n",
    "    Process and export daily aggregated data for a specified date range and country.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): The start date in the format 'YYYY-MM-DD'.\n",
    "    - end_date (str): The end date in the format 'YYYY-MM-DD'.\n",
    "    - country_code (str): The country code (e.g., 'CO' for Colombia).\n",
    "    - export_path (str): The directory where the output files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the start and end dates to datetime objects\n",
    "    start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Iterate through each day in the date range\n",
    "    current_date = start_date_obj\n",
    "    while current_date <= end_date_obj:\n",
    "        event_date = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        csv3_file = f\"od_{country_code.lower()}_agg3_daily.csv\"\n",
    "        csv5_file = f\"od_{country_code.lower()}_agg5_daily.csv\"\n",
    "        csv3_file_path = os.path.join(export_path, csv3_file)\n",
    "        csv5_file_path = os.path.join(export_path, csv5_file)\n",
    "\n",
    "        try:\n",
    "            # Read data from the SQL table for the specified day and country\n",
    "            pe_tj_df = sql_engine.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT \n",
    "                    cuebiq_id,\n",
    "                    geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "                    geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "                    geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "                    geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "                    DATE(TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date,\n",
    "                    duration_minutes,\n",
    "                    length_meters,\n",
    "                    number_of_points\n",
    "                FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "                WHERE \n",
    "                    event_date = {event_date}\n",
    "                    AND end_country = '{country_code}' \n",
    "                    AND start_country = '{country_code}' \n",
    "                \"\"\"\n",
    "            )\n",
    "            logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "            # Aggregation 3\n",
    "            aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std'],\n",
    "                'local_date': 'first' \n",
    "            }).reset_index()\n",
    "            aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                     'local_date']\n",
    "\n",
    "            # Reorder columns\n",
    "            aggregated_df3 = aggregated_df3[['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                         'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                        'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                       'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                            'local_date']]\n",
    "            logging.info(f\"Exporting aggregated data (geohash3) for date: {event_date}\")\n",
    "            if not os.path.isfile(csv3_file_path):\n",
    "                aggregated_df3.to_csv(csv3_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df3.to_csv(csv3_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "            # Aggregation 5\n",
    "            aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std'],\n",
    "                'local_date': 'first' \n",
    "            }).reset_index()\n",
    "            aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                     'local_date']\n",
    "\n",
    "            # Reorder columns\n",
    "            aggregated_df5 = aggregated_df5[['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                         'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                        'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                       'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                            'local_date']]\n",
    "            logging.info(f\"Exporting aggregated data (geohash5) for date: {event_date}\")\n",
    "            if not os.path.isfile(csv5_file_path):\n",
    "                aggregated_df5.to_csv(csv5_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df5.to_csv(csv5_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing data for date {event_date}: {e}\")\n",
    "        \n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d70a6-9f65-4410-9ed6-b0a6fc15a32f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "process_daily_data('2020-09-20', '2020-12-31', 'CO', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d005d-a17a-438d-8bfc-376dc0a21776",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_daily_data('2020-01-01', '2020-12-31', 'ID', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6de30-64b3-4c6d-acaa-8ff525f5b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_daily_data('2020-01-01', '2020-12-31', 'IN', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee43d21-4763-4652-923b-09d09794c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_daily_data('2020-01-01', '2020-12-31', 'MX', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafc639-8253-4d03-b315-b33ca7dab7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8443428e-f7ba-432a-81e4-6f3bd01fb2ed",
   "metadata": {},
   "source": [
    "# Fill up missing dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607ae65-f3be-4260-bcf8-b39fe942f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def process_daily_data(dates, country_code, export_path):\n",
    "    \"\"\"\n",
    "    Process and export daily aggregated data for a specified list of dates and country.\n",
    "\n",
    "    Parameters:\n",
    "    - dates (list): A list of dates in the format 'YYYYMMDD'.\n",
    "    - country_code (str): The country code (e.g., 'CO' for Colombia).\n",
    "    - export_path (str): The directory where the output files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    for date_str in dates:\n",
    "        # Convert the date string to a datetime object\n",
    "        event_date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "        event_date = event_date_obj.strftime('%Y%m%d')\n",
    "\n",
    "        csv3_file = f\"od_{country_code.lower()}_agg3_daily.csv\"\n",
    "        csv5_file = f\"od_{country_code.lower()}_agg5_daily.csv\"\n",
    "        csv3_file_path = os.path.join(export_path, csv3_file)\n",
    "        csv5_file_path = os.path.join(export_path, csv5_file)\n",
    "\n",
    "        try:\n",
    "            # Read data from the SQL table for the specified day and country\n",
    "            pe_tj_df = sql_engine.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT \n",
    "                    cuebiq_id,\n",
    "                    geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "                    geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "                    geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "                    geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "                    DATE(TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date,\n",
    "                    duration_minutes,\n",
    "                    length_meters,\n",
    "                    number_of_points\n",
    "                FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "                WHERE \n",
    "                    event_date = {event_date}\n",
    "                    AND end_country = '{country_code}' \n",
    "                    AND start_country = '{country_code}' \n",
    "                \"\"\"\n",
    "            )\n",
    "            logging.info(f\"Executing SQL query for date {event_date}\")\n",
    "\n",
    "            # Aggregation 3\n",
    "            aggregated_df3 = pe_tj_df.groupby(['start_geohash3', 'end_geohash3']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std'],\n",
    "                'local_date': 'first' \n",
    "            }).reset_index()\n",
    "            aggregated_df3.columns = ['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                     'local_date']\n",
    "\n",
    "            # Reorder columns\n",
    "            aggregated_df3 = aggregated_df3[['start_geohash3', 'end_geohash3', 'trip_count', \n",
    "                                         'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                        'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                       'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                            'local_date']]\n",
    "            logging.info(f\"Exporting aggregated data (geohash3) for date: {event_date}\")\n",
    "            if not os.path.isfile(csv3_file_path):\n",
    "                aggregated_df3.to_csv(csv3_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df3.to_csv(csv3_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "            # Aggregation 5\n",
    "            aggregated_df5 = pe_tj_df.groupby(['start_geohash5', 'end_geohash5']).agg({\n",
    "                'cuebiq_id': 'count',\n",
    "                'duration_minutes': ['mean', 'median', 'std'],\n",
    "                'length_meters': ['mean', 'median', 'std'],\n",
    "                'number_of_points': ['mean', 'median', 'std'],\n",
    "                'local_date': 'first' \n",
    "            }).reset_index()\n",
    "            aggregated_df5.columns = ['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                      'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                      'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                      'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                     'local_date']\n",
    "\n",
    "            # Reorder columns\n",
    "            aggregated_df5 = aggregated_df5[['start_geohash5', 'end_geohash5', 'trip_count', \n",
    "                                         'm_duration_min', 'mdn_duration_min', 'sd_duration_min', \n",
    "                                        'm_length_m', 'mdn_length_m', 'sd_length_m', \n",
    "                                       'm_points_no', 'mdn_points_no', 'sd_points_no',\n",
    "                                            'local_date']]\n",
    "            logging.info(f\"Exporting aggregated data (geohash5) for date: {event_date}\")\n",
    "            if not os.path.isfile(csv5_file_path):\n",
    "                aggregated_df5.to_csv(csv5_file_path, index=False)\n",
    "            else:\n",
    "                aggregated_df5.to_csv(csv5_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing data for date {event_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0bd13-1530-4bd3-8d39-db25f7a138f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = ['20200501', '20200918', '20200919']\n",
    "dates = ['20200501']\n",
    "process_daily_data(dates, 'CO', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244085c-0191-4dfb-9f7e-151f64dbe7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['20200501']\n",
    "process_daily_data(dates, 'IN', '/home/jovyan/Data/2020OD/daily/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a1db4-7996-48b7-ba40-5e8a94b11d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2a306-cce5-4a58-839c-be6870cbb7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92eba047-57f2-4a3a-9d76-5674398263ef",
   "metadata": {},
   "source": [
    "# Only GH3 for CO 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74575111-84cb-49e6-870c-77c877d24a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_aggregated_data(start_date, end_date, country_code, output_csv):\n",
    "    \"\"\"\n",
    "    Fetches aggregated trajectory data from the SQL database and saves the output to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (int): The start date for filtering data (e.g., 20200701).\n",
    "    - end_date (int): The end date for filtering data (e.g., 20201231).\n",
    "    - country_code (str): The country code to filter the data (e.g., 'CO').\n",
    "    - output_csv (str): The file path where the CSV should be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        local_date,\n",
    "        start_geohash3,\n",
    "        end_geohash3,\n",
    "        COUNT(cuebiq_id) AS trip_count,\n",
    "        ROUND(AVG(duration_minutes), 6) AS m_duration_min,\n",
    "        ROUND(APPROX_PERCENTILE(duration_minutes, 0.5), 6) AS mdn_duration_min,\n",
    "        ROUND(STDDEV(duration_minutes), 6) AS sd_duration_min,\n",
    "        ROUND(AVG(length_meters), 6) AS m_length_m,\n",
    "        ROUND(APPROX_PERCENTILE(length_meters, 0.5), 6) AS mdn_length_m,\n",
    "        ROUND(STDDEV(length_meters), 6) AS sd_length_m,\n",
    "        ROUND(AVG(number_of_points), 6) AS m_points_no,\n",
    "        ROUND(APPROX_PERCENTILE(number_of_points, 0.5), 6) AS mdn_points_no,\n",
    "        ROUND(STDDEV(number_of_points), 6) AS sd_points_no\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cuebiq_id,\n",
    "            geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "            geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "            DATE(TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date,\n",
    "            duration_minutes,\n",
    "            length_meters,\n",
    "            number_of_points\n",
    "        FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "        WHERE \n",
    "            event_date BETWEEN {start_date} AND {end_date}\n",
    "            AND end_country = '{country_code}' \n",
    "            AND start_country = '{country_code}'\n",
    "    ) AS subquery\n",
    "    GROUP BY \n",
    "        start_geohash3, end_geohash3, local_date\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the SQL query and read the result into a DataFrame\n",
    "    df = sql_engine.read_sql(query)\n",
    "\n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(output_csv, index=False)\n",
    "\n",
    "#     print(f\"Data successfully saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc1dfe-4eba-49bb-8649-36fb1a89258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_and_save_aggregated_data(20200101, 20200131, 'IN', \n",
    "                               '/home/jovyan/Data/2020OD/del_3h/od_in_agg5_3h_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf753bc-4dc9-4072-ad95-0bfe60003bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b9387-7167-43b6-b828-d88e82dfea6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18d326-c618-4a60-80e8-ce79c6aa3386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d3464-3062-4e5b-a411-593caafcf752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72788b-5f24-4baf-a67f-096fb281cb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b2211-b4f1-4ee4-9449-c3eca259cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddd7b1-74e1-488d-ba97-e0cd111b81aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cda9c-2691-450a-9a7d-13b00db6a1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5af91b9-2971-42b4-bd03-c78ac3fa646f",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e80f1-674c-4309-a4ba-4defbdc6e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733afc7-6a43-40ae-a7da-a8e0737d03e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_code = 'CO'\n",
    "# country_code = 'ID'\n",
    "# country_code = 'IN'\n",
    "# country_code = 'MX'\n",
    "\n",
    "file = f'/home/jovyan/Data/2020OD/daily/od_{country_code.lower()}_agg3_daily.csv'\n",
    "# file = f'/home/jovyan/Data/2020OD/daily/od_{country_code.lower()}_agg5_daily.csv'\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea6c33-fbe7-4233-a351-61b3db0230e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['local_date'] == '2020-12-29']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2c3ee-88fc-4a2b-b78e-50966646fbae",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087639b0-581b-4dbf-a9a4-d4b6c7c05505",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['start_geohash3'] = df['start_geohash3'].astype(str)\n",
    "df['end_geohash3'] = df['end_geohash3'].astype(str)\n",
    "\n",
    "# df['start_geohash5'] = df['start_geohash5'].astype(str)\n",
    "# df['end_geohash5'] = df['end_geohash5'].astype(str)\n",
    "\n",
    "# df['local_date'] = df['local_date'].astype(int)\n",
    "df['trip_count'] = df['trip_count'].astype(int)\n",
    "df['m_duration_min'] = df['m_duration_min'].astype(float)\n",
    "df['mdn_duration_min'] = df['mdn_duration_min'].astype(float)\n",
    "df['sd_duration_min'] = df['sd_duration_min'].astype(float)\n",
    "df['m_length_m'] = df['m_length_m'].astype(float)\n",
    "df['mdn_length_m'] = df['mdn_length_m'].astype(float)\n",
    "df['sd_length_m'] = df['sd_length_m'].astype(float)\n",
    "df['m_points_no'] = df['m_points_no'].astype(float)\n",
    "df['mdn_points_no'] = df['mdn_points_no'].astype(float)\n",
    "df['sd_points_no'] = df['sd_points_no'].astype(float)\n",
    "\n",
    "# Round float columns to 6 decimal places\n",
    "float_columns = ['m_duration_min', 'mdn_duration_min', 'sd_duration_min',\n",
    "                 'm_length_m', 'mdn_length_m', 'sd_length_m',\n",
    "                 'm_points_no', 'mdn_points_no', 'sd_points_no']\n",
    "\n",
    "df[float_columns] = df[float_columns].round(6)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429c7b5-1994-4731-925c-b4df369f9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the ones to remove\n",
    "# original_df = df.copy()\n",
    "\n",
    "# # Drop duplicates\n",
    "# df = df.drop_duplicates()\n",
    "\n",
    "# # Find the rows that were removed\n",
    "# removed_rows = original_df[~original_df.index.isin(df.index)]\n",
    "# removed_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159c211-e0a9-4aa3-8f88-931269922469",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop= True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0bd9a-6e22-475e-8cff-90884b039487",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['local_date'] == '2020-10-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382f121-db6a-4268-a86d-400ded442639",
   "metadata": {},
   "source": [
    "## Check missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4db05-2639-4d55-adae-fec758f9c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = df[\"local_date\"].unique()\n",
    "all_dates = pd.date_range(start='2020-01-01', end='2020-12-31').strftime('%Y-%m-%d')\n",
    "# all_dates = pd.date_range(start='2019-01-01', end='2019-12-31').strftime('%Y%m%d').astype(int)\n",
    "\n",
    "all_dates = np.array(all_dates)\n",
    "missing_dates = np.setdiff1d(all_dates, unique_values_count)\n",
    "\n",
    "missing_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4902125-d3f1-40fa-8e73-e0fca097909d",
   "metadata": {},
   "source": [
    "### Check with figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80b5fd-bdb7-4896-80a5-73f133c45b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by local_date and calculate the total number of rows and total trip_count\n",
    "daily_summary = df.groupby('local_date').agg(row_count=('trip_count', 'size'), total_trip_count=('trip_count', 'sum')).reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for row count\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(daily_summary['local_date'], daily_summary['row_count'], linestyle='-')\n",
    "plt.title('Row Count per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Row Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot for total trip count\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(daily_summary['local_date'], daily_summary['total_trip_count'], color='orange', linestyle='-')\n",
    "plt.title('Total Trip Count per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Trip Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334206e-703b-4c38-83a9-76ad474b0aa5",
   "metadata": {},
   "source": [
    "## Clean up and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a88c15-6fd1-4a84-96e7-be296ad90e7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['local_date'] = pd.to_datetime(df['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "filter_df = df[df['trip_count']>9]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6aa06b-d0e6-42a9-99f2-4c465abf89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "com_path = '/home/jovyan/Data/2020OD/daily/combined/'\n",
    "\n",
    "df.to_csv(com_path + f'od_daily_gh3_{country_code.lower()}_2020_all.csv', index=False)\n",
    "# df.to_csv(com_path + f'od_daily_gh5_{country_code.lower()}_2020_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e4794-32ab-4496-8711-d75fe1a0eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/jovyan/Data/2020OD/daily/cleaned/'\n",
    "\n",
    "filter_df.to_csv(folder_path + f'od_daily_gh3_{country_code.lower()}_2020.csv', index=False)\n",
    "# filter_df.to_csv(folder_path + f'od_daily_gh5_{country_code.lower()}_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11fb1f-630b-4ff5-85e5-92bc2edab787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c478d2-6bd7-48e4-8ff8-b4f156d5a1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7e32a-ee7f-423c-9894-9b0866b574b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c98b0d-75ca-457d-b8f0-92801cda5e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd3f731-0eb6-4b58-bd7f-bae12c7a0e74",
   "metadata": {},
   "source": [
    "Error handelling. some days missing for 3h, reget here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dcfac-47f7-463d-b5f6-4935496377d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_date = 20200229\n",
    "country_code = 'MX'\n",
    "year = 2020\n",
    "\n",
    "pe_tj_df = sql_engine.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        cuebiq_id,\n",
    "        geohash_encode(start_lat, start_lng, 5) AS start_geohash5,\n",
    "        geohash_encode(start_lat, start_lng, 3) AS start_geohash3,\n",
    "        geohash_encode(end_lat, end_lng, 5) AS end_geohash5,\n",
    "        geohash_encode(end_lat, end_lng, 3) AS end_geohash3,\n",
    "        EXTRACT(HOUR FROM TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS event_hour,\n",
    "        DATE(TRY(date_parse(substr(start_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date,\n",
    "        duration_minutes,\n",
    "        length_meters,\n",
    "        number_of_points\n",
    "    FROM cuebiq.paas_cda_pe_v3.trajectory_uplevelled\n",
    "    WHERE \n",
    "        event_date = {event_date}\n",
    "        AND end_country = '{country_code}' \n",
    "        AND start_country = '{country_code}' \n",
    "    \"\"\"\n",
    ")\n",
    "pe_tj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ee72c-a56f-4e1e-98bd-56d3f3e24796",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe_tj_df['local_date'] = pd.to_datetime(pe_tj_df['local_date'], format='%Y-%m-%d')\n",
    "pe_tj_df['grt'] = (pe_tj_df['event_hour'] // 3).astype(int)\n",
    "pe_tj_df['day'] = pe_tj_df['local_date'].dt.day\n",
    "pe_tj_df['month'] = pe_tj_df['local_date'].dt.month\n",
    "pe_tj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba85b1-43b3-4c53-89b2-3bc53f1bcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_tj_df.to_csv('/home/jovyan/Data/0801/' + f'od_{year}0229_{country_code.lower()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc8882-92b3-4ed5-9058-f9d428bfb634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df8fa0-0926-4973-a845-bb6b616398ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0faa1-e218-4d40-a6ec-32ab3b7b5f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c91c70-36cf-4c82-83f5-0dd3cb0454d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b834dd-f8a4-43d9-baac-28c0fe180819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ada579-8e78-47cc-87ee-cb30350ed2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7429b2-de49-43db-a906-6facd40e357b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436bd0d-2a92-4783-b54c-201287eebada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ac8e8-c083-4e1c-9dcb-909bf3fce30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
