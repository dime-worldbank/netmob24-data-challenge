{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e12420-8683-4154-a7f5-88d98d4528a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "from trino.dbapi import connect\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2ec24-24c0-4a8e-acdb-501f288d2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine class\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def read_sql_chunked(self, query: str, chunksize: int = 10000):\n",
    "        return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a785e-275c-4379-ba11-3f3841dfe7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the input parameters\n",
    "# country_code = 'CO'\n",
    "# country_code = 'ID'\n",
    "country_code = 'IN'\n",
    "# country_code = 'MX'\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786e035-b5d2-4cc3-863f-9f8f6a92d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFolder = \"/home/jovyan/Data/3h_pd5/\".format(country_code)\n",
    "Path(pathFolder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdf2a8-186b-4da5-8318-da8a9fcd500e",
   "metadata": {},
   "source": [
    "# January to March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a29a9-1081-45ba-8605-8d268a0622e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20190101', '%Y%m%d') AND date_parse('20190331', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}13_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2fcfb7-5fb9-49b3-b893-c7c5eb087829",
   "metadata": {},
   "source": [
    "# April to June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1c204-95d2-4d95-beec-9ced5c1a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20190401', '%Y%m%d') AND date_parse('20190630', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}46_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce8ce1-df53-4e15-9ddf-180dc37e5f12",
   "metadata": {},
   "source": [
    "# July to September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fd2f6-3ffd-4321-baf6-09fd0638a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20190701', '%Y%m%d') AND date_parse('20190930', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}79_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a928202-df60-412a-9eb9-4633040e224d",
   "metadata": {},
   "source": [
    "# Oct to December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9f1ab-e971-4258-93d9-7d2e67b178d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20191001', '%Y%m%d') AND date_parse('20191231', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}1012_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ec1a4-f49a-48b3-85a2-e67e8cee547b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd159-6199-47d5-83e6-289994ef2b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d124bc7-deba-467d-8255-58ba4982b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20191001', '%Y%m%d') AND date_parse('20191031', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}10_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03770299-4ef1-42da-9106-7d941fc47531",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20191101', '%Y%m%d') AND date_parse('20191130', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}11_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684de393-e855-449e-a6bf-76eb3cd3541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20191201', '%Y%m%d') AND date_parse('20191231', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}12_2019_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c4fb2-18b5-4e47-918d-a0ace8929444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103b0f5-7f37-4a77-92a3-862b2aea91bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a40ea-2ee3-4ebc-b0a3-886dae623501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711635e-95ff-4a06-9208-902910fc407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the input parameters\n",
    "country_code = 'CO'\n",
    "# country_code = 'ID'\n",
    "# country_code = 'IN'\n",
    "# country_code = 'MX'\n",
    "\n",
    "# Define the input schema and table name\n",
    "schema_name = {'cda': 'cuebiq.paas_cda_pe_v3'}\n",
    "pe_dl_table = f\"{schema_name['cda']}.device_location_uplevelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a77a09-a30c-4f31-ae62-9544c646ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFolder = \"/home/jovyan/Data/3h_pd5/\".format(country_code)\n",
    "Path(pathFolder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b101a-406a-452b-8397-7ce7572d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    local_date,\n",
    "    geohash_5\n",
    "FROM\n",
    "    (SELECT \n",
    "        cuebiq_id,\n",
    "        event_zoned_datetime,\n",
    "        geohash_encode(lat, lng, 5) AS geohash_5, \n",
    "        extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt, \n",
    "        DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) AS local_date\n",
    "    FROM {pe_dl_table}\n",
    "    WHERE \n",
    "        country_code = '{country_code}' \n",
    "        AND event_zoned_datetime IS NOT NULL\n",
    "        AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "        AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20200701', '%Y%m%d') AND date_parse('20201231', '%Y%m%d')\n",
    "        AND lat <> 0\n",
    "        AND lng <> 0\n",
    "    )\n",
    "GROUP BY grt, local_date, geohash_5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl = sql_engine.read_sql(query)\n",
    "\n",
    "# Convert 'local_date' to 'yyyymmdd' format\n",
    "pe_dl['local_date'] = pd.to_datetime(pe_dl['local_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# pe_dl\n",
    "\n",
    "fileName = \"{}{}712_2020_3h_agg5.csv\".format(pathFolder,country_code)\n",
    "pe_dl.to_csv(fileName,  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf56097-a281-453f-9571-e034d93d8500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd21b3-3ba8-4333-aba4-10f989fcd6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d0173-f05c-4087-ad87-f525e55e2381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7c6f8-d181-43b9-8d06-c090b8513eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2e01e-6ae5-4e9a-a205-8c03fcbcf051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be8f6f-b61c-44cb-bbcf-6e41a5f97f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2ed56-461f-44e0-b426-d135cdc0e432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce6251-9ced-4714-b994-7cf9e96b190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7e4cbc-1fb5-4cbe-a585-ec24e519b1d6",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad09dfa-7697-4257-965f-ba8a177c4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b47c7-ddfb-4ad9-9621-470507619974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d753647-235c-4a37-894d-95599d525a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pattern = '/home/jovyan/Data/3h_pd5/CO*_2020_3h_agg5.csv'\n",
    "file_list = glob.glob(path_pattern)\n",
    "file_sub_list = [path for path in file_list]\n",
    "file_sub_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fed81-ed44-46f4-9a3a-3db20076366e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448f1d8-1b8f-4c2f-9998-428eef1f964e",
   "metadata": {},
   "source": [
    "## Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bf73c-0b2d-4fe2-8cae-4dbf43328238",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df['geohash_5'] = combined_df['geohash_5'].astype(str)\n",
    "combined_df['no_of_points'] = combined_df['no_of_points'].astype(int)\n",
    "combined_df['no_of_unique_users'] = combined_df['no_of_unique_users'].astype(int)\n",
    "combined_df['local_date'] = combined_df['local_date'].astype(int)\n",
    "\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fe3ec-2018-4204-921d-0ab2aa4d2028",
   "metadata": {},
   "source": [
    "## Check missing date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d3b85-9904-46fc-9c29-59caf7b2b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = combined_df[\"local_date\"].unique()\n",
    "all_dates = pd.date_range(start='2020-01-01', end='2020-12-31').strftime('%Y%m%d').astype(int)\n",
    "all_dates = np.array(all_dates)\n",
    "missing_dates = np.setdiff1d(all_dates, unique_values_count)\n",
    "\n",
    "missing_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839eda4-7591-4c6c-961c-949ef9179ac4",
   "metadata": {},
   "source": [
    "## Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bffcd-c0b4-4ad8-858e-fd5be3ed5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_10 = combined_df[combined_df['no_of_unique_users'] >= 10]\n",
    "combined_df_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549e44e-d718-4286-9500-a6cb10587537",
   "metadata": {},
   "source": [
    "## Check with figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a7c34-243b-40b4-9d79-a4b3462a2aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebee9c-eebf-4e36-a869-b70695137122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d4b29-b335-4ff4-b7ee-cc997485d5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2e9076-c0b1-4aaa-b96a-04dc25189933",
   "metadata": {},
   "source": [
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5786cdb-b2ae-4652-8eb1-1b3e1aee0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate local_time based on grt\n",
    "def calculate_local_time(local_date, grt):\n",
    "    start_hour = grt * 3\n",
    "    end_hour = start_hour + 3\n",
    "    return f\"{str(local_date)} {str(start_hour).zfill(2)}:00:00 - {str(end_hour).zfill(2)}:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7652a-fdd5-4bec-9b6e-273f3ee75546",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df_10.loc[:, 'local_time'] = combined_df_10.apply(lambda row: calculate_local_time(row['local_date'], row['grt']), axis=1)\n",
    "combined_df_10 = combined_df_10.drop(columns=['grt'])\n",
    "\n",
    "combined_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783cdee-f3f8-48f6-8020-7eb8f0f2df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "folder_path = '/home/jovyan/Data/3h_pd5/Clean/'\n",
    "combined_df_10.to_csv(folder_path + 'pd_3h_gh5_co_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52662c72-6a81-4afa-8988-b925fb7cf0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1da0c5-474e-46fe-8a88-9ce60d2436f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60754415-e11a-466b-9d7b-45f90bf8c43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7312d-d666-455f-af6d-1dde05f43fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the SQL query\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    day,\n",
    "    month,\n",
    "    geohash5\n",
    "FROM\n",
    "(SELECT \n",
    "    cuebiq_id,\n",
    "    event_zoned_datetime,\n",
    "    extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  hour,\n",
    "    extract(DAY FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  day,\n",
    "    extract(MONTH FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  month,\n",
    "    geohash_encode(lat, lng, 5) AS geohash5, \n",
    "    extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20201101', '%Y%m%d') AND date_parse('20201130', '%Y%m%d') \n",
    ")\n",
    "GROUP BY grt, day, month, geohash5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "fileName = \"{}{}_daily_agg5_hour_nov.csv\".format(pathFolder,country_code)\n",
    "pe_dl_table_gen.sort_values(['grt','day'], inplace=True)\n",
    "pe_dl_table_gen.to_csv(fileName,  sep=',', index=False)\n",
    "kanonimization (fileName,popDensity_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850cee5-0a4d-40dd-ae2a-502ea4f65c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the SQL query\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS no_of_points,\n",
    "    COUNT(DISTINCT(cuebiq_id)) AS no_of_unique_users,\n",
    "    grt,\n",
    "    day,\n",
    "    month,\n",
    "    geohash5\n",
    "FROM\n",
    "(SELECT \n",
    "    cuebiq_id,\n",
    "    event_zoned_datetime,\n",
    "    extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  hour,\n",
    "    extract(DAY FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  day,\n",
    "    extract(MONTH FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))  month,\n",
    "    geohash_encode(lat, lng, 5) AS geohash5, \n",
    "    extract(HOUR FROM date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))/3  grt\n",
    "FROM {pe_dl_table}\n",
    "WHERE \n",
    "    country_code = '{country_code}' \n",
    "    AND event_zoned_datetime IS NOT NULL\n",
    "    AND TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s')) IS NOT NULL\n",
    "    AND DATE(TRY(date_parse(substr(event_zoned_datetime, 1, 19), '%Y-%m-%dT%H:%i:%s'))) BETWEEN date_parse('20201201', '%Y%m%d') AND date_parse('20201231', '%Y%m%d') \n",
    ")\n",
    "GROUP BY grt, day, month, geohash5\n",
    "\"\"\"\n",
    "\n",
    "pe_dl_table_gen = sql_engine.read_sql(query)\n",
    "fileName = \"{}{}_daily_agg5_hour_dec.csv\".format(pathFolder,country_code)\n",
    "pe_dl_table_gen.sort_values(['grt','day'], inplace=True)\n",
    "pe_dl_table_gen.to_csv(fileName,  sep=',', index=False)\n",
    "kanonimization (fileName,popDensity_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9dd27-e384-4587-b814-29219529664b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e5073-e361-4a26-9933-c0722a60bbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
